Question: Who created KnowHowAngels
Answer: <p>KnowhowAngels.com was created and is owned and operated by Docunexus, Inc., a company incorporated in New York in 2008.</p>
------------------------------
Question: who owns KnowHow Angels
Answer: <p>KnowhowAngels.com was created and is owned and operated by Docunexus, Inc., a company incorporated in New York in 2008.</p>
------------------------------
Question: what is the relation between Docunexus and Knowhowangels
Answer: <p>KnowhowAngels.com was created and is owned and operated by Docunexus, Inc., a company incorporated in New York in 2008.</p>
------------------------------
Question: How much does it cost for angels experts
Answer: <p>KnowHowAngels will always be free for experts. Indeed, our goal is to make money for experts. We make money when you make money.</p>
------------------------------
Question: How much does it cost for users
Answer: <p>It doesn't cost (non-angel) users anything to register on this site. In fact, users don't need to register to use the resources of KnowhowAngels.com. The answer to many questions are provided for free. When a user poses a specialized question an angel may offer to answer it for a fee. How much depends entirely on the angel. We do not set the fees. At a later stage when we have more data we may be in a position to suggest a fee to an angel. But, in the final analysis, it is up to the angel to set the fee. Users may negotiate the fee with angels.</p>
------------------------------
Question: is Knowhow Angels free
Answer: <p>Whether Knowhow Angels is free depends on whether you are an angel or a user and as a user what services you request. It will always be free for angels and experts. It doesn't cost (non-angel) users anything to register on this site. In fact, users don't need to register to use the resources of KnowhowAngels.com. The answer to many questions are provided for free. When a user poses a specialized question or request an angel may offer to answer it for a fee. The fee depends entirely on the angel. We do not set the fees. At a later stage when we have more data we may be in a position to suggest a fee to an angel. But, in the final analysis, it is up to the angel to set the fee. Users may negotiate the fee with angels.</p>
------------------------------
Question: how does one become an expert
Answer: <p>Currently becoming an expert is only by invitation. You may request an invitation. In general we require some public evidence that you have the requisite expertise and have the skill to share it. Such evidence may be published articles, blog posts, YouTube videos, github repos, etc.</p>
------------------------------
Question: Can anyone be a user of Knowhow Angels
Answer: <p>Yes, any one can be a user. There are no acceptance requirements</p>
------------------------------
Question: What are the advantages of being a registered user?
Answer: <p>Registered users can request answers and receive answers from our angels. Non-registered users can receive answers only from our chatbots.</p>
------------------------------
Question: is use of KnowhowAngels by invitation only?
Answer: <p>Anyone can be a user--no invitation required. However, angelhood is by invitation only.</p>
------------------------------
Question: how do i apply to become an expert or angel
Answer: <p>Currently becoming an angel is only by invitation. You may ask for an invitation here (link). In general we require some public evidence that you have the requisite expertise and have the skill to share it. Such evidence may be published articles, blog posts, YouTube videos, github repos, etc.</p>
------------------------------
Question: do I have to register or login to use Knowhow Angels
Answer: <p>You do not have to register or login to use this application. However, registered and logged-in users can request answers from our angels. Non-registered or non-logged-in users can receive answers only from our chatbots.</p>
------------------------------
Question: what are your terms and conditions
Answer: <p>&nbsp;Terms and conditions are being drafted and should be available shortly.</p>
------------------------------
Question: How do I setup a ec2 server in aws?
Answer: <p>This are the steps:</p><ol><li>do this</li><li>do that</li><li>do this again</li></ol>
------------------------------
Question: How can I scale up my servers based on demand in aws?
Answer: <p>With an auto scaling group.</p>
------------------------------
Question: What are your privacy policies
Answer: <p>Our privacy policies are being drafted and should be available shortly. However, it is our intention never to abuse in any way your information.</p>
------------------------------
Question: How is this venture funded
Answer: <p>As of this point, this venture is primarily being bootstrapped.</p>
------------------------------
Question: does your company have any investors
Answer: <p>As of this point, this venture is primarily being bootstrapped.</p>
------------------------------
Question: Do I get a refund if i am not satisfied with a paid answer
Answer: <p>A 50% refund in the form of credit is given in case a user claims the answer provided is not satisfactory.. We do not provide a cash refund.</p>
------------------------------
Question: What if a paid answer turns out to be totally worthless? In that case can I get a 100% refund?
Answer: <p>You get a 50% refund in the form of credit if you are dissatisfied with an answer. In special and rare cases we may give a 100% refund in the form of credit if upon review we deem the answer to be worthless.</p>
------------------------------
Question: do you plan to have advertising?
Answer: <p>Yes, we will have advertising on many of the pages of KnowHowAngels.com. We may also display ads in chat conversations.</p>
------------------------------
Question: Will KnowHow Angels  share advertising revenues with experts?
Answer: <p>We will pay an expert 40% of ad revenues that are generated by pages where most of the content is produced by the expert.. For example, in articles in Knowhow Wire.</p>
------------------------------
Question: How do I ask a question to a specific expert?
Answer: <p>&nbsp;You can ask a question to a specific expert by going to their profile page and entering your question in the chat interface on that page.</p>
------------------------------
Question: will KnowHow Angels be going beyond computing technology
Answer: <p>We plan to cover topics in areas other than computing technology, but not immediately.</p>
------------------------------
Question: Will there be foreign language versions of knowhow angels?
Answer: <p>At some point we will introduce foreign language versions of KnowHow Angels, but not in the near future.</p>
------------------------------
Question: Who is Shekhar Pradhan
Answer: <p>Shekhar Pradhan is the creator of KnowhowAngels.com and the CEO of Docunexus, which owns and operates KnowhowAngels.com. He is a computer scientist by training and has taught computer science courses at many universities, most recently at Columbia University, where he was a full-time faculty member.</p>
------------------------------
Question: How can I be sure that a so-called expert in an area is really an expert in that area
Answer: <p>The expert's profile should contain detailed information about their accomplishments in that area with links to their github repos, their LinkedIn profile, their YouTube channel, etc. You can also ask them questions to gauge their expertise, many of which will be answered for no charge. Although we try hard to ascertain that our experts have the expertise they claim, finally it is up to you to judge whether you can trust their claims to expertise.</p>
------------------------------
Question: Who created your chatbot platform
Answer: <p>We created our chatbot platform in-house from scratch.</p>
------------------------------
Question: Do you use Zoom for video conferencing?
Answer: <p>No, we do not use Zoom. We have built our own web conferencing platform.</p>
------------------------------
Question: do I get paid as soon as I have provided an answer to the user
Answer: <p>Experts do not get paid as soon as as they have provided an answer. The user must approve the answer. Then we release the money when there's at least $20 in the expert's account. That is the minimum credit you need to have in your account for us to release the money. You do not lose your money if you have less than $20.&nbsp;We will release it if and when you close your account with us.</p>
------------------------------
Question: What is KnowHow Wire?
Answer: <p>Knowhow Wire is the name of our on-line magazine. We plan to publish issues devoted to specific areas of computing technology on a periodic basis.</p>
------------------------------
Question: why did you decide to launch an online magazine
Answer: <p>We regard our magazine as a form of content marketing for our experts. We hope to earn good SEO ranking through the information provided in the magazine articles</p>
------------------------------
Question: Why should anyone use Knowhow Angels instead of Stackoverflow or Quora?
Answer: <p>Stackoverflow and Quora are great services. We love them. But often the the answers on Stackoverflow and Quora are not current. Not easy to ask follow-up questions on those sites. And, most importantly, they do not provide the feature of having one-on-one conversations with experts, which we provide through video conferencing and text messaging.</p>
------------------------------
Question: YouTube has videos on many topics. What is the advantage of using your application over YouTube
Answer: <p>A YouTube video on a certain topic, although a great resource, are not necessarily tailored to your specific question or problem. Also, YouTube does not provide the feature of having one-on-one conversations with users, which we provide through video conferencing and text messaging.</p>
------------------------------
Question: how do I find out which questions of users are in need of answering?.
Answer: <p>If a user has specifically requested that a question be sent to you or if we feel you are among those best suited to answer the question, we notify you regarding that question. These questions will also appear in the questions feed on your dashboard.</p>
------------------------------
Question: can I modify my offer to answer a question?
Answer: <p>You may modify your offer to answer a question at any time, both in terms of the sum you are asking for answering the question and the time you need to answer the question. You can also cancel your offer to answer a question.</p><p><br></p>
------------------------------
Question: Why should anyone use Knowhow Angels?
Answer: <p>In addition to the services provided by question-answering services like Stackoverflow and Quora, we provide the feature of on-demand video conferencing with an expert. Our ideal use case starts with a user experiencing some difficulty in their project, posing a question to our experts and after two or three follow-up questions books a video conference with one of our experts to solve their difficulty. Other services don't provide this experience. We believe there's a real need for this sort of service.</p>
------------------------------
Question: Why wouldn't a user just use Google to search answers to their questions?
Answer: <p>It is true that these days most people search for an answer to a question by doing a Google search. But you may have noticed that the answers Google provides come from other websites. We aspire to be one of those sites from which Google draws its answers. Furthermore, we provide the user the options of asking follow-up questions and also having a one-on-one video conference with our experts. That is, we provide much more than the one-shot question-answering service provided by Google.</p>
------------------------------
Question: Who owns the content I have created?
Answer: <p>The content posted on KnowHowAngels.com is owned by you and licensed to  Docunexus, Inc. (the company that owns and operates KnowHowAngels.com). For more specifics on the terms of the license, please see our Terms of Service. </p>
------------------------------
Question: Does every expert angel need to create a chatbot
Answer: <p>Angels are not required to create a chatbot, but we strongly recommend creating one. This gives a user a chance to judge the quality of an angel's answers before they engage them for a paid transaction. Think of it as your own 'freemium' service. And, if some of your answers in the form of videos that gives users a chance to form a better sense of who you are. It's all about creating a connection and building your brand on our site.</p>
------------------------------
Question: What is the process for creating a chatbot?
Answer: <p>You enter your questions and answers through your dashboard (The 'Add Question' button in the Actions category on the left side of the dashboard screen).. You can test and improve your chatbot using the 'sandbox' mode. You can access your chatbot in the sandbox mode simply by clicking the 'Test Your Chatbot' button and entering the question you want to test in the chat interface that  pops up.</p>
------------------------------
Question: How do I test my chatbot before it is published
Answer: <p>You can test your chatbot by accessing it in the sandbox mode and entering the question you want to test in the chat interface on your dashboard. To access your chatbot in the sandbox mode just click on the 'Test Your Sandbox button.</p>
------------------------------
Question: Can I change a question or an answer in my chatbot
Answer: <p>Yes, you can change one of your questions or answers at any time (even after your chatbot has been published). You can do this through your dashboard. Click on the Edit Questions button under Actions in the menu on the left hand side of the dashboard.</p>
------------------------------
Question: Can I give a name to my chatbot?
Answer: <p>The default name is your user name followed by 'Bot'. Thus, if your user name is 'AIGuru' then your bot will be named 'AIGuruBot'. At this point, we don't allow you to change your default bot name, but we may change that policy in the future.</p>
------------------------------
Question: Can I install my chatbot on my website
Answer: <p>You may create a link to your chatbot on your website but the link must point to KnowHowAngels.com. There is no charge for that. However, if you want to install&nbsp;your chatbot on a different site we will charge you for installing the chatbot on the other site and a monthly fee for operating it. Contact us as requests@docunexus.com.</p>
------------------------------
Question: How soon do you transfer money to my account after I have earned it.
Answer: <p>Once your answer is approved by the other party (who engaged you to provide the answer), you will get the money within forty-eight hours if there's at least US$20 in your account. Otherwise, you'll have to wait until your account has accumulated at least $20.</p>
------------------------------
Question: What is the minimum amount you will transfer into my account?
Answer: <p>The minimum amount we will transfer to an angel is US$20.</p>
------------------------------
Question: How do I submit an article to knowHow wire
Answer: <p>Generally submission to Knowhow Wire is by invitation. Please contact us if you are interested in submitting an article.</p>
------------------------------
Question: Do you pay authors to write an article for knowhow wire?
Answer: <p>We do not pay authors for publishing on Knowhow Wire. However, we share the revenues earned through ads displayed in their articles.&nbsp;</p>
------------------------------
Question: Does a user need to create a profile
Answer: <p>Angels (experts) need to create a profile. For other users, it is optional.</p>
------------------------------
Question: Can I include my contact information on my profile?
Answer: <p>We request that angels do not include any contact information external to this site. Users can contact you through our messaging service. The primary reason for this requirement is that our business model depends on contracts between users and angels going through our platform. Additionally, both users and angels have certain protections in case of disputes if contracts between them are executed through our platform.</p>
------------------------------
Question: Why does Knowhow Angels prohibit experts from including their contact information on their profile?
Answer: <p>We request that angels do not include any contact information external to this site. Users can contact you through our messaging service. The primary reason for this requirement is that our business model depends on contracts between users and angels going through our platform. Additionally, both users and angels have certain protections in case of disputes if contracts between them are executed through our platform.</p>
------------------------------
Question: How can a user communicate with an expert if they don't have their contact information
Answer: <p>Users can send a message to an expert through our messaging service. Just address your message to the profile name of the expert.</p>
------------------------------
Question: Does Knowhow Angels make payments only in US dollars
Answer: <p>We are happy to make payments in any currency. A small surcharge may be levied for such payments in currencies other than US dollars to cover the cost of currency conversion charged by our financial partners.</p>
------------------------------
Question: Does Knowhow Angels accept payments only in US dollar?s
Answer: <p>At this time we accept payments only in US dollars.</p>
------------------------------
Question: What is the difference between an angel and an expert
Answer: <p>An angel is just our term for our experts because we see them as your saviors.</p>
------------------------------
Question: What is the purpose of a load balancer?
Answer: <p>It is a server that routes traffic among an application running in several nodes in order to distribute the server load among them.</p>
------------------------------
Question: Why should I use aws?
Answer: <p>It's very cost beneficial and has a good pricing model</p>
------------------------------
Question: What does Knowhow Angels do
Answer: <p>Knowhow Angels connects experts with those seeking their expertise. Say, you are stuck at some point in executing your project. If you have a friend who has the expertise, you'd ask them. But what if you don't have such a friend who has the expertise and who is available to you? That's when you can use Knowhow Angels because we have such experts and they are ready to help you. Problem solved!</p>
------------------------------
Question: Are your experts available day and night?
Answer: <p>Our chatbots, which may answer many of your questions, operate 24/7, Although we plan to recruit experts residing in many time zones, they may not be available to respond to your requests immediately. We are not a live service desk (not yet, anyway). But we will make every effort to get our experts to respond to your requests as soon as they can.</p>
------------------------------
Question: What is Knowhow Angels
Answer: <p>Knowhow Angels is a platform that connects experts with those in need of their expertise. It provides users a way to get on-demand expertise--as little as an answer to a single question or an extended sessions of any length. It is based on three mega trends: unbundling of knowledge, gig economy, and conversational interfaces.</p>
------------------------------
Question: what is natural language processing
Answer: <p>Natural language processing can be defined as that branch of artificial intelligence which is concerned with getting computers to respond to inputs in a 'natural' language such as English (as opposed to a formal language like logic or programming language) in ways that a human who understands such inputs would respond. For example, getting computers to extract the occurrence of certain events in a document or to summarize a document. Or, to answer a question, as I, the Knowhow Bot, is doing right now.</p>
------------------------------
Question: Which disciplines are involved in natural language processing?
Answer: <p>Natural language processing is above all a branch of computer science (artificial intelligence in particular, which is itself a branch of computer science). It also draws from linguistics, statistics, and philosophical semantics.</p>
------------------------------
Question: What is a language model?
Answer: <p>A language model for a language such as English or Hindi is a probabilistic model for that language such that given the occurrence of a sequence of words it can predict the probability of the next word in that sequence. </p>
------------------------------
Question: What is part of speech tagging?
Answer: <p>In part of speech tagging each token in a sequence (such as a sentence) is tagged with a label denoting whether it is a verb, noun, article, etc. There are algorithms for doing this programmatically such as the well-known Viterbi algorithm. There are many different tag sets (or parts of speech sets), but the Penn tag set has become a <em>de facto</em> standard among natural language practitioners.</p>
------------------------------
Question: What is the difference between text mining and natural language processing
Answer: <p>Natural language processing and text mining (also known as information extraction) are very closely related. Some may even use it interchangeably. As the word 'mining' in text mining suggests, it consists in extracting some information from a body of text. This could be the occurrence of various types of entities, relations, events, processes, etc. It could even be the extraction of summaries or answers to questions. Although NLP covers all this, it also includes such things as parsing the structure of a sentence, part of speech tagging, spelling correction and broadly natural language understanding, none of which can be accurately considered as extracting information.</p><p>Furthermore, in text mining, as in any form of data mining, a lot of effort is spent on assembling the data (the corpus),cleaning it and normalizing it. Whereas, in natural language processing it is assumed that we have a corpus ready for analysis.</p>
------------------------------
Question: What is the difference between extractive and abstractive summarization?
Answer: <p>Extractive summaries are created by extracting sentences or sentence fragments from the text which is being summarized. On the other hand, abstractive summaries may have sentences that do no occur in the text but which are believed to convey the gist of the main points of the text.</p>
------------------------------
Question: What is dependency parsing?
Answer: <p>A dependency parser takes a sentence (or phrase) as input and outputs the grammatical structure of the sentence in terms of the dependency relationships among the words. For example, in the phrase "fat cat" the word "fat" is a dependent of "cat". In this phrase "cat" is the  'head' word. One way of representing this is in terms of a directed arc from the head to the dependent. The parse is called 'typed' if such arc is labelled with the type of dependency. (In this case it would be 'adjmod', meaning adjectival modifier.) This type of parsing provides information about what role each word plays relative to the other words in the sentence (or phrase).</p><p>Dependency parsing is often contrasted with 'constituency parsing' in which the parse tree identifies the constituents of the sentence being parsed, for example noun phrase, verb phrase, etc., without any information what role a noun phrase plays (subject? direct object? indirect object?).</p>
------------------------------
Question: What is shallow parsing
Answer: <p>We won't distinguish between shallow parsing and chunking although there are some subtle differences in them. Basically, chunking rules identify the main constituents of a sentence (noun chunks, prepositional phrases, etc.) in terms of simplified grammar rules without identifying the relationships between the parts of the chunk. For example, a rule for identifying a noun chunk may be 'zero or one determiner followed by zero or more adjectives followed by  a noun'. In the resulting noun chunk we may not care to identify which are the adjectives and which is the determiner and which is the noun (a flat parse). Take the sentence, "the chairwoman of the ad hoc committee called a meeting." A noun chunker would identify 'the chairwoman" and "ad hoc committee" and "a meeting" as noun chunks without identifying "the" or "a" as determiners  (in the output as opposed to in the processing).</p><p>The chunking rules are simplified (in comparison to the rules of constituent parsers) in that they cannot be recursive. For example, the rule for identifying a noun chunk cannot mention a noun chunk (as opposed to a noun). It is for this reason a noun chunker will not identify "the chairwoman of the ad hoc committee" as a noun chunk.</p>
------------------------------
Question: What is a probabilistic parser?
Answer: <p>In the process of building a parse tree of a sentence, at a given stage more than one rule may be applicable. One of them may lead to a correct parse of the sentence and the others may lead to dead ends. How does an algorithm decide which rule to apply? If there were probabilities assigned to each of the candidate rules, then the algorithm could choose the one with highest probability. This is the idea behind probabilistic parsers.</p><p>But where do these probabilities come from? We start with a corpus of sentences and use well-trained humans to create a parse tree for each sentence. Based on this 'tree bank' we can assign probabilities to each rule depending on how many times it was applied at that stage in the construction of a parse tree in the tree bank.</p>
------------------------------
Question: What is a parse tree?
Answer: <p>A parse tree for a sentence (or a phrase)  is an upside-down tree with the sentence as its root. Each node of the tree is a constituent (as in constituent parsing) labelled with the name of the constituent (e.g., noun phrase). But since the constituent (e.g. 'the quick brown fox') is itself a phrase, it can also be parsed in term of its constituents. So the parse tree for that phrase is a sub-tree of the parse tree for the sentence (e.g., 'the quick brown fox jumped over the fence') in which that phrase occurs. This process continues until we reach the leaves of the tree, which are just the single words (often called lexical units) of the sentence.</p>
------------------------------
Question: What is top-down parsing?
Answer: <p>Top-down parsing is the process of creating a parse-tree for a sentence (or an expression) starting with a sentence node as the root of the tree and successively applying the grammar re-write rules to expand each node in the tree until one arrives at the leaves of the tree, which should consist entirely of literals (i.e., words of the sentence).</p><p>Thus, a rule might be of the form S-&gt; NP VP PP. This rule can be applied to expand a node if it matches the left-hand side of the rule (in this case, S). After applying this the tree expands to contain sub-trees with roots NP, VP and PP. Let us say, the grammar contains only one rule with PP on the left-hand side (PP--&gt; quickly). This rule can be applied to expand the PP node with the leaf node, 'quickly'. Repeatedly applying such steps creates a parse tree with only literals as leaves. The parse is incorrect if the leaves don't include all the words in the sentence and if it includes words not in the sentence. </p>
------------------------------
Question: What is bottom-up parsing?
Answer: <p>Bottom-up parsing consists in creating a parse tree for a sentence starting with the leaves of the tree which are the words in the sentence being parsed and creating higher nodes in the tree until one gets to a sentence node, which is the root of the tree. In the bottom-up method (unlike the top-down method) the grammar rules are applied from the right to the left. The right side of the rule must match the node(s) from which one is trying to create a higher level node. Thus, if there's a rule NP--&gt; Det Noun, then that rule can be applied to create the node NP if the tree already has a Det and a Noun node adjacent to each other (in that order). Application of that rule will create the NP node which will be the parent of the Det and Noun node in the tree.</p>
------------------------------
Question: What is the difference between top-down and bottom-up parsing?
Answer: <p>Both top-down and bottom-up are techniques for creating a parse tree for a sentence (or an expression). They differ in their starting points (and, thus, their end points) and how they process the rules of the grammar used to create parse trees. </p><p>The bottom-up method starts with the leaves of the tree, which are always words (lexical units) of the language to which the sentence belongs. The bottom-up method starting with the leaves proceeds to create the parent nodes of these and then the parent nodes of the higher level nodes until it arrives at the sentence node, which is the root of the tree. (For more details see the answer to "What is bottom-up parsing"?). </p><p>The top-down method creates the parse-tree by stating with the sentence node, which is the root of the tree, and step by step creating the descendants of each node until it arrives at the leaves, which is are always word of the language to which the sentence belongs. (For more details see the answer to "What is top-down parsing"?). </p>
------------------------------
Question: Which programming language is best for natural language processing?
Answer: <p>Python is generally regarded as the best programming language for doing natural language processing. It has many excellent open source nlp resources such as NLTK, Spacy, etc. Furthermore, as  NLP techniques rely more and more on vector semantics (word embeddings), Python shines because of libraries such as NumPy and Pandas, which are optimized for doing complex matrix operations.</p>
------------------------------
Question: What is laravel?
Answer: <p>Laravel is a free, open-source PHP framework for creating modern web applications. It is built on top of the Symphony framework, follows the model-view-controller pattern, and has libraries to help you create and manage all aspects of your web application, like database access, email sending, notifications, caching, and much more.</p>
------------------------------
Question: Which databases does laravel support?
Answer: <p>Laravel has support for MySQL, PostgreSQL, SQLite, and SQL Server out of the box. There are third-party drivers available that can be used to connect Laravel's Query Builder and ORM libraries to unsupported RDMS. </p><p><br></p><p>Laravel does not natively support NoSQL databases.</p>
------------------------------
Question: which parsing technique is the best
Answer: <p>Both top-down and bottom-up parsing have their disadvantages in terms of wasted effort in developing tree paths that lead to dead-ends. In bottom-up parsing one may end up choosing to apply a sequence of rules which do not lead to end state of the sentence node (S node), a dead-end. However, in top-down parsing we may apply a sequence of rules that does not result in the end state of leaves with all the literals and only the literals of the sentence to be parsed, also a dead-end. In either case the parser upon reaching a dead-end has to backtrack and try a different sequence of rules.</p><p>It is not clear that one method is clearly superior to the other.</p>
------------------------------
Question: what are the different types of word embeddings?
Answer: <p>By word embedding we mean a vector representation of each linguistic unit (word or sequence of words or sequence of characters) of a language in a vector space. Typically, such vector representations are based on co-occurrence of words of that language, but this is not essential to the definition of 'word embedding'. For instance, a vector representation for each word can be based on the tf-idf score for each word in a corpus of documents. .</p><p>The three main types of word embeddings to-date are word2vec, GloVe, and FastText. Of these, word2vec was the first. All three produce a multi-dimensional vector representation of each linguistic unit in the language.</p>
------------------------------
Question: Why should I use laravel?
Answer: <p>There are several advantages to using Laravel as your PHP framework of choice. Some of them are:</p><ul><li>It has a very straightforward, elegant syntax. </li><li>It has a library or packages to help you with most problems your application might face, be it caching, job scheduling, authentication, notification, localization, and many more</li><li>It has a robust command-line tool, with commands generating common classes, managing the application cache and support for creating new commands.</li><li>It is a very popular framework, with a very large community that can help you with any issues you might have.</li><li>Is built on top Symphony, one of the most tried and tested PHP frameworks in the market.</li></ul>
------------------------------
Question: Can I use websockets with laravel?
Answer: <p>While Laravel does not directly support WebSocket connections, it does include a broadcasting library that covers most WebSocket use-cases. It integrates with the events and notifications libraries to send the relevant data through broadcasting channels that your frontend application can consume. </p><p>It has built-in support for Pusher and Ably as the broadcast provider, but also recommends a community-driven package as an open-source alternative. Laravel also provides a javascript library called Laravel Echo, that helps you consume those broadcasting channels in your frontend application.</p>
------------------------------
Question: What do I need to run a laravel application?
Answer: <p>Laravel has a minimum PHP version requirement that depends on the version of the framework being run. Laravel 8, the most current version requires PHP to be at least version 7.3. It also requires a number of extensions for the PHP installation. A full list of the extensions needed can be found in their <a href="https://laravel.com/docs/8.x/deployment#server-requirements" rel="noopener noreferrer" target="_blank">documentation</a>. </p><p><br></p><p>Laravel also provides pre-configured docker and vagrant packages, named Laravel Sail and Laravel Homestead respectively, that contains everything you need to run a development server on your machine, including a configured PHP runtime, a MySQL database, and useful tools like NPM and Composer, all out of the box, no configuration needed.</p><p><br></p><p>A third-party project also worth mentioning is <a href="https://laradock.io/" rel="noopener noreferrer" target="_blank">Laradock</a>, which provides a pre-configured docker environment, similar to Laravel Sail, but with many more packages and tools pre-configured to use in your Laravel project, including things like Memcached, Redis, ElasticSearch, Selenium, and many many more. It is the perfect project to kickstart your development environment for more complex applications.</p>
------------------------------
Question: Can I use laravel with react for the frontend?
Answer: <p>Absolutely. Laravel has a package called Laravel mix, which provides an easy to use API for configuring Webpack for common, frontend architectures. It by default includes all the necessary pre-processors for React and Vue, along with pre-processors for Sass, PostCSS, and Tailwind CSS. Also, since Mix is built on top of Webpack, you can easily extend the pre-made Webpack configuration and add any pre-processors and plugins for all your frontend needs.</p>
------------------------------
Question: What is the advantage of using Laravel over the other PHP frameworks?
Answer: <p>Laravel has three main advantages over other PHP frameworks: It's excellent developer experience, its big community, and great documentation.</p><p><br></p><p>Laravel goes out of its way to make the life of the developer simpler. It will not only have libraries that cover most web-applications use cases, like a caching system, cloud storage, worker queues, job scheduling, and many more, all with very elegant syntax and minimal configuration required, but also will provide Facades and helper functions that will give easy access to these internal components and extends PHP own default library shortcomings. As an example, it has included out of the box a long list of helper functions that handle common string and array manipulation tasks.</p><p><br></p><p>Laravel is the most popular PHP framework according to Stack Overflow's 2020 survey, and popularity is a very important metric for choosing a web framework. It indicates how easy it will be to find solutions to common problems, and also how easy it is to find help for your specific needs.</p><p><br></p><p>The last key advantage of Laravel is its documentation. It is very complete, covering all of its internal libraries and also providing tons of examples of common use cases for each of them. Even in the unlikely event of a specific use case not covered in their documentation, diving into its source code to understand how a system or library works under the hood is a breeze, due to its highly modular architecture and very well documented code sections.</p>
------------------------------
Question: What is Ruby on Rails?
Answer: <p>Ruby on Rails is a server side web-application framework written in Ruby. It was developed by Danish programmer David Heinemeier Hansson (website: dhh.dk) and was initially released in 2004. It is a model-view-controller framework and famously prefers "convention over configuration", which is to say that has opinions about the best way to achieve a certain goal. This leads to less time spent writing configuration files, but can sometimes be problematic when the developer wants more fine-grained control. More information on RoR: https://www.skillcrush.com/blog/13-ruby-rails/, https://en.wikipedia.org/wiki/Ruby_on_Rails, official website: https://rubyonrails.org/</p><p> </p>
------------------------------
Question: Who created laravel?
Answer: <p>Laravel was created and is maintained by Taylor Otwell. He, along with his team, also maintains most of the Laravel ecosystem packages as well.</p>
------------------------------
Question: Why should I not use just plain PHP for my application?
Answer: <p>The main advantage of using a framework is security. There are several vulnerabilities that a web application can have, from XSS attacks, SQL Injection, mishandling of passwords, and many others. While it is not impossible to make your application secure with just plain PHP, a lot of time will be spent researching and developing countermeasures for all those vulnerabilities, resources that could have been spent in creating your application. With a framework, you will not only have your team but the support of the entire community of contributors to keep track and patch all those vulnerabilities, with code that is tested by potentially hundreds of people.</p><p><br></p><p>Another great advantage of frameworks is that they usually have many libraries and tools that facilitate solving common web application problems like authentication, event propagation, database access, and others. This drastically minimizes time spent creating boilerplate code and helps you focus on what makes your application unique.</p>
------------------------------
Question: How do I edit my profile?
Answer: null
------------------------------
Question: How does one evaluate the goodness of a Word2Vec model?
Answer: <p>The best way of testing a Word2Vec model is by how well it performs on an NLP task (called 'extrinsic' evaluation). For example, if you have created a model as part of a question-answering pipeline, you would judge it in terms of how well your pipeline performs on question-answering tasks. But it is not always possible to isolate the contribution of the Word2Vec to the overall performance.</p><p>For this reason it may be better to evaluate in terms of intrinsic criteria. Since Word2Vec models are supposed to give us measures of similarity of pairs of words or relatedness of pairs, one might test the model in terms of how well one does on these tasks.</p><p>You could compile your own test sets or use a test set like WordSim-353 for relatedness of pairs (e.g., kiss and hug) or use a set like SimLex-999 for similarity (e.g., car and automobile). </p><p>Another criterion (also, intrinsic) one can use is in terms of how well the model performs on word analogies. Miloklov and his colleagues have compiled  a set of such analogies. </p><p>The book<em> Speech and Language Processing</em>, 3rd edition by Jurafesky and Martin has a set of references to various test sets for evaluating Word2Vec models.</p>
------------------------------
Question: What are some test sets for evaluating Word2Vec models?
Answer: <p>There are a number of test sets but they are all for intrinsic evaluations. These can be divided in terms of  test sets for similarity, relatedness, or word analogies.</p><ul><li>Test sets for word similarity: SimLex-999, Stanford Contextual Word Similarity (SCWS), Word-in-Context (WiC), the TOEFL dataset</li><li>Test sets for relatedness: WordSim-353</li><li>Test sets for word analogies: &nbsp;SemEval-2012 Task 2 dataset, test sets compiled by Mikolov and collaborators, and those compiled by Gladkova and collaborators. </li></ul>
------------------------------
Question: What are the ways in which a Ruby on Rails application can interact with databases?
Answer: <p>Ruby on Rails uses ActiveRecord to interact with databases. ActiveRecord is what builds the "M" in MVC. The model in Ruby on Rails is a set of classes, each of which also has a corresponding database table. The table columns are available as functions for the corresponding class. ActiveRecord provides ways to interact with the database through a fairly extensive query interface (https://guides.rubyonrails.org/active_record_querying.html).  </p>
------------------------------
Question: Does laravel support social login buttons?
Answer: <p>Yes. There is a package in the Laravel ecosystem called <a href="https://laravel.com/docs/8.x/socialite" rel="noopener noreferrer" target="_blank">Laravel Socialite</a>, that provides easy integration with third party oauth providers like Facebook, Google, and others. </p><p><br></p><p>Also, due to the extensible nature of Laravel's authentication guards, you can extend its functionality to use any form of authentication that you want with little effort.</p>
------------------------------
Question: How do I run a task asynchronously in Laravel?
Answer: <p>If you are coming from Node.js it is important to note that PHP has no support for running code asynchronously by default. While there are some libraries that try to mimic that functionality generator functions or some other methods, at the end of the day the main thread has to always wait for any async tasks to finish running before it can return any data back to the user.</p><p><br></p><p>Fortunately, running long-duration tasks triggered by user requests is a common scenario in web development, and Laravel has some libraries to help with that. Those libraries are Queues and Task Scheduling. </p><p><br></p><p>Both libraries work by running a separate worker process, independent from the main web server, that will run your long-duration tasks in the background. The libraries also offer you easy ways to create tasks and pass down serializable data to them with very little effort.</p>
------------------------------
Question: What is the difference between task scheduling and queues?
Answer: <p>Task Scheduling is used to schedule tasks to be run at specific intervals. For example, you could have a database cleanup task that deletes or archives data, running every day at 12 A.M.</p><p><br></p><p>Queues on the other hand are used for one-time tasks, usually triggered by a user action. A common example of a queued task is sending emails. Because sending emails requires communication with an external email server, a common practice is to queue the sending of those emails to the worker queue. That way your application can few much more responsive.</p>
------------------------------
Question: How do I create a json response in laravel?
Answer: <p>To return a JSON response to the user you have a few options:</p><ol><li>Return an associative array in the controller function. Laravel will automatically convert the array into JSON format and set the appropriate Content-Type header</li><li>Return an Eloquent model or a collection/array of models in the controller function. Ghost will serialize all non-hidden fields into a JSON object and set the appropriate headers.</li><li>Return a Resource class in the controller. A Resource class is used to map an Eloquent model to a JSON object. It is useful to give you finer control into what fields from a model get included and also allows you to easily include fields from relationships.</li><li>Use the response() helper function. This can be used in conjunction with any of the methods above but also allows you to customize other aspects of your response, like cookies, headers, and status codes.</li></ol>
------------------------------
Question: How do I interact with session data in laravel?
Answer: <p>Laravel ships with a common session interface that supports a variety of drivers depending on your application needs. Laravel automatically handles all aspects of the session mechanism for you, like managing the session id cookie, you just need to use the session() helper function to store and retrieve your session data. </p>
------------------------------
Question: What are the advantages of GloVe over word2vec as a word embedding?
Answer: <p>It is not clear that Word2Vec is clearly superior to GloVe or the other way around. On some tasks one model does better than the other.</p><p>The primary difference between GloVe and Word2Vec is that GloVe is based on statistical analysis of a global co-occurrence matrix whereas as Word2Vec is based on predictive tasks (skip-gram or CBOW) on local co-occurrences in sliding windows over a body of text. </p><p>What does 'global' mean in this context? Let us fix the size of the window unit in which terms co-occur. This could be the same as the size of the sliding window for a Word2Vec model or something larger like a paragraph. Global means the values in the co-occurrence matrix are all the window units (in the entire corpus) in which two terms co-occur.</p><p>Word2Vec doesn’t utilize such global information. That is, information that spans across window units. In Word2Vec the (local) co-occurrences are not subject to statistical analysis to draw any insights but simply used as ground truth for machine learning. The fact that in such and such training examples the same two terms co-occur is a matter of indifference in the learning task. What matters is whether the learned model made the correct predictions in those training examples.</p><p>Furthermore, Word2Vec is based on supervised learning whereas GloVe is based on unsupervised learning. GloVe derives its ground truths (statistics) from the global co-occurrence matrix and views the learning tasks as coming up with word-vectors such that their dot product is as close to these statistics as possible.</p>
------------------------------
Question: What can be done with a migration in Ruby on Rails?
Answer: <p>Migrations in Ruby on Rails can be used to evolve your database schema. A list of migrations is stored in the db/migrate directory. Each migration file is named using a very specific format, i.e.  YYYYMMDDHHMMSS_migration_name.rb. The migrations are hence run from the least recent to the most recent. Rails provides a handy function to generate these timestamps for you: `bin/rails generate migration migration_name`. Migrations can be used to create tables, drop tables, add columns, change columns, remove columns, etc. See https://guides.rubyonrails.org/active_record_migrations.html for more details.</p>
------------------------------
Question: What are the main differences between FastText and Word2Vec?
Answer: <p>FastText is very similar to Word2Vec except that the vectors in the first place are associated with character ngrams, not words. Vector representation of words can be created by summing the vector representations of ngrams that constitute the word.</p><p><br></p><p>There are several reasons why FastText mat be preferred to Word2Vec representations. One, new words may not be in the vocabulary recognized in the construction of Word2Ve mode. But even so its character ngrams may be recognized by FastText model because the character ngrams may come from other word. For instance, the vocabulary may contain 'tensor' and 'flow' but not 'tensorflow'. But if the FastText procedure breaks this new word into the ngram 'tensor' and 'flow', it would have no problem with the new words. Two, some languages like German, which has many morphemes that can be combined in new ways, can create new words on the fly. Third, for languages with large vocabularies and many rare words, there may be few or no sliding windows in which these rare words occur. But there morphemes may be well-represented in the corpus.</p><p>There are some disadvantages to using FastText. For one, it is slower to train than Word2Vec because there are more ngrams than words, and, also, they don't perform as well on analogy tasks, which has become an important evaluation criterion for word embedding models.</p>
------------------------------
Question: What are the main differences between FastText and GloVe word embeddings?
Answer: <p>The primary difference between GloVe and FastText is that GloVe is based on statistical analysis of a global co-occurrence matrix whereas as FastText like Word2Vec is based on predictive tasks (skip-gram or CBOW) on local co-occurrences in sliding windows over a body of text.</p><p>What does 'global' mean in this context? Let us fix the size of the window unit in which terms co-occur. This could be the same as the size of the sliding window for a FastText model or something larger like a paragraph. Global means the values in the co-occurrence matrix are all the window units (in the entire corpus) in which two terms co-occur.</p><p>FastText (like Word2Vec) doesn’t utilize such global information. That is, information that spans across window units. In FastText the (local) co-occurrences are not subject to statistical analysis to draw any insights but simply used as ground truth for machine learning. The fact that in such and such training examples the same two character ngrams co-occur is a matter of indifference in the learning task. What matters is whether the learned model made the correct predictions in those training examples.</p><p>Furthermore, FastText is based on supervised learning whereas GloVe is based on unsupervised learning. GloVe derives its ground truths (statistics) from the global co-occurrence matrix and views the learning tasks as coming up with word-vectors such that their dot product is as close to these statistics as possible.</p>
------------------------------
Question: What laravel provides in terms of testing?
Answer: <p>Laravel provides out of the box a preconfigured phpunit.xml file and several libraries and packages to help you create your automated test cases.</p><p>By default, it supports unit and feature testing. Unit testing is meant for testing specific and contained portions of your code, like a single class or even a single method. Feature testing on the other hand is meant to test a more general feature of your application that may involve several components.</p><p>It provides a handful of helpers to create mocks for databases, event listeners, queue jobs, and others, allowing you to isolate the portion of your application you want to test from other side effects that a request, for example, can cause.</p>
------------------------------
Question: What is seq2seq modelling?
Answer: <p>Seq2Seq modelling (sequence to sequence) is a model that given a sequence of tokens (linguistic units), maps it to another sequence of tokens, in a token by token manner. In this sense even a sequence of transducers (a type of finite state automata) or hidden Markov models (as used in parts of speech tagging) can be consider examples of se2seq modelling. But today the term is used to mean mapping from an input sequence to an output sequence using a set (stack) of encoders and decoders. The stack of encoders outputs a (vector) representation of the input which is sent as input to the stack of decoders. The decoders use this representation to produce the output token by token. The internal processing unit of each encoder and decoder can be RNN or LSTM or GRU or Transformers. Seq2Seq technology was invented for machine translation but has been used in other applications such as question answering.</p>
------------------------------
Question: What is meant by parsing?
Answer: <p>In the context of natural language parsing 'parsing' means deriving (computing) the structure of a sentence. The result of that is called a parse. There are two main types of parsing: constituency and dependency. The former computes the main constituents of the sentence and the hierarchical relations among them, On the other hand, dependency parsing computes the relations of 'dependencies' among the units of the sentence.</p>
------------------------------
Question: What are some good tools for extracting the content of a document in creating a corpus?
Answer: <p>There are many such tools. Beautiful Soup is a popular Python library for extracting the content of HTML and XML documents. PyPDF2 is a Python library for extracting the content of PDF files. </p>
------------------------------
Question: What steps are recommended in compiling a corpus for natural language processing tasks?
Answer: <p>First, you need to be clear about why you are creating a corpus. Is it for building a model for a specific NLP application? Or, is to create a corpus that others can use to build their own NLP applications?</p><p>Second, you need to determine the type of documents you want in your corpus. News items or scholarly articles or blog posts or some mixture of all these?</p><p>Third, how many documents do you need. For some applications such as building a Word2Vec model , you may need many more than for, say a naïve Bayes, classifier.</p><p>Fourth, are the documents already available for you in some repository like a Wikipedia dump, ready for ingestion. Or, do you need to find these documents, say on the Internet? If the latter, you may use tools like Webhose.io (and their several competitors) or you may do your own crawling using a library like Scrapy. Both of these do both web crawling and scraping. Or, you may use an API provided by Bing for crawling and do scraping using the tools mentioned in the next step.</p><p>Fifth, you may need to extract the content of the documents you retrieved so they are all in the same format, text files most likely. You may use Python libraries like Beautiful Soup to extract the content of html documents or PyPDF2 to extract the content of PDF documents.</p><p>Sixth, if you want your corpus to consist of documents in one language, such as English, you'll need to do language detection to remove non-English documents. Some English documents may have non-English parts and you may need to identify those and remove the,.</p><p>Seventh, depending on your application, you may need to remove duplicates and nearly duplicates from your corpus. This is called de-duping. For that, you may use locality-sensitive hashing techniques for de-duping.</p><p>Eighth, some portions of  some of your documents may have gotten garbled in the process of applying the above steps. You need to have a way of detecting them and removing them.</p><p>Ninth, depending on your application you may need to normalize your documents. Do some of the documents use American spelling and some use British spelling? You may need to make them the same spelling.</p><p>There are probably more steps, depending on why you are creating a corpus. A rule of thumb is that 80% of the effort in a data mining project is in procuring, cleaning, normalizing data. This may well hold for text-mining projects as well.</p><p><br></p><p><br></p><p><br></p>
------------------------------
Question: How do I do caching in laravel?
Answer: <p>Similar to other libraries, Laravel has caching library that is very simple to use, allowing you to store, retrieve, and manage any serializable data in caching storage, using a unified API for multiple differents cache storage solutions. Under the hood, Laravel supports several drivers for this caching storage, for example, a file base driver, relational databases, Memcached, Redis, and even an in-memory array driver useful for testing.</p>
------------------------------
Question: What are service providers in laravel?
Answer: <p>The service container is the mechanism that Laravel uses to manage class dependencies and performs dependency injection. The routing system, caching system, and every other library inside the Laravel framework is a service in the service container. That way, all of those classes can be injected wherever they are needed in the application automatically.</p><p><br></p><p>The most common place where dependencies are injected in a typical Laravel application is in a controller's method that responds to a request. Whenever the developer needs to use a particular class to respond to that system, let's say a class responsible for communicating with another HTTP endpoint, the developer can simply add that class in the method's parameter list, including the class type as a type-hint. The service container will then automatically resolve that class based on the type-hint, and inject an instance as a parameter to the controller's method. If that class depends on other classes to function, it can also include those classes in its constructor's parameter list, and the service container will inject those as well, and this process happens recursively until all necessary classes are injected.</p><p><br></p><p>While most of the type the service container will be able to resolve a dependency without any configuration, some classes require special resources to work properly that the service container can't provide, like configuration parameters for a connection to external services for example. That's where service providers come in. </p><p><br></p><p>Service providers are a special type of class that instructs the service container on how to create an instance of a class. All the developer needs to do is to implement a register method, where it will use a reference to the service container to associated a particular type to a closure, in which the closure will be called whenever an instance of that type is needed and should return a new instance of that class being injected. </p><p><br></p><p>The service provider is also useful for registering singleton classes, in which the instance created by the closure will be saved and reused in subsequent injections. It can also be used when the concrete type is that needs to be injected is a subclass or an implementation of an interface that is used as the type-hint, which can be useful for having multiple implementations of an interface, and letting them being automatically injected based on an app configuration.</p><p><br></p><p><br></p>
------------------------------
Question: What is artisan in Laravel?
Answer: <p>Artisan is the command-line tool that comes with the framework. It allows you to do all sorts of development tasks like generating controllers and other common classes, generating and running migrations, generating encryption keys, clearing, and much more. Artisan also contains a special command called tinker, which is a REPL tool for PHP. Like every other Laravel library, Artisan is also extensible for creating and running your own commands.</p>
------------------------------
Question: What is composer in Laravel?
Answer: <p>Composer is the main package manager for PHP. Similar to npm in node, you can configure which packages your application depends on and install them with a single command. In addition to that composer also provides a built-in autoloader that automatically includes the necessary files in your application, removing the necessity of include or require statements in your code.</p><p><br></p><p>Laravel uses composer as its default package manager. In fact, when you create your Laravel application, apart from the initial files and folder structure, all internal Laravel files are maintained via composer and are located in the vendor folder. Upgrading your Laravel version is also done through composer, although it is important to follow the upgrade guide in Laravel's documentation.</p>
------------------------------
Question: What is a neural language model
Answer: <p>A neural language model is based on neural nets rather than the conventional way, which is based on n-grams, Markov models, and corpus counts. The neural language model uses word embedding to encode the prefix. in terms of which the next word is to be predicted. This allows for considerable flexibility. Even if the prefix (say, "the cat fetched the")  is followed by a word (say, "ball") that hasn't occurred in a prefix for that word in the corpus, because of analogies and similarity in word embedding between related words ("cat" and "dog"), the neural language model will assign a high probability to "ball" as the next word. The n-gram model will fail to do that.</p>
------------------------------
Question: What is ULMFiT?
Answer: <p>ULMFiT like Elmo and BERT is a transfer learning model for NLP. The basic idea is to develop a base model (usually trained for a language modelling task) and use that for all sorts of other NLP tasks by fine-tuning the base model in terms of the embedding for those specific tasks. This produces a significant improvement both in terms of accuracy and improvement required compared to developing a model from scratch for the specific tasks. Unlike earlier approaches using a base model which regards the base model as a fixed parameter, ULMFiT modifies (fine tunes) the base model in terms of the embedding for the specific task at hand.</p>
------------------------------
Question: What is knowhow system
Answer: <p>Knowhow System is the entire application behind KnowAngels.com. Any question you may have about this application, can be posed in the knowledge area we call 'Knowhow System'.</p>
------------------------------
Question: What is a neural network?
Answer: <p>A modern neural networks consists of an input layer of neurons, an output layer, and one or more hidden layers of neurons. The layers can be thought of as ordered from left to right. The neurons in one layer are connected to all or some neurons of the next layer with varying degrees of strength which are called weights. </p><p>There is an activation function which determines the value (strength of the signal) which is propagated from a neuron in an earlier layer to a neuron in the next layer to which it is connected depending on the value in the propagating neuron and the weight associated with the connection between them. </p><p>Depending on the application, the signals from the output layer may be modulated in terms of functions such as softmax, sigmoid, etc.</p><p>There is an objective function in terms of which the loss maybe computed (that is, the divergence between the predicted value emitted by the output layer and the the actual values given in the training set).</p><p>Backpropagation is used to adjust the weights in all the layers in such a way that gradually the loss is diminished at each step until the network comes up with a set of weights which yield output values that have a minimal or at least acceptable degree of loss.<img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/4713d35e341585d2dfd4a9af48f8b5a7.png"></p><p>Image from Wikipedia</p>
------------------------------
Question: What is a convolutional neural network?
Answer: <p>Convolutional neural networks (CNN) were designed to overcome a simple neural network's lack of a mechanism to take into account the near-term dependencies and influences in processing data. In a simple (fully-connected) neural network the value of every neuron is equally relevant in adjusting or evaluating the data (such as pixel value or a vector representation of a word) stored in a neuron. But there is a powerful intuition that certain neurons (the "closer" ones) are more relevant than other (the more "distant" ones). CNN were designed to let the neurons close to a neuron play a role in determining the value stored in that neuron. </p><p>CNNs accomplishes this by taking "snapshots" (sliding windows or filters) of the neurons around each neuron and uses the weights associated with those neurons in computing the value stored in that neuron in the next layer of the network.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/209337877c8eff313e88053db6deaa59.png"></p><p>(Image from Wikipedia)</p><p> `</p>
------------------------------
Question: What is the difference between Observers and Callbacks in Ruby on Rails?
Answer: <p>Observers have been removed from core in Rails 4.0. You have to now install it as a gem (https://github.com/rails/rails-observers).</p><p><br></p><p>Each model may have a corresponding observer class that responds to life cycle callbacks to implement trigger-like behavior outside the original class. This is a nice way to remove bloat from the model class, especially if the callbacks deal with behavior that is not core to that model's functions.</p><p><br></p><p>Callbacks, meanwhile, are implemented in the model class itself. It does add bloat to the class but, especially if the callbacks implement behavior essential to the core functions of the class, it may be nice for all of that model's behavior to be grouped together in one class. See https://guides.rubyonrails.org/active_record_callbacks.html for more information.</p><p><br></p><p>As an alternative to observers in newer versions of Rails, you may consider using concerns (https://api.rubyonrails.org/v6.1.0/classes/ActiveSupport/Concern.html) or a gem like Wisper (https://github.com/krisleech/wisper).</p><p><br></p>
------------------------------
Question: What are some pagination gems for Ruby/Rails?
Answer: <p>Some pagination gems you may consider using are:</p><ul><li>Kaminari: https://github.com/kaminari/kaminari</li><li>pagy: https://github.com/kaminari/kaminari</li><li>will_paginate: https://github.com/mislav/will_paginate</li></ul><p>They are all actively maintained.</p>
------------------------------
Question: What is virtual desktop infrasturcture?
Answer: <p><span style="color: rgb(0, 0, 0);">Virtual desktop infrastructure (VDI) is defined as the hosting of desktop environments on a central server. It can be regarded as a form of desktop virtualization, because resources are provided to run desktop images within&nbsp;virtual machines and these images can be accessed by end clients over a network. Those endpoints may be PCs or other devices, like tablets or thin client terminals.</span></p>
------------------------------
Question: How does virtual desktop infrastructure (VDI) work?
Answer: <p>The following characteristics are common to all virtual desktop setups:</p><ul><li>The virtual desktop is live within virtual machines (VM) on a centralized server</li><li>Each virtual desktop image contains an operating system image</li><li>End clients can maintain access the virtualized desktop only via a connection to the central server that hosts the virtualized desktop.</li><li>The VDI implementation’s connection broker finds a virtual desktop within the resource pool for each client to connect to upon its successful access of the VDI environment</li><li>Meanwhile, a hypervisor creates, runs and manages the various host machine VMs that encapsulate the individual virtual desktop environments</li></ul><p><br></p>
------------------------------
Question: What are the benefits of a virtualized desktop infrastructure?
Answer: <p>We can divide the benefits of using virtualized desktop infrastructure to the end client and to the IT department of an organization.</p><p><strong>Benefits to the end client:</strong></p><p><span style="color: rgb(0, 0, 0);">For workers who need to travel frequently VDI enables remote access, as a standardized desktop can be reached from almost any (supported) endpoint in any location. Such workers can pull up a virtual desktop containing a full range of virtual apps and data, </span></p><p><strong style="color: rgb(0, 0, 0);">Benefits to the IT department:</strong></p><ul><li><span style="color: rgb(0, 0, 0);">Since most of the processing is done by the centralized server, individual workers do not need to be provided with their own expensive computers.</span></li><li><span style="color: rgb(0, 0, 0);">Administrators can apply any patches and apply any policies at the level of the centralized server, which is lot simpler than doing this at the level of each computing device used by workers.</span></li></ul>
------------------------------
Question: What is a recurrent neural network?
Answer: <p>In a recurrent neural network the input (e.g., a string of tokens) is fed to a neuron in a sequence of time steps (e.g., the vector representation of a token at each time step). The vector representation sent at a time step <em>t </em>is then processed possibly through several hidden layers, modifying this vector, until it is emitted in the output layer. </p><p>Okay, so what do we do with it? We don't yet assign a label to it (e.g., 'spam')--labels are assigned only after the last piece of the input goes through this process. Instead, we take the vector output for time step t and feed it as input to the token processed at time step <em>t+1</em> (along with its own vector representation and a hidden state). </p><p>The idea here is that the output vectors at each time step play a role in creating a vector representation for the next step and through it for the step after that it and so on. The output vector representation for the final token will thus be influenced to some degree by the output vector representations of the earlier states. Clearly though as a token is further removed in time steps from the last token, its influence on the final output will be correspondingly less.</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/47c8a20996f8e0692060927bc7b68b55.png"></p><p>(Image from Kdnuggets.com)</p>
------------------------------
Question: What are the advantages of a recurrent neural network over a feed-forward neural network?
Answer: <p>In processing a string of token a feed-forward neural network will assign each token (or its vector representation) to a neuron. Thus, when it comes time to adjusting the weights (in the back propagation step) of neurons, every neuron will have equal influence on the weight adjustment of any neuron (because we want to minimize loss by making these adjustments and that is all we care about). Thus, the adjustment of weights to the first token in the sequence plays as much role in the adjustment of the last token as the tokens near the last one. But we have a strong intuition that in most cases nearby words have a far greater role in determining the  meanings (or vector representations) of a word than distant ones. But feed-forward neural networks do not have the mechanism for taking such dependencies into account.</p><p>On the other hand, recurrent neural networks, because they contain a feedback loop, can model such dependencies. Although the vector representation of every word that comes before a word play a role in determining the vector representation of that word, the words that come just before it play a greater role.</p><p>Secondly, feed-forward networks cannot handle strings of arbitrary length. Because the width of the network (the number of neurons in the input layer) is fixed in advance, it cannot processes sequences of a greater length than this width. But RNNs are not limited in this way. A sequence of arbitrary length <em>n </em>can be processed in <em>n</em> number of time steps.</p><p><br></p>
------------------------------
Question: What are the limitations of using recurrent neural networks?
Answer: <p>The primary disadvantage of recurrent neural networks in the context of natural language processing is that they are not good at capturing long distance dependencies. The influence (of the output) of a token (t<sub>i</sub>) earlier in the sequence on (the output of) a token being currently processed (t<sub>n</sub>) fades proportional to how many other tokens intervene between ti and tn. But there are many cases in which the two tokens may be closely related regardless of how many tokens intervene. For example, consider the sentence "The dog having found a bone on the trail ignored his master's commands." In this sentence, clearly, "ignored" and "the dog" are closely related in that "the dog" is the subject of the verb "ignored". RNNs are not good at interpreting such sentences in which there are long distance  dependencies.</p><p>LSTMs (Long Short-term Memory) and Transformers (and other attention-based models) are much better at interpreting sentences with long distance dependencies and, in general, are improvements over RNNs.</p>
------------------------------
Question: Can I use laravel as an api service?
Answer: <p>Yes, absolutely! Laravel has several features that facilitate its use as a backend API only, like automatically serializing models as JSON to supporting many types of API-based authentication methods. </p><p><br></p><p>For a more micro-service-oriented architecture, where services only have from one to a few responsibilities and speed is very important, Lumen might also be a good choice Lumen built using regular Laravel components, but with some more convenience or full-stack oriented libraries disabled for better performance. Because Lumen is based on Laravel, any of the disabled libraries can be re-enabled to fit each developer needs.</p>
------------------------------
Question: What is LSTM (Long Short-Term Memory) networks?
Answer: <p>LSTM networks are a type of recurrent neural networks with a memory which gets continually updated as each token in the input sequence is processed in such a way that the memory contains information about those tokens earlier in the sequence that are most relevant to processing the current input. By contrast, recurrent neural networks contain a (vector) representation of earlier tokens where the tokens closest to the current token dominate the ones that occurred earlier even if the earlier ones are more relevant to processing the current token than the ones closer to it. That is, the 'memory' in recurrent neural networks is updated in a fixed manner whereas the memory in LSTMs is updated in a more flexible manner so as to be most useful for processing the current token in the sequence.</p>
------------------------------
Question: What are the advantages of LSTM networks over RNNs?
Answer: <p>The main advantage of LSTM networks over recurrent neural networks is that they are better at handling long distance as well as near distance dependencies among the sequence of tokens being processed. However, they are both limited in that they can only handle dependencies between the current token (being processed) and tokens that occurred <em>earlier</em> in the sequence, but they cannot handle dependencies between the current token and tokens occurring <em>later</em> in the sequence.</p>
------------------------------
Question: What is a low-resource language for NLP applications?
Answer: <p>Many natural language processing applications for a language require machine learning and statistical methods. These methods in turn presuppose the availability of certain low-level resources for that language, such as a corpus of documents and utterances, a comprehensive lexicon (which, in turn, presupposes a tokenizer for that language), part-of-speech tagging, parsers, etc. For many languages such resources are simply not available at this point. Such languages are called low-resource languages. It turns out most languages at this point are low-resource to varying degrees. On the other hand, languages such as English and several European languages and Chinese and Japanese would fall into the high-resource category.</p>
------------------------------
Question: What is Spark Core
Answer: <p><span style="color: rgb(79, 93, 115);">Spark Core is the underlying general execution engine for the Spark platform that provides the foundation for all other functionalities of Spark. It provides critical functionalities such as task dispatching, scheduling, and input-output operations. It provides an API that defines and manipulate RDDs. All the basic functionality of Apache Spark Like&nbsp;in-memory computation,&nbsp;fault tolerance, memory management, monitoring, task scheduling is provided by Spark Core.  </span></p><p><span style="color: rgb(79, 93, 115);">Spark Core is exposed through APIs for Java, Scala, Python and R, which provide high-level operators for interacting with a Spark application.</span></p>
------------------------------
Question: What is the difference between narrow and wide transformations in Spark?
Answer: <p>A narrow transformation in Spark (such as, .filter()) is such that executing the transformation requires access only to a single data partition, whereas a wide transformation (sometimes called a 'shuffle') is one that requires access to data from multiple data partitions (groupBy(), for example). Best practice is to avoid using wide transformations to the extent possible. </p>
------------------------------
Question: What is lazy evaluation in Spark?
Answer: <p>Spark contains two types of instructions, transformations and actions. Issuing sequence of transformations does not result in the execution of any action until an action instruction is issued. We may say, transformations are executed lazily, meaning they do not result in any change to the data until an action instruction is called.</p><p>An example of transformation is the filter function which takes a function as a parameter; an example of an action is count().</p>
------------------------------
Question: Which deployment modes are supported by Spark?
Answer: <p>Spark supports two deploy modes: Client and Cluster. The difference has to do with where the driver program of a Spark application is run. In the client mode <span style="color: rgb(79, 93, 115); background-color: rgb(235, 237, 239);">the driver runs on the host where the job is submitted; whereas in the cluster mode, </span><span style="color: rgb(37, 37, 37);">driver program won't run on the machine from the job submitted but it will on the cluster as a sub-process of ApplicationMaster. </span></p><p><span style="color: rgb(37, 37, 37);">The default value for deployment mode is client mode.</span></p>
------------------------------
Question: What are the comparative advantages and disadvantages of the client mode versus the cluster mode in Spark?
Answer: <p><span style="color: rgb(37, 37, 37);">The primary disadvantage of using the client mode is its dependence on the driver program functioning: if the driver program fails, the entire job will fail. On the other hand, in the cluster mode in case of failure of the driver program it can be re-instantiated because the driver program is running in the ApplicationMaster.</span></p><p><br></p><p><span style="color: rgb(37, 37, 37);">One advantage of running a job in the client program is that it can more efficiently allocate resources because it uses YARN to allocate resources.</span></p>
------------------------------
Question: What is an ApplicationMaster in Spark?
Answer: <p><span style="color: rgb(79, 93, 115);">In Spark (as in other distributed systems), the ApplicationMaster is responsible for negotiating between the application driver and the ResourceManager (which is the cluster manager) for accessing the resources necessary to run tasks and store the data necessary to run the application.</span></p>
------------------------------
Question: What is a Spark executor?
Answer: <p><span style="color: rgb(79, 93, 115);">Executors are processes running in worker nodes. They are launched at the beginning of a Spark application and their job is to run the separate tasks in a Spark application and after all the tasks are completed they send the results to the driver that invoked the executor. </span></p>
------------------------------
Question: What is a Spark cluster manager?
Answer: <p><span style="color: rgb(79, 93, 115);">In general a cluster manager manages the allocation and deallocation of nodes in a cluster (of nodes) and the execution of tasks for an application to complete its job.</span>&nbsp;</p><p>Spark&nbsp;supports three types of cluster managers:</p><ol><li>Standalone cluster manager</li><li>Hadoop Yarn</li><li>Apache Mesos</li></ol><p><br></p>
------------------------------
Question: Can Kubernetes run on Windows?
Answer: <p><span style="color: rgb(79, 93, 115);">"</span><span style="color: rgb(34, 34, 34);">To enable the orchestration of Windows containers in Kubernetes, include Windows nodes in your existing Linux cluster...</span><span style="color: rgb(79, 93, 115);">In order to run Windows containers, your Kubernetes cluster must include multiple operating systems, with control plane nodes running Linux and workers running either Windows or Linux depending on your workload needs. Windows Server 2019 is the only Windows operating system supported, enabling Kubernetes Node on Windows (including kubelet, container runtime, and kube-proxy)."---From </span>https://kubernetes.io/docs<span style="color: rgb(79, 93, 115);">&nbsp;</span></p>
------------------------------
Question: What does the dd() function do in laravel?
Answer: <p>The dd() function stands for "dump and die", and is designed to be a helper function while developing your application. The function dumps the content of a variable in the screen and exits the application, preventing any further execution. It is a great way to inspect the contents of a variable or object in which you are not clear how its data is structured.</p>
------------------------------
Question: What is a csrf token in laravel?
Answer: <p>Cross-Site Request Forgery or CSRF is a common vector of attack for web applications, where a malicious actor creates a site that when loaded or interacts with sends a request to your application from the user's browser, in other to obtain or temper with that user's private information. Because the request is made through the user's browser, any authentication cookies that your application stored in the user's browser will be sent along with the request, forging what would look like an authentic request from an authenticated user.</p><p><br></p><p>The most common way to prevent cross-site request forgery (CSRF) attacks is to validate any non-GET request by looking for a secret token that matches a token stored in that user's session. Because the token is stored in the user's session data, the malicious website will be unable to access that token and subsequently forge a valid request.</p><p><br></p><p>In Laravel, all of that is done automatically for you. Any POST, PUT, PATCH, or DELETE request is by default only accepted if it contains a valid CSRF token along with its data. All you have to do is to make a call to the csrf_field() function in any form on your site, to generate a hidden _token field with a valid CSRF token. The csrf_token() function can also be to generate the token alone (outside of a form hidden field)</p>
------------------------------
Question: What is the cursor() method in an eloquent model in laravel?
Answer: <p>The cursor() method is a more memory-efficient way to iterate through a large amount of data returned from a database query. Unlike the get() method that loads all the results from the query to the memory in a single collection object, the cursor method returns a cursor object that can be used to access one record of in the result set at a time, greatly improving memory efficiency, while not needing to run multiple queries.</p>
------------------------------
Question: What are relationships in laravel eloquent?
Answer: <p>Eloquent relationships define how one model is related to one another. They are defined as a method in your model's class and defines the type of relationship, the related model, and how these models are linked, via foreign and primary keys. </p><p>This method can later be used to easily build join queries between related models, or even be accessed as members of the class directly.</p><p><br></p><p>A classic example is in a Blog application, you would likely have a model for posts and another for comments. These two models are related to one another such that each post may contain multiple comments. This type of relationship is called a One-to-Many relationship. In the Post model, you would define the relationship as:</p><p><br></p><p>public function comments() {</p><p>	return $this-&gt;hasMany(Comment::class)</p><p>}</p><p><br></p><p>Now from an instance of a Post model, you can call $post-&gt;comments, and it will return automatically returns a collection of posts that are related to that model. You can also call it as a method to create a query builder in order to filter out more comments, for example, $post-&gt;comments()-&gt;where('user_id', $user-&gt;id)-&gt;get(). By default it will use the model name followed by "_id" as the foreign key that relates both models, so in the example would be "post_id". This can be customized by passing a custom foreign key name as a parameter to the relationship function.</p>
------------------------------
Question: What is docker?
Answer: <p>Docker is a platform for packaging applications into containers.&nbsp;<span style="color: rgb(77, 81, 86);">Containers are isolated from one another and bundle their own software, libraries.</span></p>
------------------------------
Question: What is the advantage of using attention layers in NLP pipelines
Answer: <p><span style="color: rgb(79, 93, 115);">Attention mechanism has revolutionized the way we create NLP models and is currently a standard fixture in most state-of-the-art NLP models, it enables the model to “remember” all the words in the input and focus on specific words when formulating each part of its response. The intuition is that as the task of producing a response proceeds different parts of the encoding of the input becomes relevant. Thus, by being able to shift attention as the task proceeds, the output is more likely to be accurate.</span></p><p><span style="color: rgb(79, 93, 115);">It </span>was first introduced in NLP for machine translation tasks and due to the performance improvement over traditional networks, it has been widely accepted as a good addition to modern day NLP pipelines.</p>
------------------------------
Question: What is the difference between GPT-2 and GPT-3
Answer: <p><span style="color: rgb(79, 93, 115);">The main difference between GPT-3 and its predecessor, GPT-2 is its size. </span><span style="color: rgb(41, 41, 41);">GPT-3, a newer language model is more than 100 times larger than GPT-2, with 175 billion parameters and 96 layers trained on a corpus of 499 billion tokens of web content. The </span><span style="color: rgb(79, 93, 115);">GPT-2 model comparatively had 1.5 billion parameters and 48 layers. Interestingly, people are unable to distinguish GPT-3 generated news stories from real ones, only exacerbating the ethical concerns already raised by GPT-2. OpenAI's cutting edge GPT-3 which has been trained using text from the internet, also has some gender and racial bias issues and is unaware of things happening after 2019 such as the COVID-19 pandemic.</span></p>
------------------------------
Question: What is T5 in NLP
Answer: <p><span style="color: rgb(64, 64, 64); background-color: rgb(252, 252, 252);">Google's T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks. </span><span style="color: rgb(79, 93, 115);">BERT-like models take a text sequence as an input and output a single class label or a span of text from the input. A relevant output layer is added to suit the needs of the the particular NLP task each time in BERT-like models. However, T5 instead reframes any NLP task such that both the input and the output are text sequences which allows the same T5 model to be used for any NLP task, without any aftermarket changes to the architecture, thereby reducing its complexity. It treats a wide variety of many-to-many and many-to-one NLP tasks in a unified manner by encoding the different tasks as text directives in the input stream. This enables a single model to be trained supervised on a wide variety of NLP tasks such as translation, classification, summarization etc. The </span><span style="color: rgb(64, 64, 64); background-color: rgb(252, 252, 252);">T5 model works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task, e.g., translating English to German.</span></p>
------------------------------
Question: What is Chomsky Normal Form?
Answer: <p><span style="color: rgb(32, 33, 34);">In formal language theory, every grammar in Chomsky normal form is context-free, and conversely, every context-free grammar can be transformed into an equivalent&nbsp;one&nbsp;which is in Chomsky normal form and has a size no larger than the square of the original grammar's size. </span>A CFG(context free grammar) is in CNF(Chomsky normal form) if all production rules satisfy one of the following conditions:</p><ul><li>Start symbol, S, generating ?. For example, S ? ?.</li><li>A non-terminal generating two non-terminals. For example, C ? AB.</li><li>A non-terminal generating a terminal. For example, C? a.</li></ul><p><br></p><p>The advantage of converting a CFG into Chomsky Normal Form is that then we can create a binary parse tree for every sentence recognized by the grammar.</p>
------------------------------
Question: what are transformers in natural language processing
Answer: <p><span style="color: rgb(89, 88, 88);">The Transformer in NLP is a novel architecture that aims to solve sequence-to-sequence tasks while handling long-range dependencies with ease. I</span><span style="color: rgb(41, 41, 41);">t differs from the previously existing sequence-to-sequence(Seq2Seq) models because it does not imply any Recurrent Networks (GRU, LSTM, etc.).</span><span style="color: rgb(89, 88, 88);"> </span><span style="color: rgb(79, 93, 115);">In context to NLP, transformer based language models have shown better language understanding abilities to achieve state-of-the-art&nbsp;results in multiple tasks. Transformers are designed to handle sequential data, such as natural language, for tasks such as translation and text summarization. However, unlike traditional neural networks, transformers do not require that the sequential data to be processed before sending as input data, which has </span><span style="color: rgb(32, 33, 34);">enabled training on larger datasets.</span><span style="color: rgb(79, 93, 115);"> For instance, if the input data is a natural language sentence, the transformer architecture based models do not need to process the beginning of it before the end. Due to this feature, the transformers allows for much more parallelization than traditional networks resulting in much lesser training times. Pre-trained models such as BERT and GPT have leveraged the transformer architecture and been trained on massive datasets to improve accuracy and reduce training times substantially.</span></p>
------------------------------
Question: What is the textrank algorithm for keyword extraction
Answer: <p><span style="color: rgb(79, 93, 115);">TextRank is a graph-based ranking algorithm under the hood for ranking chunks of text segments in order of their importance in the text document. It can be used for keyword and sentence extraction and revolves around the idea of representing the concerned lexical unit in the form of graphs and later using the famous PageRank algorithm to rank those chunks. The edge of </span>TextRank over traditional techniques, is that it does not require deep linguistic knowledge, nor domain or language specific annotated corpora, which makes it highly portable to other domains, genres and languages. The TextRank keyword extraction algorithm is fully unsupervised, and proceeds as follows. First the text is tokenized, and annotated with part of speech tags – a preprocessing step required to enable the application of syntactic filters. Next, all lexical units that pass the syntactic filter are added to the graph, and an edge is added between those lexical units that co-occur within a window of word. Once a final score is obtained for each vertex in the graph, vertices are sorted in reversed order of their score, and the top vertices in the ranking are retained for post-processing. During post-processing, all lexical units selected as potential keywords by the algorithm are marked in the text, and sequences of adjacent keywords are collapsed into a multi-word keyword. The origin of the TextRank algorithm for text extraction, can be referenced <a href="https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf" rel="noopener noreferrer" target="_blank">here</a>.</p>
------------------------------
Question: What is cross-entropy in deep learning?
Answer: <p><span style="color: rgb(85, 85, 85);">Cross-entropy is commonly used in deep learning as a loss function. It </span><span style="color: rgb(64, 64, 64); background-color: rgb(252, 252, 252);">measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy increases as the predicted probability diverge from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. Log loss although different from cross-entropy is used interchangeably with one another for classification problems. A perfect model would have a log loss of 0. </span>Cross-entropy is different from KL divergence (Kullback-Leibler divergence)  but can be calculated using KL divergence, and is different from log loss but calculates the same quantity when used as a loss function.</p>
------------------------------
Question: What is the difference between RoBERTa and distillBERT?
Answer: <p>RoBERTa and DistilBERT are both retrained versions of the original BERT model using more data and different parameters.</p><p><span style="color: rgb(17, 17, 17);">&nbsp;RoBERTa, introduced at Facebook, is a retraining of BERT with improved training methodology, 1000% more data, and compute power. To improve the training procedure, RoBERTa removes the Next Sentence Prediction (NSP) task from BERT’s pre-training and introduces dynamic masking so that the masked token changes during the training epochs. Larger batch-training sizes were also found to be more useful in the training procedure. </span>Importantly, RoBERTa uses 160 GB of text for pre-training, including 16GB of Books Corpus and English Wikipedia used in BERT. On the other hand, distilBERT<span style="color: rgb(17, 17, 17);">&nbsp;learns a distilled (approximate) version of BERT, retaining 95% performance but using only half the number of parameters. Specifically, it does not has token-type embeddings, pooler, and retains only half of the layers from Google’s BERT. DistilBERT uses a technique called distillation, which approximates Google’s BERT, i.e. the large neural network by a smaller one.&nbsp;In reality, the choice between the models depends heavily on the use case. If you really need a faster inference speed but can compromise few-% on prediction metrics, DistilBERT is a starting reasonable choice, however, if you are looking for the best prediction metrics, you’ll be better off with Facebook’s RoBERTa owing to higher performance at the cost of high computational complexity.</span></p>
------------------------------
Question: What is distillBERT
Answer: <p>DistilBERT is a small, fast, cheap, and light Transformer model based on the BERT architecture. It is a general-purpose pre-trained version of BERT, 40% smaller, 60% faster, that retains 97% of the language understanding capabilities. Knowledge distillation is performed during the pre-training phase to reduce the size of a BERT model by 40%. To leverage the inductive biases learned by larger models during pre-training, the authors of the paper<a href="https://arxiv.org/pdf/1910.01108v4.pdf" rel="noopener noreferrer" target="_blank"> DistilBERT, a distilled version of BERT: smaller, faster, cheaper, and lighter</a>, introduce a triple loss combining language modeling, distillation, and cosine-distance losses. The paper introduced DistilBERT and its purpose in the world where newer pre-trained models are becoming too complex and computationally expensive to try and improve performance by small margins. The main idea behind this model is that, once a large neural network has been trained, its full output distributions can be approximated using a smaller network. In cases where a faster inference speed is required, but few-% prediction metrics can be compromised, DistilBERT is a valid choice and a compelling option specifically for edge applications.</p>
------------------------------
Question: What is an activation function in deep learning
Answer: <p>Activation functions are a critical part of the design of a neural network. The choice of activation function in the hidden layer will control how well the network model learns the training dataset and the type of predictions the model can make. A<span style="color: rgb(79, 93, 115);">s such, a careful choice of activation function must be made for each deep learning neural network project. Some of the famous activation functions, commonly used are </span><strong style="color: rgb(64, 66, 78);">Step Function, Sigmoid Function, ReLU, </strong><span style="color: rgb(64, 66, 78);">and </span><strong style="color: rgb(64, 66, 78);">Leaky ReLU. T</strong><span style="color: rgb(32, 33, 34);">he&nbsp;activation function&nbsp;of a node defines the output of that node given an input or set of inputs. For instance, a</span> standard integrated circuit can be seen as a digital network of activation functions that can be "ON" (1) or "OFF" (0), depending on the input.</p>
------------------------------
Question: What is reinforcement deep learning
Answer: <p><span style="color: rgb(77, 78, 79);">Deep reinforcement learning combines artificial neural networks with a reinforcement learning (RL) architecture that enables software-defined agents to learn the best actions possible in a virtual environment in order to attain their goals, that is it unites function approximation and target optimization, mapping state-action pairs to expected rewards.</span><span style="color: rgb(32, 33, 34);"> </span>Deep reinforcement learning has been used for a diverse set of applications including but not limited to robotics, video games, natural language processing, computer vision, education, transportation, finance, and healthcare. At the highest level, there are two types of deep RL approaches, they are:</p><ul><li>In <strong><u>model-based</u></strong> deep reinforcement learning algorithms, a forward model of the environment dynamics is estimated, usually by supervised learning using a neural network. Then, actions are obtained by using model predictive control using the learned model.&nbsp;</li><li>In <strong><u>model-free</u></strong> deep reinforcement learning algorithms, a policy is learned without explicitly modeling the forward dynamics. A policy can be optimized to maximize returns by directly estimating the policy gradient but suffers from high variance, making it impractical for use with function approximation in deep RL.</li></ul><p>Deep reinforcement learning reached a milestone in 2015 when AlphaGo, a computer program trained with deep RL to play Go, became the first computer Go program to beat a human professional Go player without handicap on a full-sized 19×19 board.</p>
------------------------------
Question: when should tanh be used instead of Relu
Answer: <p>The range of Tanh is all real numbers [-1,1], and ReLU is in the range of [0, infinity]. Rectified Linear Unit or (ReLU) operates on max(0,x), which means that anything less than zero will be returned as 0 and linear with the slope of 1 when the values are greater than 0. Tanh on the other hand works well for implicit state activation functions of GRU and LSTMs. Tanh has the problem of gradient disappearance, although it’s much smaller than sigmoid, comparatively, there is no vanishing gradient problem in the positive half of ReLU, however, there is a “gradient disappearance” in the negative half.  A four-layer CNN with ReLU activation function reaches a 25% training error rate on CIFAR-10, six times faster than an equivalent network with tanh neurons. The biggest advantage of ReLu is indeed the non-saturation of its gradient, which greatly accelerates the convergence of stochastic gradient descent compared to the tanh activation function.&nbsp;Tanh should be preferred when, the negative inputs need to be mapped strongly, and the zero inputs need to be mapped near zero.</p>
------------------------------
Question: What is convolutional autoencoding in deep learning
Answer: <p>The convolution operator allows filtering an input signal in order to extract some part of its content. Autoencoders(AEs) in their traditional formulation do not take into account the fact that a signal can be seen as a sum of other signals. Convolutional Autoencoders, instead, use the convolution operator to exploit this observation. They learn to encode the input in a set of simple signals and then try to reconstruct the input from them. Convolutional Autoencoders (CAEs) approach the filter definition task from a different perspective: instead of manually engineer convolutional filters we let the model learn the optimal filters that minimize the reconstruction error. These filters can then be used in any other computer vision task. CAEs are the state-of-art tools for unsupervised learning of convolutional filters. Once these filters have been learned, they can be applied to any input in order to extract features. Due to their convolutional nature, scale well to realistic-sized high-dimensional images because the number of parameters required to produce an activation map is always the same, no matter what the size of the input is. Therefore, CAEs are general-purpose feature extractors differently from AEs that completely ignore the 2D image structure. AEs introduce redundancy in the parameters, forcing each feature to be global (i.e., to span the entire visual field), while CAEs do not.</p>
------------------------------
Question: What is statistical natural language processing
Answer: <p>Natural language processing (NLP) is a field of artificial intelligence concerned with the interactions between computers and human (natural) languages. It refers to a technology that creates and implements ways of executing various tasks concerning natural language (such as designing natural language-based interfaces with databases, machine translation, etc.). The statistical dominance of the field also often leads to NLP being described as Statistical Natural Language Processing, perhaps to distance it from the classical computational linguistics methods. Statistical NLP has been the most widely used term to refer to nonsymbolic and nonlogical work on NLP over the past decade. It comprises all quantitative approaches to automated language processing, including probabilistic modeling, information theory, and linear algebra. Roughly speaking, statistical NLP performs statistical analysis on a corpus of documents and uses such statistical analysis to compute probabilities of the alternative interpretations  or properties of an input text  and  accepts the most probable outcome as the correct one.&nbsp;</p>
------------------------------
Question: What are actor-critic methods in deep learning?
Answer: <p>Actor-critic approaches have grown in popularity as an effective means of combining the benefits of policy search methods with learned value functions, which are able to learn from full returns and/or TD errors. They can benefit from improvements in both policy gradient methods, such as GAE, and value function methods, such as target networks. </p><ul><li>The “Critic” estimates the value function. This could be the action-value (the Q value) or state-value (the V value).</li><li>The “Actor” updates the policy distribution in the direction suggested by the Critic (such as with policy gradients).</li></ul><p><span style="color: rgb(41, 41, 41);">and both the Critic and Actor functions are parameterized with neural networks. The pseudo-code for actor-critic approach is as follows:</span></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/41b4dea8587d65b56a38a58e08f588d2.png"></p><p><br></p><p><span style="color: rgb(41, 41, 41);">As illustrated, we update both the Critic network and the Value network at each update step.</span></p><p><br></p><p>In any event, actor-critic methods are likely to remain of current interest because of two significant apparent advantages:</p><ul><li>They require minimal computation in order to select actions. Consider a case where there are an infinite number of possible actions--for example, a continuous-valued action. Any method learning just action values must search through this infinite set in order to pick an action. If the policy is explicitly stored, then this extensive computation may not be needed for each action selection.</li><li>They can learn an explicitly stochastic policy; that is, they can learn the optimal probabilities of selecting various actions. This ability turns out to be useful in competitive and non-Markov cases.</li></ul><p>In addition, the separate actor in actor-critic methods makes them more appealing in some respects as psychological and biological models. In some cases, it may also make it easier to impose domain-specific constraints on the set of allowed policies.</p>
------------------------------
Question: What is variational autoenconding?
Answer: <p><span style="color: rgb(41, 41, 41);">The general idea of autoencoders is pretty simple and consists in&nbsp;</span><strong style="color: rgb(41, 41, 41);">setting an encoder and a decoder as neural networks</strong><span style="color: rgb(41, 41, 41);">&nbsp;and learning</span><strong style="color: rgb(41, 41, 41);"> the best encoding-decoding scheme using an iterative optimization process</strong><span style="color: rgb(41, 41, 41);">. </span><span style="color: rgb(79, 93, 115);">Variational autoencoders (VAEs) are a group of generative models in the field of deep learning and neural networks.&nbsp;They have also been used to draw images, achieve state-of-the-art results in semi-supervised learning, as well as to interpolate between sentences. They let us design complex generative models of data, and fit them into large datasets. They can generate images of fictional celebrity faces and high-resolution digital artwork. By definition, </span><strong style="color: rgb(41, 41, 41);">a variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable the generative process. </strong>Just as a standard autoencoder, a variational autoencoder is an architecture composed of both an encoder and a decoder and that is trained to minimize the reconstruction error between the encoded-decoded data and the initial data. However, in order to introduce some regularisation of the latent space, a slight modification of the encoding-decoding process:&nbsp;<strong>instead of encoding an input as a single point,  it is encoded as a distribution over the latent space</strong>(data compression from initial space to encoded space). The model is then trained as follows:</p><ul><li>first, the input is encoded as a distribution over the latent space</li><li>second, a point from the latent space is sampled from that distribution</li><li>third, the sampled point is decoded and the reconstruction error can be computed</li><li>finally, the reconstruction error is backpropagated through the network</li></ul><p>In variational autoencoders, the loss function is composed of a reconstruction term (that makes the encoding-decoding scheme efficient) and a regularisation term (that makes the latent space regular). The VAEs can be differentiated from normal autoencoders as : </p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/60746c45911fe6d8465bcac5071937ac.png"></p><p><br></p>
------------------------------
Question: What is a Q-learning policy
Answer: <p>Q-learning is a reinforcement learning technique that is used for learning the optimal policy in a Markov Decision Process(MDP). A Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision-making in situations where outcomes are partly random and partly under the control of a decision-maker. MDPs are useful for studying optimization problems solved via dynamic programming. The objective of Q-learning is to find a policy that is optimal in the sense that the expected value of the total reward over all successive steps is the maximum achievable. So, in other words, the goal of Q learning is to find the optimal policy by learning the optimal Q-values for each state-action pair. The ‘q’ in q-learning stands for quality. Quality in this case represents how useful a given action is in gaining some future reward. For any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly random policy. The Q-function uses the Bellman equation and takes two inputs: state (s) and action (a). <img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/753aa272cfae6c2d3329fbe9f9e37b31.png"></p><p><strong><u>The algorithm for the Q-learning process is as follows:</u></strong></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/753aa272cfae6c2d3329fbe9f9e37b31.png"></p>
------------------------------
Question: What are generative adversarial networks?
Answer: <p><span style="color: rgb(79, 93, 115);">Generative Adversarial Networks (GANs) were introduced in 2014 by Ian J. Goodfellow and co-authors. GANs are algorithmic architectures that use two neural networks, pitting one against the other (thus the “adversarial”) in order to generate new, synthetic instances of data that can pass for real data. They are used widely in image generation, video generation, and voice generation.GANs perform unsupervised learning tasks in machine learning.&nbsp;A GAN is a deep neural network framework that is able to learn from a set of training data and generate new data with the same characteristics as the training data. For example, a generative adversarial network trained on photographs of human faces can generate realistic-looking faces which are entirely fictitious. </span>A generative adversarial network is made up of two neural networks:</p><ul><li><strong>The generator </strong>learns to produce realistic fake data from a random seed. The fake examples produced by the generator are used as negative examples for training the discriminator.</li><li><strong>The discriminator </strong>learns to distinguish the fake data from realistic data. If the generator produces implausible results, the discriminator penalizes the generator.</li></ul><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/c6983877f39e6ccc580762ff5a5f195e.png"></p><p>Initially, before training has begun, the generator’s fake output is very easy for the discriminator to recognize. Since the output of the generator is fed directly into the discriminator as input, this means that when the discriminator classifies an output of the generator, the backpropagation algorithm can be applied through the whole system and the generator’s weights can be updated. Over time, the generator’s output becomes more realistic and the generator gets better at fooling the discriminator. Eventually, the generator’s outputs become so realistic, that the discriminator is unable to distinguish them from the real examples. This is the end goal of the adversarial network (GAN).</p><p><br></p><p>For more details regarding GANs, their working principle, and the math behind them, refer<a href="https://deepai.org/machine-learning-glossary-and-terms/generative-adversarial-network" rel="noopener noreferrer" target="_blank"> here</a>.</p><p><br></p>
------------------------------
Question: What is SVM?
Answer: <p>Support Vector Machine (SVM) is a relatively simple Supervised Machine Learning Algorithm used for classification and/or regression. It is more preferred for classification but is sometimes very useful for regression as well. Basically, SVM finds a hyper-plane that creates a boundary between the types of data. Hyperplanes are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.&nbsp;In 2-dimensional space, this hyper-plane is nothing but a line. In SVM, each data item is plotted in the dataset in an N-dimensional space, where N is the number of features/attributes in the data. Next, the optimal hyperplane is identified to separate the data. Inherently, since there is an optimal hyper-plane and multiple data points, by default SVM can only perform binary classification however there are multiple ways to use SVM for multi-class classification. </p><p>The computations of data points separation depend on a kernel function. There are different kernel functions: Linear, Polynomial, Gaussian, Radial Basis Function (RBF), and Sigmoid. Simply put, these functions determine the smoothness and efficiency of class separation, and playing around with their hyperparameters may lead to overfitting or underfitting.&nbsp;</p><p>For multi-class problems, the idea is to map data points to high dimensional space to gain mutual linear separation between every two classes. This is called a One-to-One approach, which breaks down the multiclass problem into multiple binary classification problems. A binary classifier per each pair of classes.</p><p>Another approach one can use is One-to-Rest. In that approach, the breakdown is set to a binary classifier per each class.</p><p><br></p><p>A single SVM does binary classification and can differentiate between two classes. So that, according to the two breakdown approaches, to classify data points from <img src="https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-8d7776a954bd6cdfe352eeb868b388ab_l3.svg" alt="\pmb{m}" height="8" width="16"> classes data set:</p><ul><li>In the&nbsp;<em>One-to-Rest</em>&nbsp;approach, the classifier can use&nbsp;<span style="background-color: initial;"><img src="https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-8d7776a954bd6cdfe352eeb868b388ab_l3.svg" alt="\pmb{m}" height="8" width="16"></span>&nbsp;SVMs. Each SVM would predict membership in one of the&nbsp;<span style="background-color: initial;"><img src="https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-8d7776a954bd6cdfe352eeb868b388ab_l3.svg" alt="\pmb{m}" height="8" width="16"></span>&nbsp;classes.</li><li>In the&nbsp;<em>One-to-One</em>&nbsp;approach, the classifier can use&nbsp;<span style="background-color: initial;"><img src="https://www.baeldung.com/wp-content/ql-cache/quicklatex.com-934df0b99ae15a7dc1ee9c0811337f72_l3.svg" alt="\pmb{\frac{m (m-1)}{2}}" height="25" width="54"></span>&nbsp;SVMs.</li></ul><p><br></p>
------------------------------
Question: What is the difference between bias and variance in machine learning models?
Answer: <p><span style="color: rgb(79, 93, 115);">Bias and variance are used in supervised machine learning, in which an algorithm learns from training data or a sample data set of known quantities. The correct balance of bias and variance is vital to building machine-learning algorithms that create accurate results from their models. So what does this mean? In general, one could say that a high variance is proportional to the overfitting and a high bias is proportional to the underfitting.</span></p><p><strong style="color: rgb(79, 93, 115);"><u>BIAS:</u></strong><span style="color: rgb(79, 93, 115);"> </span><span style="color: rgb(41, 41, 41);">Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. A model with high bias pays very little attention to the training data and oversimplifies the model. It always leads to a high error on training and test data.</span></p><p><strong style="color: rgb(41, 41, 41);"><u>VARIANCE:</u> </strong><span style="color: rgb(41, 41, 41);">Variance is the variability of model prediction for a given data point or a value that tells us the spread of our data.&nbsp;A model</span> with high variance is overly tuned to training data and, therefore, doesn't generalize on the data which it hasn’t seen before.<span style="color: rgb(41, 41, 41);">&nbsp;As a result, such models perform very well on training data but have high error rates on test data. </span></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/c543fdbaeeaf7a6841f580627b6f5ef5.png">If a model is too simple and has very few parameters then it may have high bias and low variance. On the other hand, if the model has a large number of parameters then it’s going to have high variance and low bias. So a right/good balance needs to be identified without overfitting or underfitting the data. Therefore understanding bias and variance is critical for understanding the behavior of prediction models.</p>
------------------------------
Question: What is a convolutional neural network?
Answer: <p>Convolutional neural networks (CNN) were designed to overcome a simple neural network's lack of a mechanism to take into account the near-term dependencies and influences in processing data. In a simple (fully-connected) neural network the value of every neuron is equally relevant in adjusting or evaluating the data (such as pixel value or a vector representation of a word) stored in a neuron. But there is a powerful intuition that certain neurons (the "closer" ones) are more relevant than other (the more "distant" ones). CNN were designed to let the neurons close to a neuron play a role in determining the value stored in that neuron.</p><p>In the context of computer vision there is the intuition that visual data is often hierarchical, amenable to being organized in terms of smaller shapes, lines, corners, etc., which can be combined into larger figures. So certain clusters of neurons representing the pixels that belong to these shapes, lines, etc., should be more relevant to interpreting each other.</p><p>CNNs accomplishes this by taking "snapshots" (sliding windows or filters) of the neurons around each neuron and uses the weights associated with those neurons in computing the value stored in that neuron in the next layer of the network.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/209337877c8eff313e88053db6deaa59.png"></p><p>(Image from Wikipedia)</p><p>`</p><p><br></p>
------------------------------
Question: What are contextualized word embeddings
Answer: <p>Simply speaking, word embeddings are a type of word representation that allows words with similar meanings to have a similar representation. By definition, word embeddings are dense vector representations of words in lower-dimensional space. After translating a word to an embedding, it becomes possible to model the semantic importance of the word in a numeric form and thus perform mathematical operations on it. Word embedding methods learn a real-valued vector representation for a predefined fixed-sized vocabulary from a corpus of text. Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus.</p><p><br></p><p>Contextual embeddings, such as ELMo and BERT, go beyond global word representations like Word2Vec and achieve ground-breaking performance on a wide range of natural language processing tasks. Contextual word embedding, as the name suggests, assigns each word a representation based on its context, thereby capturing uses of words across varied contexts and encodes knowledge that transfers across languages, significantly improving every NLP task.</p><p>Context is very important when dealing with human language, training the models to retain, and use the context to learn information is very useful and works wonders compared to traditional, context-free word embeddings. Some examples of context-based word-embeddings in action can be witnessed in the image below, along with the importance of context for understanding a natural sentence.</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/6b6f7953662ff633eb2df87c7c4f30cd.png"></p><p><br></p><p><span style="color: rgb(41, 41, 41);">In this example, the embeddings for the word</span><u style="color: rgb(41, 41, 41);"> “bank”</u><span style="color: rgb(41, 41, 41);">, when it means a financial institution are far from the embeddings for it when it means a riverbank or the verb form of the word.</span></p><p><br></p>
------------------------------
Question: What is XLNet in natural language processing?
Answer: <p>XLNet was proposed in 2020 <span style="color: rgb(79, 93, 115);"> by the researchers from Carnegie Mellon University and Google Brain team</span> in the paper "<a href="https://arxiv.org/abs/1906.08237" rel="noopener noreferrer" target="_blank"><strong>XLNet: Generalized Autoregressive Pretraining for Language Understanding</strong></a>". According to the authors, it is "a generalized autoregressive pretraining method that&nbsp;(1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation."&nbsp;</p><p>In practice, XLNet is known to outperform BERT on 20 NLP benchmark tasks, usually with a large margin due to its ability to overcome the limitations of BERT using autoregressive formulation. It leverages the best of both autoregressive (AR) language modeling and autoencoding (AE), the two most well-known pretraining objectives while avoiding their limitations. Approaches such as BERT and ELMo improved on the then state of the art models, by incorporating both left and right contexts into predictions. XLNet took this a step further by making sure that the model's contribution is to predict each word in a sequence using any combination of other words in that particular sequence. In practice, XLNet samples from all possible permutations, so it doesn't get to see every relation; however, the likelihood of possible permutations is taken deeply into consideration while also preserving the contextual information.</p>
------------------------------
Question: What is the difference between logistic regression and linear regression?
Answer: <p>Regression is a supervised learning technique that helps in finding the correlation between variables and enables us to predict the continuous output variable based on one or more predictor variables. Linear and logistic regression are two of the most commonly used techniques of regression analysis used for analyzing a data set in finance and investing and help managers to make informed decisions.&nbsp;</p><p>Linear regression is the most simple and popular technique for predicting a continuous variable. It assumes a linear relationship between the outcome and the predictor variables.</p><p>Logistic regression, on the other hand, is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval, or ratio-level independent variables</p><p><u>The most notable differences between these are:</u></p><p><br></p><p><u>USE-CASE:</u> Logistic regression is used when the dependent variable can take a small set of values (usually two), Thus, logistic regression is used for classification. Linear regression is used when the dependent variable is continuous and the nature of the regression line is linear.</p><p><br></p><p><u>LINEAR RELATIONSHIP:</u> The dependent and independent variables don't need to be linearly related to one another in the case of Logistic regression, however in contrast Linear Regression cannot perform if there is no linear relation between the two variables.</p><p><br></p><p><u>COMPUTATION</u>: Linear Regression <span style="color: rgb(68, 68, 68);">is a simple process based on the </span><u style="color: rgb(68, 68, 68);">Least&nbsp;Square Estimation Method</u><span style="color: rgb(68, 68, 68);"> and therefore, takes relatively less time to compute when compared to logistic regression.&nbsp;In contrast, Logistic Regression is an iterative process utilizing the maximum likelihood estimation method and requires a relatively longer computation time</span></p><p><br></p><p><br></p>
------------------------------
Question: What is expected maximization (EM) algorithm?
Answer: <p><span style="color: rgb(79, 93, 115);">The Expectation-Maximization (EM) algorithm is a way to find </span><em style="color: rgb(79, 93, 115);">maximum-likelihood estimates</em><span style="color: rgb(79, 93, 115);"> for model parameters when the data is incomplete, has missing data points, or has unobserved (hidden) latent variables. According to Wikipedia, the expectation-maximization (EM) algorithm is an iterative method to find (local) maximum likelihood estimates of parameters in statistical models, where the model depends on unobserved latent variables. Roughly speaking, the EM algorithm works by </span>choosing random values for the missing data points and uses those guesses to estimate a second set of data. The new values obtained are then fed back and used to create a better guess for the Initial set. This iterative process continues until the algorithm converges on a fixed point. The EM Algorithm always improves a parameter’s estimation through this multi-step process. However, it sometimes needs a few random starts to find the best model.</p><p><br></p><p><u>The EM algorithm has many applications, including:</u></p><ul><li>Dis-entangling superimposed signals,</li><li>Estimating Gaussian mixture models (GMMs) and Hidden Markov models (HMMs).</li><li>Estimating parameters for compound Dirichlet distributions,</li></ul><p><br></p><p>The EM algorithm can be very slow even on the fastest computers, due to the iterative process of finding the maximum likelihood estimates. It works best when you only have a small percentage of missing data and the dimensionality of the data isn’t too big.&nbsp;</p>
------------------------------
Question: What is gradient descent in machine learning
Answer: <p><span style="color: rgb(85, 85, 85);">Optimization is an integral part of machine learning, and without it, models cannot nearly reach expected accuracy levels. Almost every machine learning algorithm has an optimization algorithm at its core. </span>Gradient descent is a popularly used optimization algorithm used to find the values of parameters (coefficients) of a function (f) that minimizes a cost function (cost).</p><p>It is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm. In machine learning, we use gradient descent to update the parameters of our model. Here, parameters refer to <em>coefficients</em> in Linear Regression and <em>weight</em>s in the case of neural networks. <span style="color: rgb(58, 59, 65);">The gradient descent algorithm starts by defining the initial parameter's&nbsp;values and from there gradient descent uses calculus to iteratively adjust&nbsp;the values and minimize the given cost function.&nbsp;</span></p><p><u style="color: rgb(58, 59, 65);">There are three popular types of gradient descent that mainly differ in the amount of data they used:</u></p><ul><li><u style="color: rgb(58, 59, 65);">Batch gradient descent</u><span style="color: rgb(58, 59, 65);"> calculates the error for each example within the training dataset, but only after all training examples have been evaluated does the model get&nbsp;updated.:&nbsp;</span></li><li><u style="color: rgb(58, 59, 65);">Stochastic gradient descent (SGD)</u><span style="color: rgb(58, 59, 65);">  updates the parameters for each training example one by one. Depending on the problem, this can make SGD faster than batch gradient descent. One advantage of SGD over traditional batch gradient descent is that the frequent updates allow us to have a pretty detailed rate of improvement.</span></li><li><u style="color: rgb(58, 59, 65);">Mini-batch gradient descent</u><span style="color: rgb(58, 59, 65);"> is an unconventional method, however, it's the go-to method due to its ability to combine the concepts of SGD and batch gradient descent. It simply splits the training dataset into small batches and performs an update for each of the&nbsp;batches.</span></li></ul>
------------------------------
Question: What is a Boltzmann machine?
Answer: <p><span style="color: rgb(79, 93, 115);">A Boltzmann machine is a type of recurrent neural network (RNN) in which nodes make binary decisions with some bias. Boltzmann machines can be strung together to make more sophisticated systems such as deep belief networks. </span>It is an unsupervised DL model in which every node is connected to every other node. Unlike&nbsp;ANNs, CNNs, RNNs, and SOMs, the Boltzmann Machines are undirected i.e, the connections are bi-directional and they are not a deterministic DL model but a stochastic/generative DL model. In<span style="color: rgb(64, 66, 78);"> Boltzmann machines, the energy of the system is defined in terms of the</span><strong style="color: rgb(64, 66, 78);">&nbsp;</strong><span style="color: rgb(64, 66, 78);">weights of synapses. Once the system is trained and the weights are set, the system always tries to find the lowest energy state for itself by adjusting the weights.</span></p><p><u>There are mainly 3 types of Boltzmann machine models, they are:</u></p><ul><li>Restricted Boltzmann Machines (RBMs)</li><li>Deep Belief Networks (DBNs)</li><li>Deep Boltzmann Machines (DBMs)</li></ul><p>According to the research paper, termed "<a href="https://www.cs.toronto.edu/~hinton/csc321/readings/boltz321.pdf" rel="noopener noreferrer" target="_blank">Boltzmann Machines</a>" , these unsupervised DL models are used to solve two quite different computational problems. </p><p>(1) For <em>search problems</em>, the weights on the connections are fixed and are used to represent the cost function of an optimization problem.</p><p>(2) For <u>learning problems</u>, the Boltzmann machine is shown a set of binary data vectors and it must find weights on the connections so that the data vectors are good solutions to the optimization problem defined by those weights.</p><p><br></p>
------------------------------
Question: What is the K-nearest neighbors algorithm?
Answer: <p>The K-nearest neighbors(Knn) algorithm is one of the classics in the field of supervised machine learning algorithms and one of the most popular, owing to its simplicity. The knn algorithm depends only on the processed data, the value of <strong>k</strong>(usually odd number to avoid a tie in samples during classification), and a <strong>distance function</strong>(Euclidean, Manhattan, etc.) and uses the training data directly without any explicit training phase. Knn doesn't perform generalization i.e, it lacks a separate training phase, and only requires to keep the entire training set in memory making it computationally expensive. The algorithm is based on the concept of ‘<u>feature similarity</u>’ assuming that similar things exist in close proximity, to predict the values of new data points in each group such that they are similar to the other data points in that particular group. It further means that the new data point will be assigned a value based on how closely it matches the points in the training set. KNN takes <strong>k</strong> number of neighbors of a particular data point to decide where it belongs to depending on its similarity in characteristics with its direct neighbors.</p><p><u>The main advantages of the Knn algorithm are</u>:</p><ul><li>It is easy to interpret the output of the knn model on a given dataset.</li><li>It has a very low Implementation time owing to simplicity and no training requirement.</li></ul><p><u>Comparatively, due to its basic nature, there are many disadvantages faced by the algorithm:</u></p><ul><li>It does not work well with a large dataset as the distance from each point to each other point must be calculated.</li><li>KNN is sensitive to noise in the dataset, it needs to be manually removed as the presence of outliers harms the model.</li></ul>
------------------------------
Question: What is K-means clustering
Answer: <p>By definition of a cluster, it is a collection of data points aggregated together because of certain similarities in their features. Each cluster is different from the other, by a well-defined margin thereby validating its usage as an unsupervised machine learning algorithm. K-Means Clustering is a very widely used unsupervised learning algorithm, which groups the data points into different clusters of similar features. Here K defines the number of pre-defined clusters that need to be created in the process of clustering, for e.g, if K=2, there will be two clusters.</p><p><strong><u>The k-means clustering algorithm mainly performs two tasks:</u></strong></p><ul><li>It determines the best value for K center points by an iterative process to get the optimal number of clusters.</li><li>It assigns each data point to its closest k-center. Therefore, the data points which are closest to a particular k-center create a cluster of their own.</li></ul><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/9f1ade568aae7f36a073ed3246029a16.png"></p><p><strong><u>The K-means algorithm works as follows:</u></strong></p><p><strong>step1:</strong>Choose an appropriate k value for eg: k=2.</p><p><strong>step2:</strong>Initialize the centroids randomly.</p><p><strong>step3:</strong>Calculate the euclidean distance from the centroids to each data point and form clusters that are close to centroids.</p><p><strong>step4:</strong>&nbsp;Find the centroid of each cluster and update existing centroids</p><p><strong>step:5 </strong>Repeat step3 with updated centroid until the solution converges (i.e, the centroid no updates)</p><p><br></p><p>K-means like the knn algorithm are dependent on the distance function and k value and since no separate training is required, it is comparatively fast, to other unsupervised algorithms irrespective of the iterative process. </p><p><br></p>
------------------------------
Question: What is the use of regularization in machine learning
Answer: <p>To understand regularization, one needs to have an idea about its applications. Overfitting is a common problem in machine learning that occurs when an ML model is too constricted to the training data, however cannot perform well on unseen data. An overfit model typically has a 100% accuracy on the training data, however, when tested on the testing data, it cannot make sense of the data. <span style="color: rgb(64, 66, 78);">Regularisation is a technique used to reduce errors by fitting the function appropriately on the given training set and adding data if required, to avoid overfitting. </span>The main working principle followed by regularization is that it will discourage the learning of more complex models and penalize the complex models, to pursue more simpler and efficient solutions that won't lead to a loss.</p><p><u>The most commonly used regularisation techniques to handle overfitting are :</u></p><ul><li>L1 regularisation</li><li>L2 regularisation</li><li>Dropout regularisation</li></ul><p><span style="color: rgb(64, 66, 78);">Often, a regression model is seen to overfit, and this method aims at reducing the complexities of regularization to reach a desirable solution without having problems in the fitting of the model(underfitting/overfitting).</span></p><p>A regression model which uses the L1 Regularisation technique is known as <u>lasso</u>(Least Absolute Shrinkage and Selection Operator) regression, whereas a regression model that uses the L2 regularisation technique is called <u>Ridge</u> regression.</p><p><u>The main difference between L1 and L2 Regularization is:</u></p><p><span style="color: rgb(32, 33, 36);">The main intuitive difference between the&nbsp;</span><strong style="color: rgb(32, 33, 36);">L1 and L2 regularization</strong><span style="color: rgb(32, 33, 36);">&nbsp;is that&nbsp;</span><strong style="color: rgb(32, 33, 36);">L1 regularization</strong><span style="color: rgb(32, 33, 36);">&nbsp;tries to estimate the median of the data while </span><strong style="color: rgb(32, 33, 36);">L2 regularization</strong><span style="color: rgb(32, 33, 36);">&nbsp;tries to estimate the mean of the data to avoid overfitting.</span></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/7bc420c74da3b2ccb95ed00f1e0d20b5.png"></p><p><br></p><p><br></p>
------------------------------
Question: what is the ensemble method in machine learning
Answer: <p>Ensemble learning is a subfield of machine learning that aims at a combination of multiple weak base models, which aren't capable of providing optimal solutions singlehandedly and combining them together to produce an optimal predictive model which is up to the standards. Roughly speaking, t<span style="color: rgb(89, 88, 88);">hey combine the decisions from multiple models to improve the overall performance and reach an optimal solution.&nbsp;The working principle behind ensemble methods in machine learning is that i</span>nstead of training one complex model for a large dataset, multiple simpler models (weak-learners) can be trained and aggregated to form an ideal model fit for prediction.<img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/d6f44a99b02372d3480b324a75d20be1.png"></p><p class="ql-align-center">Visualising Ensemble Learning [Reference: (<a href="https://towardsdatascience.com/what-are-ensemble-methods-in-machine-learning-cac1d17ed349" rel="noopener noreferrer" target="_blank">SOURCE</a>)]</p><p><u>The three of the most commonly used ensemble learning techniques are:</u></p><ul><li><strong>Bagging:</strong> It gets its name because it combines the principles of Bootstrapping and Aggregation to form one ensemble model. Bagging uses the subsets of observations created from the original dataset (bags) using bootstrapping to get a fair idea of the distribution and run weak models parallelly and independently, before aggregating the results at the end to reach a final solution.&nbsp; (example: Bagging meta-estimator, Random Forest, etc.)</li></ul><p><br></p><ul><li><strong>Boosting:</strong> It utilizes, multiple weak-learners in sequential manner such that, each subsequent model is trained by giving more importance to the data points that were misclassified by the previous weak-learners allowing them to focus on specific data points and reducing prediction bias. (example: AdaBoost,GBM,XGBM,etc.)</li></ul><p><br></p><ul><li><strong>Stacking:</strong> Stacking is an ensemble learning technique that uses predictions from multiple models (e.g, decision tree, knn, SVMs, etc.) to build a new model which is used for making predictions on the test set.</li></ul><p><br></p><p><br></p>
------------------------------
Question: What is bootstrapping in machine learning
Answer: <p><span style="color: rgb(79, 93, 115);">The Bootstrap sampling method, commonly known as bootstrapping is a simple concept that serves as a building block for some of the most advanced machine learning algorithms like AdaBoost, XGBoost, etc. Bagging is a technique in ensemble learning which utilizes the concept of bootstrapping to resample the original dataset and create bags of observations on which to train individual, weak models. </span>Re-sampling is the method of iteratively extracting samples from the original large dataset. S<span style="color: rgb(85, 85, 85);">amples are constructed by drawing observations from a large dataset one at a time(iterative) and returning them to the data sample after being chosen, allowing a given observation to be included in a given small sample more than once.</span>&nbsp;A useful feature of the bootstrap method is that the resulting sample of estimations thus formed after the iterative process is often a Gaussian distribution.&nbsp;</p><p><br></p><p>Bootstrapping is useful in the following cases:</p><ul><li>When there is a small sample size on which the null hypothesis tests have to be run.</li><li>To indirectly assess the properties of the distribution using the sample data.</li><li>To resample the original data into samples of smaller size.</li><li><span style="color: rgb(32, 33, 36);">&nbsp;To estimate summary statistics such as the mean or standard deviation from the data.</span></li></ul><p><br></p><p>The process of building <u>one sample</u> can be summarized as follows:</p><ul><li>Choose a dedicated size for the extracted samples.</li><li>While the size of the sample is &lt; the chosen size by the user:</li></ul><ol><li class="ql-indent-1">Randomly select an observation from the original dataset AND</li><li class="ql-indent-1">Add the chosen observation to the sample to be created.</li></ol><p><br></p><p><br></p>
------------------------------
Question: what is hierarchical clustering in machine learning?
Answer: <p>Clustering is one of the simplest, unsupervised learning algorithms. It is a technique that groups data points having similar features together into distinct groups without referring to the labeled outcomes of the features. These clusters are formed purely upon the concepts of feature similarity. Hierarchical clustering is another unsupervised ML algorithm that groups unlabeled data into clusters in a top-to-bottom or a bottom-up approach., where there exists a clear hierarchy amidst the clusters in the form of a tree. This tree-shaped structure is known as the dendrogram. The main need for this algorithm, roots from the drawback of the most popularly used, K-means clustering algorithm where the algorithm intuitively tries to generate a&nbsp;predetermined set of clusters i.e, the K value. The hierarchical clustering approach is preferred when we don't have knowledge about the predefined number of clusters, rather we want to discover all possible clusters and the inherent hierarchy between them.</p><p>There are two primary distinctions in this algorithm based on their approach. They are:</p><ul><li><strong>Agglomerative Hierarchical Clustering</strong>- This follows a bottom-up approach i.e, it <span style="color: rgb(64, 66, 78);">treats each data as a single cluster at the start and then successively agglomerates pairs of similar clusters(initially single data-points) until all clusters have been merged into a single cluster that contains all data.</span></li><li><strong>Divisible Hierarchical Clustering</strong>- This follows a top to bottom approach, i.e, it is<span style="color: rgb(64, 66, 78);"> a method for splitting a cluster that initially contains the whole data and proceeds by splitting the initial cluster recursively until individual data have been split into sets of a distinct and single cluster.</span></li></ul><p>The main drawback of the Hierarchical clustering algorithm is that it performs <span style="color: rgb(73, 84, 94);">well on small sets of data, however, in the case of a large dataset, this algorithm does not fit well, K-means performs much better on such large datasets.</span></p>
------------------------------
Question: Why is padding needed in convolutional neural networks>
Answer: <p>In general, pixels in the center of an image are used more frequently compared to those on the corners and edges. Consequently, the information on the borders of an image is not preserved as well as the information in the middle. As the window-frame approaches the edges (in a CNN), it may go past the image, resulting in issues with the final model, due to faulty image data. Padding ensures that edges fall in the scope of the sliding window and the image boundary is maintained to obtain accurate image data. Padding is performed by adding layers of zeros to the border of the input image to preserve the actual corners and edges.</p><p><u>There are many reasons, supporting the importance of padding for a CNN:</u></p><ul><li>It's easier to design networks if the <u>height</u> and <u>width </u>are preserved.</li><li>It allows us to design deeper networks. Without padding, a reduction in volume size would decrease too quickly.</li><li>Padding actually improves performance by keeping information at the borders.</li></ul><p><u>There are mainly two types of popularly used padding techniques, they are:</u></p><ul><li><strong style="color: rgb(64, 66, 78);">Valid Padding:&nbsp;</strong><span style="color: rgb(64, 66, 78);">It implies no padding at all i.e., the input image is left in its valid or unaltered shape.</span></li><li><strong style="color: rgb(64, 66, 78);">Same Padding:&nbsp;</strong><span style="color: rgb(64, 66, 78);">In this case, ‘p’ number of padding layers are added, such that the output image obtained has the same dimensions as the input image.</span></li></ul>
------------------------------
Question: What are the advantages of Apache Spark over MapReduce?
Answer: <p>Both the <strong>Apache Spark</strong> and <strong>MapReduce</strong> are essential components of <strong>Hadoop Ecosystem</strong> but Apache Spark is better than MapReduce.</p><p><strong><u>Apache Spark is superior for the following reasons</u>:-</strong></p><p>1) Apache Spark's processing speed is faster than MapReduce's as it uses in-memory storage for intermediate computations, whereas MapReduce is completely disk-based.</p><p>2) For repetitive data analysis, MapReduce takes too long for iterations because between each iteration it must save data to disk and retrieve it from disk for the next iteration, whereas Apache Spark is ideal for such situations because it stores data in memory between iterations.</p><p>3) Apache Spark can handle real-time data (for example, data from real-time event feeds such as Twitter). MapReduce, on the other hand, struggles when it comes to real-time data analysis.</p><p>4) Apache Spark supports a variety of programming languages (such as java, python, R, Scala).</p><p>5) Apache Spark has a built-in machine learning library called 'Mlib' that facilitates Machine learning. It also supports Graph processing.</p><p>6) Apache Spark can store data in memory, improving overall device speed.</p><p>7) MapReduce is only capable of batch processing, while Spark can accommodate any kind of requirements (batch, iterative, streaming, interactive).</p>
------------------------------
Question: What is Apache Spark?
Answer: <h3><br></h3><ul><li>Apache Spark is a fast and cohesive engine optimized for large-scale data processing.</li><li>Spark has features such as high processing speed, in-memory storage, fault tolerance, real-time data processing, and support for multiple languages.</li><li>Spark can run on any operating system. Spark employs the RDD (Resilient Distributed Datasets) principle, which requires data to be stored in memory and moved to disk only when required.</li><li>Spark has been the go-to tool for large companies and developers due to its speed and simple API.</li></ul><p>Apache Spark, in a nutshell, is an open source, scalable, and cluster computing-based platform developed for fast computation, which can handle batch processing as well as streaming data and can be installed on premises as well as the cloud. It is very fast because it does most of the computation in memory..</p>
------------------------------
Question: What are the key features of Spark?
Answer: <h2><strong><u>KEY FEATURES OF APACHE SPARK</u>:-</strong></h2><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/590d8ba5a9e0fbfb3ed53ebae48e5ea2.png"></p><p><br></p><p>1) <strong>High Performance</strong>: Apache Spark can run an application 10 times faster on disc and 100 times faster in memory.</p><p>2) <strong>Language support</strong>: Spark includes APIs in Python, Scale, R, and Java. As a result, it supports a good range of programming languages.</p><p>3) <strong>Real-time data computation</strong>: Spark can process data from real-time event sources.</p><p>4) <strong>In-memory computation</strong>: Spark can process tasks in memory. there's no need to save the info to a disk.</p><p>5) <strong>Open source</strong>: Spark is a free and open source application that needs no license costs and can be embedded in any OS.</p><p>6) <strong>Fault tolerance</strong>: Spark RDD is configured to resist node faults. As a result, data loss is zero.</p>
------------------------------
Question: Which languages are supported by Apache Spark?
Answer: <h3><strong><u>Languages Supported by Apache Spark:-</u></strong></h3><p>Spark procedures can be extended to a wide range of workloads and represented in any of the compatible languages:-</p><ul><li><strong>Scala</strong></li><li><strong>Java</strong></li><li><strong>Python</strong></li><li><strong>SQL</strong></li><li><strong>R</strong></li></ul><p>Spark provides centralized libraries of well-documented APIs that provide the following modules as key components: Spark SQL, Spark Structured Streaming, Spark MLlib, and GraphX, integrating all workloads operating under one engine.</p>
------------------------------
Question: In which language is Apache Spark written?
Answer: <p>Apache Spark is written in <strong style="background-color: rgb(255, 255, 0);">Scala</strong><strong> </strong> (<strong>Scalable Language</strong>), an open-source programming language that supports both object-oriented and functional programming.</p><p>Apache Spark is written in Scala as it's less complex and delivers the best path for developing scalable big data applications.</p>
------------------------------
Question: What is a RDD in Spark?
Answer: <p class="ql-align-justify">RDD is an abbreviation for Apache Spark <span style="color: rgb(0, 71, 178);">Resilient Distributed Dataset</span>. <span style="color: rgb(0, 71, 178);">Resilient</span> because they can't be modified once created. <span style="color: rgb(0, 71, 178);">Distributed</span> because they are distributed across clusters. <span style="color: rgb(0, 71, 178);">Dataset</span><strong style="color: rgb(0, 71, 178);"> </strong>because it holds data.</p><ul><li class="ql-align-justify">RDD is Spark's fundamental data structure.</li><li class="ql-align-justify">RDD was introduced to address some of Hadoop MapReduce's limitations.</li><li class="ql-align-justify">RDDs are a key component of Apache Spark's big data processing architecture.</li><li class="ql-align-justify">RDD supports&nbsp;<span style="color: rgb(0, 71, 178);">in-memory data storage</span> and can persist the stored data to disk as needed.</li><li class="ql-align-justify">Spark Transformation APIs are used to compute partitions on various nodes of the cluster. RDDs can include Python, Java, or Scala elements, as well as user-defined classes.</li><li class="ql-align-justify">Spark RDDs are a set of data components that are immutable, fault-tolerant, and potentially distributed. <span style="color: rgb(0, 71, 178);">Immutable</span> means that RDDs are read-only collections of various types of objects, thus can't be altered. <span style="color: rgb(0, 71, 178);">Fault tolerant</span><strong> </strong>means if any partition of RDD is lost due to failure, lineage helps build only that particular lost partition.</li></ul>
------------------------------
Question: How do you create RDDs in Spark?
Answer: <h2><span style="color: rgb(0, 71, 178);">Three ways to build an RDD in Apache Spark:-</span></h2><h3><strong>1) Parallelizing collection (Parallelized)</strong></h3><ul><li>In this approach, we use an already existing collection and transfer it to the SparkContext's parallelize() function.</li><li>This is a novel approach for easily creating RDDs in Spark-shell and then performing operations on them.</li><li>It is also rarely used because it usually requires the whole Dataset on a single device.</li></ul><h3><strong>2) Using an External Dataset</strong></h3><ul><li>RDDs in Spark can be created from any Hadoop-supported data source, such as local file systems, HDFS, Hbase, Cassandra, and so on.</li><li>Information is integrated from an external dataset in this case.</li><li>To generate a text file RDD, we can use SparkContext's textFile tool. It would interpret the file's URL as a series of lines. The URL may be a local route on the machine.</li></ul><h3><strong>3) RDD creation from an existing RDD</strong></h3><ul><li>Transformation is the process of transforming one RDD into another, while transition is the process of creating an RDD from an existing RDD. This differentiates Apache Spark from Hadoop MapReduce.</li><li>Conversion works similar to a machine that takes in an RDD and outputs one. Since RDDs are constant, it produces varying RDDs by adding operations to the input RDD.</li></ul>
------------------------------
Question: How can I install Spark on my computer?
Answer: <h3><strong><u>Requirements</u></strong></h3><ul><li>A computer running Windows 10</li><li>A user account with administrative rights&nbsp;</li><li>Powershell or Command Prompt</li><li>Tool for extracting .tar files such as <span style="color: rgb(0, 55, 0);">7-Zip</span></li></ul><p><br></p><h3><strong><u>Steps to install Spark on Computers-</u></strong></h3><p><br></p><p><strong style="color: rgb(0, 0, 0);">Step 1</strong><span style="color: rgb(0, 0, 0);">: Install Java 8</span></p><p><strong style="color: rgb(0, 0, 0);">Step 2</strong><span style="color: rgb(0, 0, 0);">: Install Python</span></p><p><strong style="color: rgb(0, 0, 0);">Step 3</strong><span style="color: rgb(0, 0, 0);">: Download Apache Spark</span></p><p><span style="color: rgb(0, 0, 0);">Here is the link to download:-</span><strong style="color: rgb(0, 0, 0);"> </strong><a href="https://spark.apache.org/downloads.html" rel="noopener noreferrer" target="_blank" style="color: rgb(0, 108, 168); background-color: rgb(255, 255, 255);">https://spark.apache.org/downloads.html</a></p><p><strong style="color: rgb(0, 55, 0);">Step 4</strong><span style="color: rgb(0, 55, 0);">: Review the file's checksum to ensure the integrity of your download. This means that you are dealing with unaltered, undamaged software.</span></p><p><strong style="color: rgb(0, 0, 0);">Step 5</strong><span style="color: rgb(0, 0, 0);">: Install Apache Spark</span></p><p><span style="color: rgb(0, 55, 0);">Extracting the downloaded file to the desired location is the first step in installing Apache Spark.</span></p><p><span style="color: rgb(0, 55, 0);">a. Make a new Spark folder in the root of your C: drive. Enter the following into a command line:</span></p><pre class="ql-syntax" spellcheck="false">cd \
mkdir Spark
</pre><p><span style="color: rgb(0, 55, 0);">b. Navigate to the Spark file you saved in Explorer.</span></p><p><span style="color: rgb(0, 55, 0);">c. Right-click the file and use the tool (e.g., 7-Zip) on your machine to extract it to C:\Spark .</span></p><p><span style="color: rgb(0, 55, 0);">d. Your C:Spark folder now contains a new folder </span><strong style="color: rgb(0, 55, 0);">spark-2.4.5-bin-hadoop2.7</strong><span style="color: rgb(0, 55, 0);"> containing the required data.</span></p><p><strong style="color: rgb(0, 0, 0);">Step 6: </strong><span style="color: rgb(0, 0, 0);">Add winutils.exe File</span></p><p><strong style="color: rgb(0, 0, 0);">Step 7: </strong><span style="color: rgb(0, 0, 0);">Configure Environment Variables</span></p><p><strong style="color: rgb(0, 0, 0);">Step 8:</strong><span style="color: rgb(0, 0, 0);"> Launch Spark</span></p>
------------------------------
Question: What is the difference between a Dstream and a structured stream?
Answer: <p><strong> DSTREAM</strong></p><ul><li>The specific abstraction in Spark Streaming is a DStream (Discretized Stream). DStream is a continuous stream of RDDs (Resilient Distributed Datasets) representing a continuous stream of data.</li><li>When discussing continuous streams, The word "<strong>continuous</strong>" refers to the fact that we never begin or end receiving data as part of the stream.</li><li>DStreams may be produced via live data (for example, data from HDFS, Flume, or Kafka) or by transforming existing DStreams.</li><li>Streaming with DStreams approach guarantees at-least-once delivery, but can provide millisecond latencies.</li></ul><p><strong> STRUCTURED STREAM</strong></p><ul><li>To take advantage of the Spark-SQL API's optimization, Apache Spark Structured Streaming is installed on top of it.</li><li>Structured Streaming, in a nutshell, offers fast, scalable, fault-tolerant, end-to-end stream processing.</li><li>Structured Streaming offers exactly-once delivery with 100+ milliseconds latency.</li></ul><p><strong>Summarize both by following table-</strong></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/71fd53c9d40c7bca4c0cc8f85c370cad.png"></p>
------------------------------
Question: What is a structured stream in Spark?
Answer: <p class="ql-align-justify">Structured Streaming is a fault-tolerant and scalable <span style="color: rgb(0, 71, 178);">stream processing engine</span> built on the Spark SQL engine. Structured Streaming queries are processed internally by default using a micro-batch processing engine, which handles data streams as a sequence of small batch tasks, achieving end-to-end latencies as low as 100 milliseconds. The core principle in Structured Streaming is to think of a live data stream as a table that is constantly being added to. Think of the input data stream as the "Input Table." Every data object that enters the stream is like adding a new row to the Input Table.</p><p class="ql-align-justify"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/a1e5ced1012f187a71c05b218c3bc12b.png"></p><p class="ql-align-justify">Structured Streaming reads the most recent available data from the streaming data source, incrementally processes it to update the result, and then discards the source data.</p>
------------------------------
Question: What is the difference between supervised vs. unsupervised learning?
Answer: <p>Classical machine learning algorithms can be broadly classified into 2 types depending on the type of data used: supervised and unsupervised learning algorithms. The main difference between the two is the fact that supervised learning algorithms take labeled data as input and test its performance on similarly labeled data i.e., they have a knowledge of the possible output. Meanwhile, unsupervised learning algorithms do not have anything to do with using labeled data, leading to unfamiliarity with the output of the algorithm.</p><p>In supervised learning, the algorithm “learns” from the training dataset as the name suggests, the labeled data "supervises" the algorithm as a teacher would, and makes it understand the intricate patterns in the data. The better the teacher/data, and the more intelligent/powerful in this case the algorithm is, the better will it be able to predict. While supervised learning models tend to be more accurate than unsupervised learning models, in most cases, they require upfront human intervention i.e., data annotation which is expensive and time-consuming. For example, a supervised learning model can predict how long your commute will be from point A to B, on a rainy day at a particular time. But first, it needs to be trained to know that rainy weather extends the driving time and also on the daily time taken to cover the distance at that time of the day.</p><p>Meanwhile, unsupervised learning models, in contrast, work on their own to discover the inherent structure of unlabeled data, i.e., they don't require data annotation and are "self-taught". However, they still require some degree of human intervention for validating the output of the model. For example, an unsupervised learning model can identify that online shoppers often purchase groups of products at the same time. The further sub-divisions and their respective real-world use-cases can be seen in the image(<a href="https://medium.com/@dkatzman_3920/supervised-vs-unsupervised-learning-and-use-cases-for-each-8b9cc3ebd301" rel="noopener noreferrer" target="_blank">SOURCE</a>) below, which highlights the difference in more detail, based on the different sub-categories.</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/b9e5706f24fca7f5fcd4bd54b3b48dd9.png"></p>
------------------------------
Question: What is a DataFrame in Spark?
Answer: <p class="ql-align-justify">A spark data frame is a <span style="color: rgb(0, 71, 178);">distributed data set</span> that is structured into named columns. It<span style="color: rgb(77, 89, 104);"> is the distributed dataset that stores as a </span><span style="color: rgb(0, 71, 178);">tabular structured format</span> that is used to provide operations such as searching, aggregation computation, sorting, and can also be used for Spark SQL. Consider it similar to a spreadsheet or SQL table. The various sources that produce a data-frame are :- Existing RDD, Structured data files and databases, Hive Tables. It is essentially referred to as an abstraction layer that is installed on top of RDD and is followed by the dataset API. Data frames, also known as<span style="color: rgb(0, 71, 178);"> DFs</span>, are logical columnar formats that allow dealing with RDDs faster and more efficiently while also performing the same functions as RDDs.</p><ul><li class="ql-align-justify">Since the data frame is a distributed set of data, the data is arranged in named columns.</li><li class="ql-align-justify">Data-frames are used to control <span style="color: rgb(0, 71, 178);">SQL queries</span>.</li><li class="ql-align-justify">It is used to process both structured and unstructured data.</li><li class="ql-align-justify">Spark SQL Data-frame facilitates <span style="color: rgb(68, 68, 68);">fault tolerance</span> and <span style="color: rgb(68, 68, 68);">in-memory</span> analysis.</li><li class="ql-align-justify">Spark SQL Data-frames are<span style="color: rgb(0, 71, 178);"> scalable</span> and capable of processing massive amounts of data.</li><li class="ql-align-justify">The libraries are available in a variety of programming languages, including Python, Scala, Java, and R.</li><li class="ql-align-justify">The ability to handle data ranging from Kilobytes to Petabytes on a single node cluster to a massive cluster.</li></ul>
------------------------------
Question: What are the roles of a Spark driver?
Answer: <ul><li class="ql-align-justify">A <strong>Spark driver </strong>is a Java virtual machine (JVM) mechanism that hosts the SparkContext for a Spark application. In a Spark application, it is the master node.</li><li class="ql-align-justify">Spark Driver contains various components –<span style="color: rgb(0, 71, 178);">DAGScheduler </span>(<span style="color: rgb(68, 68, 68);">primary component responsible for executing a Spark Action</span><span style="color: rgb(41, 41, 41);">), </span><span style="color: rgb(0, 71, 178);">TaskScheduler</span> (<span style="color: rgb(68, 68, 68);">scheduling the tasks for execution</span><span style="color: rgb(41, 41, 41);">), </span><span style="color: rgb(0, 71, 178);">BackendScheduler</span> (<span style="color: rgb(51, 51, 51);">support various cluster managers), </span><span style="color: rgb(0, 71, 178);">BlockManager</span> (<span style="color: rgb(68, 68, 68);">acts as a local cache that runs on every "node" in a&nbsp;Spark</span><strong style="color: rgb(68, 68, 68);">&nbsp;</strong><span style="color: rgb(68, 68, 68);">application</span><span style="color: rgb(32, 33, 36);">)</span></li><li class="ql-align-justify"><span style="color: rgb(0, 71, 178);">Spark Driver is the central coordinator</span>, and it deals with all of the Workers. It serves as Spark Shell's focal point and entry point.&nbsp;The driver software executes the application's main () function and is the place where the Spark Context is generated.</li><li class="ql-align-justify">It hosts the environment's Web UI. It reveals details about the currently operating spark program through a Web UI on port 4040.</li><li class="ql-align-justify">It transforms the RDDs into the execution graph and divides it into stages.</li><li class="ql-align-justify">The metadata for all Resilient Distributed Databases and their partitions are stored in the driver.</li><li class="ql-align-justify">The job scheduler resides in a driver and distributes assignments to workers.</li><li class="ql-align-justify">A driver is in charge of coordinating workers and the general execution of activities.</li></ul>
------------------------------
Question: What is hadoop?
Answer: <p class="ql-align-justify"><strong>HADOOP</strong></p><p class="ql-align-justify">Apache Hadoop is an <span style="color: rgb(0, 71, 178);">open source platform</span> for storing and manipulating large amounts of data. Hadoop is <span style="color: rgb(0, 71, 178);">written in Java </span>programming language.</p><p class="ql-align-justify">HADOOP is defined as<span style="color: rgb(230, 0, 0);"> </span><span style="color: rgb(0, 71, 178);">High Availability Distributed Object Oriented Platform</span>.</p><p class="ql-align-justify">It is intended to scale from a single server to thousands of computers, each of which provides local computing and storage as well as local computing.</p><p class="ql-align-justify">Rather than relying on hardware to have high availability, the Hadoop platform is designed to predict and manage failures at the application layer, providing a highly accessible service on top of a cluster of machines, each of which can malfunction.</p><p class="ql-align-justify">In simple terms, <span style="color: rgb(95, 99, 104);">Hadoop</span><span style="color: rgb(77, 81, 86);">&nbsp;is&nbsp;</span><span style="color: rgb(95, 99, 104);">used</span><span style="color: rgb(77, 81, 86);">&nbsp;for storing and processing big data in a distributed and highly scalable manner.</span></p><p class="ql-align-justify"><strong style="color: rgb(77, 81, 86);">Basic principle</strong><span style="color: rgb(77, 81, 86);">: </span><span style="color: rgb(0, 71, 178);">Distributing data + Distributing computation</span></p><p class="ql-align-justify"><strong>BENEFITS OF HADOOP:-</strong></p><p class="ql-align-justify"><strong>Resilience</strong> — If a node fails, there is still a copy of the data in the cluster.</p><p class="ql-align-justify"><strong>Scalability</strong> — Unlike standard systems, which have storage limitations, Hadoop is modular because it runs in a distributed environment.</p><p class="ql-align-justify"><strong>Speed</strong> — The Hadoop MapReduce programming model is used to compute data in each node where the data is stored and in a parallel fashion.</p>
------------------------------
Question: What is the relationship between bias and overfitting in machine learning?
Answer: <p><span style="color: rgb(79, 93, 115);">Bias and Variance are two of the most popular fundamental terms in machine learning and often used to explain overfitting and underfitting.&nbsp;</span>The bias of a specific machine learning model trained on a specific dataset describes how well the model can capture the relationship between the features of the data and the target variable.&nbsp;Having a high bias can cause an algorithm to miss the relevant relations between features and target outputs resulting in the model being underfitted. Increasing the bias will decrease the variance, conversely increasing the variance will decrease the bias, this is the popular Bias-Variance tradeoff in machine learning. Minimizing these two sources of error(Bias and Variance) that prevent supervised learning algorithms from generalizing beyond their training set is the main goal of the Bias-Variance tradeoff problem.</p><p>Therefore, with the decrease in the bias of the model, the variance will conversely increase resulting in the model overfitting on the data. That is why, it is required to have a balance between low bias and low variance, to prevent the model from being overfitted or underfit on the given data.</p>
------------------------------
Question: What are the main components of Hadoop?
Answer: <p><strong>There are 4 main components of Hadoop:</strong></p><ol><li><strong>Hadoop HDFS</strong> - Hadoop Distributed File System is the Hadoop's storage unit.</li><li><strong>Hadoop MapReduce</strong> - Hadoop MapReduce is the Hadoop processing unit.</li><li><strong>YARN</strong>&nbsp;- YARN is a Hadoop's resource control unit.</li><li><strong>Hadoop Common</strong>- It is a set of utilities and libraries that are used to support other Hadoop modules.</li></ol><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/cc2a8bfd92ad91c46a3b3a301eb9963f.png"></p><p><strong>Hadoop HDFS</strong></p><ul><li>HDFS stores data in a distributed way.</li><li>HDFS is made up of two parts: the name node and the data node. Although there is only one name node, numerous data nodes may exist.</li><li>It allows for distributed computing.</li><li>It is feasible to deploy on commodity hardware.</li><li>It ensures data security.</li><li>It is extremely fault-tolerant; if one computer fails, the data from that machine is transferred to the next machine.</li></ul><p><strong>Hadoop MapReduce</strong></p><ul><li>MapReduce is a software application and programming model for dealing with massive volumes of data.</li><li>Hadoop MapReduce is the processing unit of Hadoop.&nbsp;</li><li>The computation is performed at the slave nodes in the MapReduce approach, and the end outcome is sent to the master node.</li></ul><p><strong>Hadoop YARN</strong></p><ul><li>Hadoop YARN is an acronym that stands for Yet Another Resource Negotiator. It is Hadoop's resource management unit, and it is used in Hadoop version 2.</li><li>Hadoop YARN functions as an operating system for Hadoop.</li><li>It is in charge of handling cluster resources to ensure that no single computer is overloaded.</li><li>It performs work scheduling to ensure that jobs are planned in the correct location.</li></ul><p><strong style="color: rgb(66, 66, 66);">Hadoop Common</strong></p><ul><li>Hadoop Common is a set of utilities and libraries that are used to help other Hadoop modules.</li></ul>
------------------------------
Question: What is the role of YARN in the Hadoop ecosystem?
Answer: <p>YARN is an acronym that stands for <span style="color: rgb(0, 71, 178);">Yet Another Resource Negotiator</span>.</p><p><strong>Role of YARN in Hadoop Ecosystem:-</strong></p><ul><li>Apache Hadoop YARN is the resource control and task scheduling technology. YARN, a key component of Apache Hadoop, is in charge of allocating server resources to the numerous applications operating in a Hadoop cluster and arranging tasks to be performed on separate cluster nodes.</li><li>As required, YARN will dynamically <span style="color: rgb(0, 71, 178);">distribute resources to applications</span>.</li><li>YARN supports a variety of scheduling techniques, many of which are built on a queue format for submitting processing jobs.&nbsp;Default FIFO Scheduler runs applications on basis of first-in-first-out.</li><li>Hadoop YARN also has a<span style="color: rgb(0, 71, 178);"> Reservation System</span> feature, which allows the users to reserve cluster services ahead of time for critical processing jobs to ensure they function successfully.</li><li>YARN is known as Hadoop's operating system because it is in <span style="color: rgb(0, 71, 178);">responsible for handling and managing workloads</span>. It enables many data processing engines to manage data processed on a single platform, such as real-time streaming and batch processing.</li></ul><p>YARN's entire functionality is based on three key components:-</p><ul><li>The <strong>ResourceManager</strong> (RM) is the first component and serves as the leader for all cluster services.</li><li>The second component is a<strong> NodeManager</strong> (NM), which handles users' jobs and workflow on a specific node.&nbsp;</li><li>The <strong>ApplicationMaster</strong>, a user work life-cycle boss, is the third component. The user application is stored in the ApplicationMaster.</li></ul>
------------------------------
Question: What is the structure of an Apache YARN process?
Answer: <p>The main components of YARN architecture include:-</p><ul><li><strong>ResourceManager</strong>: The ResourceManager (RM) is the first component and serves as the leader for all cluster services.</li><li><strong>NodeManager:</strong> The second component is a NodeManager (NM), which handles users' jobs and workflow on a specific node.&nbsp;</li><li><strong>ApplicationMaster</strong>: It is third component. The user application is stored in the ApplicationMaster.</li><li><strong>Container</strong>: A container is a single node in a cluster that represents a resource (memory).</li></ul><p><strong>APACHE YARN PROCESS STRUCTURE</strong></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/f2f08b1bb425fdf37f7f472d74515a7a.png"></p><p>1. First of all, the client makes a request for resources to the Resource Manager..</p><p>2. Then, Resource Manager assigns a container to the Application Manager in order to launch it.</p><p>3. After that, Application Manager authenticates with the Resource Manager.</p><p>4. The Application Manager deals with the Resource Manager for containers.</p><p>5. The Application Manager notifies the Node Manager that containers are to be launched.</p><p>6. Application code runs in the container.</p><p>7. The client calls the Resource Manager/Application Manager to check on the state of the application.&nbsp;</p><p>8. After the processing is over, the Application Manager unregisters from the Resource Manager.</p>
------------------------------
Question: What is the function of  Apache Pig?
Answer: <p class="ql-align-justify">Pig is a free and open-source application that is a component of the Hadoop ecosystem.<span style="color: rgb(79, 93, 115);"> And </span>is being used to handle large amounts of structured and unstructured data such as web logs, and also used to do data processing for search platforms. It is an abstraction over MapReduce in that Pig scripts are executed by automatically transformed into MapReduce jobs. Its advantage over directly using MapReduce is that spares the programmer with having to deal with the complexities of setting up a MapReduce job.</p><ul><li class="ql-align-justify">Pig provides a high-level language called Pig Latin to write data processing programs. This language offers many operators to create their own read, write and process data functions.</li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Apache Pig can handle structured, unstructured, and semi-structured data. It stores the results in HDFS</span><span style="color: rgb(0, 0, 0);">.</span></li><li class="ql-align-justify">Pig allows users to write User-defined Functions in other programming languages, like Java, and then invoke or encode them in Pig Scripts.</li></ul>
------------------------------
Question: What is the difference between semi-supervised learning and reinforcement learning?
Answer: <p> <span style="color: rgba(0, 0, 0, 0.87);">Semi-supervised machine learning uses a small amount of labeled data and a large amount of unlabeled data, to provide the benefits of both unsupervised and supervised learning while overcoming the limitations of finding a large amount of labeled data. Therefore, a model can be trained to label data without having to use as much labeled training data.&nbsp;</span></p><p>Reinforcement learning, on the other hand, is another type of non-traditional machine learning approach that differs from semi-supervised learning in many ways. For starters, reinforcement learning is a method where there are reward values attached to the different steps that the model is supposed to go through. It does not start with any relevant data, however, every time the model makes a mistake it is punished so that it learns its mistake, and every time it does something right, the reward is given, this is how it learns without the presence of existing data as the algorithm’s end goal is to accumulate as many reward points as possible. On the contrary, semi-supervised learning although can make do without large quantities of labeled data, still needs data to train the models.&nbsp;</p>
------------------------------
Question: What is reinforcement learning?
Answer: <p>Reinforcement learning is a type of non-traditional machine learning approach that can be used to train a model when data is missing. For starters, reinforcement learning is a method that utilizes reward values attached to the different steps that the model is supposed to go through. It does not start with any relevant data, just a set of instructions where every time the model makes a mistake it is punished so that it learns its mistake, and every time it does something right, a reward is given. This is how it learns without the presence of existing data as the algorithm’s end goal is to accumulate as many reward points as possible. One of the most popular examples of Reinforcement Learning in action is DeepMinds AlphaGo which proves that by combining raw speed, deep learning, and reinforcement learning, a model can overcome human performance on many tasks by defeating a former world champion on the game Go. Some of the more real-world applications can be seen in more detail in this <a href="https://towardsdatascience.com/applications-of-reinforcement-learning-in-real-world-1a94955bcd12" rel="noopener noreferrer" target="_blank">blog post</a>.  The main challenge in reinforcement learning lies in preparing the simulation environment, i.e, the training environment for the model, which is highly dependant on the task to be performed. When in context to games like Go, chess, etc, preparing the environment is relatively simple owing to a finite set of possibilities. However when it comes to building a model capable of driving an autonomous car, building a realistic simulator is crucial before letting the car ride on the street, else an accident of large scale could easily become a reality. In the case of autonomous vehicles, the model has to figure out how to brake or avoid a collision in a safe environment, and where and how to ideally take turns, etc. Scaling and tweaking the neural network controlling the agent is another challenge as there is no way to communicate with the network other than through the system of rewards and penalties, leading to valuable information being left out in many cases.</p>
------------------------------
Question: What is semi-supervised machine learning?
Answer: <p><span style="color: rgb(64, 66, 78);"> Semi-Supervised learning falls somewhere in between supervised and unsupervised machine learning algorithms while overcoming the disadvantages of both to get the best possible performance. </span><span style="color: rgba(0, 0, 0, 0.87);">It uses a small amount of labeled data and a large amount of unlabeled data, which provides the benefits of both unsupervised and supervised learning while avoiding the challenges of finding a large amount of labeled data. </span></p><p><br></p><p><strong>Some of the widely used practical applications of Semi-Supervised Learning are:</strong></p><ol><li><strong>Speech Analysis:</strong>&nbsp;Since labeling the audio files is a very time and resource-consuming task, Semi-Supervised learning is a very natural approach to solve this problem.</li><li><strong>Internet Content Classification:</strong>&nbsp;Labeling each webpage is an impractical and unfeasible process and thus Semi-Supervised learning algorithms, can make a big change in classifying web pages.</li></ol><p><br></p><p><span style="color: rgb(34, 34, 34);">When the problem at hand is complicated and the labeled data fails to represent the entire distribution, semi-supervised learning cannot help. For instance, if we want to classify color images of objects that look somewhat different from various angles, then semi-supervised learning might not be of much use unless a good deal of labeled data is present. However, if we already have a large volume of labeled data, then we can directly use supervised learning techniques, thereby leading to a scenario where semi-supervised learning isn't particularly a feasible solution.</span></p>
------------------------------
Question: What is the role of Ambari in the Hadoop ecosystem?
Answer: <p><strong>AMBARI IN HADOOP</strong></p><p>Ambari is an open source cluster management platform that <span style="color: rgb(0, 71, 178);">keeps track of running applications and their status</span>. It is basically mounted on top of the Hadoop cluster. Furthermore, we can think of it as an open source web-based monitoring platform that controls, tracks, and provisions the wellbeing of Hadoop clusters.</p><p><strong>ROLE OF AMBARI IN HADOOP ECOSYSTEM</strong></p><p><strong>Pre-range Operational Metrics</strong></p><p>Ambari has pre-configured operating measurements that provide instantaneous insight into the health of the Hadoop cluster.</p><p><strong>Authentication</strong></p><p>Ambari offers verification, permission, and auditing by deploying Kerberos-based Hadoop clusters.</p><p><strong>Observation</strong></p><p>Dependencies and success are tracked by visualizing and reviewing roles and activities.</p><p><strong>Administrative Management Capabilities </strong></p><ul><li>It has the ability to connect and delete hosts from the cluster.</li><li>It can also connect, delete, resume, interrupt, and restart services.</li><li>Furthermore, it aids in restarting clusters or resources after configuration changes.</li><li>Allows for rollback and editing of service/component settings.</li><li>It has the ability to transfer nodes to a separate host.</li></ul><p>In a nutshell,<span style="color: rgb(0, 71, 178);"> Ambari's primary responsibility is to control and handle dynamic distributed networks</span>. It gathers information from cluster nodes and presents it to users through the Ambari Web user interface. Alternatively, Apache Ambari simplifies Hadoop cluster management and reporting by offering an easy-to-use web UI and REST API.</p>
------------------------------
Question: What is the difference between managed and unmanaged tables in Spark?
Answer: <p class="ql-align-justify">A <strong>managed table</strong> is a table that handles both data and metadata. In this instance, the 'DROP TABLE' command deletes all the table's metadata and the data itself. On the other side, the metadata from a database, such as the schema and data position, is handled by an <strong>unmanaged table</strong>, but the data itself is in a separate location, often backend by a blob store such as Azure Blob or S3. In this case, when we execute a drop table operation, Spark will not delete the table. Dropping an unmanaged table removes only the metadata associated with the table, leaving the data alone. External Table is another name for an unmanaged table.</p>
------------------------------
Question: What is the difference between a temporary view and a global temporary view in a Spark application?
Answer: <p class="ql-align-justify"><strong>Temporary views</strong> are session-scoped and are deleted after the session ends so they avoid persisting the description in the underlying meta-store, whether one exists.</p><p class="ql-align-justify">You can establish a <strong>global temporary view</strong> if you like a temporary view that is shared across all sessions and remains active until the Spark program is terminated.</p><p class="ql-align-justify">For Example.</p><pre class="ql-syntax" spellcheck="false">CREATE GLOBAL TEMPORARY VIEW tempo_view 
AS 
SELECT a + 1, b + 2 FROM table
</pre><p class="ql-align-justify">The <strong>global temporary view </strong>is linked to the system-preserved database global temp, and therefore we must refer to it using the eligible name., e.g.</p><pre class="ql-syntax" spellcheck="false">SELECT * FROM global_temp.tempo_view
</pre>
------------------------------
Question: What is Apache Kafka?
Answer: <p class="ql-align-justify"><strong>APACHE KAFKA</strong></p><ul><li class="ql-align-justify">Apache Kafka is a distributed data streaming application that allows you to publish, subscribe to, store, and process records streams in real time.</li><li class="ql-align-justify">It is a messaging system which lets you send messages between processes, application, and servers.</li><li class="ql-align-justify">Kafka is often used for operational monitoring data as it is strongly durable.</li></ul><p class="ql-align-justify"><strong>CORE CAPABILITIES OF KAFKA </strong></p><p class="ql-align-justify"><strong>High throughput</strong></p><p class="ql-align-justify">Using a cluster of devices with latencies as low as 2ms, deliver messages at network-limited throughput.</p><p class="ql-align-justify"><strong>Adaptable</strong></p><p class="ql-align-justify">Scale up output clusters to 1,000 brokers, trillions of messages every day, petabytes of files, and hundreds of thousands of partitions. Storage and distribution may be expanded and contracted elastically.</p><p class="ql-align-justify"><strong>Permanent storage</strong></p><p class="ql-align-justify">Data streams can be securely stored in a clustered, long-lasting, fault-tolerant cluster.</p><p class="ql-align-justify"><strong>Extremely available</strong></p><p class="ql-align-justify">Clusters can be easily stretched across availability areas, or different clusters can be linked across geographic regions.</p><p><strong>Client Libraries</strong></p><p class="ql-align-justify">Write, read, and manage event streams in a variety of programming languages.</p>
------------------------------
Question: What is meant by transformations in Spark?
Answer: <p class="ql-align-justify"><strong>Transformations</strong> in Spark are <strong>functions</strong> that generate a new RDD from an existing RDD.&nbsp;It accepts RDD as input and outputs one or more RDD. <span style="color: rgb(89, 88, 88);">Filter, groupBy and map are the examples of transformations. Also, tr</span>ansformations transform a Spark DataFrame into a new DataFrame without changing the existing data, granting it immutability. In other words, an operation like select() or filter() does not alter the original DataFrame; rather, it returns the modified effects of the operation as a fresh DataFrame. Transformations are inherently <strong>lazy</strong>, which means that when we invoke an operation in RDD, it does not run instantly. Transformations may be divided into two types based on their dependencies:</p><ul><li class="ql-align-justify">A <span style="color: rgb(0, 71, 178);">narrow transformation</span> is any transformation under which a single output partition may be computed from a single input partition.</li><li class="ql-align-justify">A <span style="color: rgb(0, 71, 178);">wide transformation</span> is any transformation under which data from other partitions is read in, merged, and written to disk.</li></ul>
------------------------------
Question: What does SparkSession do?
Answer: <p><strong>SparkSession</strong> was implemented in Spark 2.0 and serves as an entry point to underlying Spark code for programmatically creating Spark RDD, DataFrame, and DataSet. <strong>Consider a scenario:- </strong>Multiple users want to use your cluster and run their queries on it. How are you going to deal with it? Each customer wants to have the same table name. To fulfil this type of situation, you would need to generate several spark contexts. It is, though, very complicated.</p><p>This is where the Spark session enters the picture. In spark 2.0, we can have multiple spark sessions but just one spark context.</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/d2665c97d0149568ca80429d4f4fc537.png"></p><p>In a nutshell, each user has their own Spark session, where they can set their own properties, configure their own settings, and have their own tables. And it will not be visible to anyone until and unless they create a global view.</p>
------------------------------
Question: What are some unsupervised machine learning algorithm?
Answer: <p>Unsupervised learning algorithms are useful in the absence of large quantities of labeled data and are trained to be self-sufficient. It tries to learn the patterns from the unlabeled datasets, thereby reducing the need of annotating the data.</p><p><u>Some of the most popular unsupervised learning algorithms are:</u></p><ul><li><strong style="color: rgb(93, 93, 93);">K-means clustering</strong><span style="color: rgb(93, 93, 93);"> is an algorithm that defines the features present in the dataset and is used to group certain bits with common elements into clusters, it takes a k value, and a defined distance function as input and forms clusters between them.</span></li><li><strong style="color: rgb(93, 93, 93);">Hidden Markov Models(HMM) </strong> is a weighted finite model with probabilities weight on the arcs, to indicate how likely a path is to be taken. It assigns unobserved attributes to observed events which are part of a sequence, by analyzing the pattern of a sequence of observed symbols.</li><li><strong>PCA</strong> is a dimensionality reduction algorithm where new features are generated, which represents the original feature dimensions in a lower dimension with a little loss of the total information.</li><li><strong>Association Rule</strong> is <span style="color: rgb(93, 93, 93);">a series of techniques that aims to uncover the relationships between different objects and is widely used in predictive analytics using unsupervised data. E.g., Apriori Algorithm, which is </span>useful in mining frequent itemsets and relevant association rules.</li></ul>
------------------------------
Question: What is the k-means clustering algorithm?
Answer: <p>Clustering is a type of unsupervised learning which aims at grouping unlabeled data which have a high degree of feature similarity. It is widely used to gain insights from the unlabeled data and can be broadly divided into 4 sub-types depending on the approach used to form the clusters. They are:</p><ul><li><strong style="color: rgb(48, 48, 48);">Hierarchical Clustering</strong></li><li><strong style="color: rgb(48, 48, 48);">Centroid-Based Clustering</strong></li><li><strong style="color: rgb(48, 48, 48);">Distribution-Based Clustering</strong></li><li><strong style="color: rgb(48, 48, 48);">Density-Based Clustering</strong></li></ul><p>The K-means clustering algorithm is one of the most popularly used algorithms which uses the Centroid-Based Clustering approach, i.e, it is important to<span style="color: rgb(48, 48, 48);"> specify the number of clusters and the algorithm then identifies which data points belong to which cluster/group of data. </span></p><p>The K- Means Clustering Algorithm requires the following inputs:</p><ul><li><strong>K</strong> = number of subgroups or clusters to be formed</li><li><strong>Sample </strong>or <strong>Training Set</strong> = {x1, x2, x3,………xn} (unlabeled data, with undiscovered feature similarity)</li></ul><p><span style="color: rgb(48, 48, 48);">Each cluster is uniquely different from one another and it is widely used owing to its </span>simplicity, robustness, and speed. K-means is an iterative algorithm i.e, the series of steps must be repeated at each iteration to make progress. There are mainly five steps to remember when applying the k-means clustering algorithm, they are:</p><ol><li>Assign a value for&nbsp;<strong><em>k</em></strong>&nbsp;which is the number of clusters to be created by the algorithm.</li><li>Randomly assign&nbsp;<strong><em>k</em>&nbsp;</strong>centroids using the unlabeled data.</li><li>Assign each data point to its closest centroid, at the first iteration and update the values along the way.</li><li>Recalculate the new cluster and update the corresponding centroids.</li><li>Repeat steps <strong>3 </strong>and <strong>4</strong> until convergence, such that each cluster is a well-defined, unique, group of data.</li></ol>
------------------------------
Question: What is computer vision?
Answer: <p>According to the definition by Wikipedia, "Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos". In essence, computer vision is a field of study as the name suggests, which allows computers to truly perceive their surroundings using images(2D) and videos(2D/3D). </p><p>It works much like traditional human vision, except humans have a head start. The human sight has the advantage of lifetimes of context,  to train how to tell objects apart, depending on memory and judgment which is in a process of development from the day we are born. Computer Vision<span style="color: rgb(85, 85, 85);"> is a multidisciplinary field that can broadly be labeled as a subfield of artificial intelligence and machine learning, which may involve the use of specialized methods, and equipment and make use of general learning algorithms. Some of the main industries, where Computer Vision is broadly used are:</span></p><ul><li>Optical character recognition (OCR)</li><li>Retail (e.g. automated checkouts)</li><li>Medical imaging</li><li>Automotive safety</li><li>Motion capture (mocap)</li><li>Surveillance and Security</li><li>Fingerprint recognition and biometrics</li></ul><p><span style="color: rgb(43, 43, 43);">The main working principle behind computer vision algorithms that are widely used today is based on the principles of pattern recognition. The computers are trained using a massive amount of visual data(obtained using cameras and video cameras) mainly images and video which they process, label, and find patterns in the labeled objects to perform a varied range of tasks, .</span></p>
------------------------------
Question: What is the difference between a Spark driver and an executor?
Answer: <p class="ql-align-justify">Spark uses a Master-Slave architecture, with one central coordinator and several hierarchical worker nodes. <span style="color: rgb(0, 71, 178);">Spark Driver is the central coordinator</span>, and it deals with all of the Workers. The Spark driver is a program that is executed on the master node. It runs the user code and generates a SparkSession or SparkContext. It keeps records of data in the form of metadata that was cached in the memory of the Executor. Each Worker node is made up of one or more <span style="color: rgb(0, 71, 178);">Executor(s) who are in charge of carrying out the Task</span>. Executors must register with Driver. The Driver has complete knowledge of the Executors at all times. The Executor node is located in the Worker node. Executors are launched in collaboration with the Cluster Manager at the outset of a Spark Application. The Driver launches and removes them dynamically as required. The executor's main responsibility is to run a single Task and return the outcome to the Driver. It has the ability to cache data in the Worker node.</p><p class="ql-align-justify">This collaboration between Drivers and Workers is known as the Spark Application.</p><p class="ql-align-justify"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/74ad5e9aa053e8b71e983bf79ecb24e1.png"></p><p class="ql-align-justify"><br></p>
------------------------------
Question: Why is accuracy not considered a reliable metric for a machine learning model?
Answer: <p>Accuracy is one of the most widely used metrics, used to quantify the performance of a machine learning model trained using a particular dataset. Although it's very simple and widely used, to evaluate a model's performance, accuracy is not a reliable metric for correctly quantifying the effectiveness of a model.  Classification accuracy is defined as the "percentage of correct predictions", this means since the dataset itself is imbalanced, the percentage of misclassified/minority class data would be much lesser in numbers leading to a false accuracy score.</p><p>For example, let's take an instance of a cancer detection system, realistically the majority of the patients sampled at random are unlikely to have cancer, therefore class A i.e, instances of people not having cancer is extremely high. Let's assume the data has 99% of cases as <strong>class A (NOT CANCER)</strong>, and only 1% of instances are <strong>class B (CANCER)</strong> then predicting that every case is a class A i.e, NOT CANCER will make the model have an accuracy of 99%, which signifies a very good model. However, in practice this is a very bad model as it misclassifies the people who have cancer and are in immediate need of treatment as class A i.e, they do not have cancer, leading to potential loss of life. This is the Accuracy Paradox which shows how high accuracy isn't the absolute score to evaluate model effectiveness and is a well-established concept which further elaborates on the basis as to why accuracy isn't a good metric to classify a model’s actual effectiveness as it is susceptible to the imbalance in data.</p><p><br></p>
------------------------------
Question: What is the relation between YARN and Spark?
Answer: <p class="ql-align-justify">To begin with, there is no direct connection among Spark and Yarn since they are distinct. Yarn seems to be a cluster control technology, whereas Apache Spark is an in-memory distributed data processing engine. Spark doesn't even need YARN, but it could run under Yarn if you're using Spark to access Hadoop data.</p><p class="ql-align-justify">The relation would be between <strong>Spark executors</strong> and the <strong>Yarn container</strong>. As Spark is operated on YARN, each Spark executor acts like a YARN container. This implies that the number of containers generated by a Spark application will always be the same.</p><p class="ql-align-justify">In cluster mode, the Spark driver operates within an application master process operated by YARN on the cluster, and the client may exit after the application is launched.</p><p class="ql-align-justify">The Spark driver operates in the client process in client mode, and the application master is only there to request services from YARN.</p>
------------------------------
Question: What is MapReduce
Answer: <p class="ql-align-justify"><strong>MapReduce</strong> is a distributed computing program model that allows one to write applications that process massive quantities of data. It is an integral part of the Hadoop Ecosystem. MapReduce is a processing method based on the divide-and-conquer strategy. The MapReduce algorithm consists of two phases: <span style="color: rgb(0, 71, 178);">map and reduce</span>. In the map phase instances of a processing script is sent to all the nodes in which data is stored. The instances of the script operate on the data of their node in parallel. The result of each instance of the map script is sent to instances of the reduce script whose job is to aggregate and sort the data sent by the instances of the map script, which is saved in HDFS. The MapReduce framework's real advantage is its scalability. If a MapReduce program is written, it is simple to extend it to run over a cluster of thousands of nodes.</p>
------------------------------
Question: What is Apache Sqoop?
Answer: <p>Simply, <span style="color: rgb(0, 71, 178);">SQL + Hadoop = Apache Sqoop</span></p><p>Apache Sqoop is a method for transferring large amounts of data between Hadoop (HDFS, HBase) and relational database servers ( MySql, Oracle). It is used to import data from RDBMS to Hadoop and to export data from Hadoop back to RDBMS. It supports importing and exporting data through a CLI (command line interface). Developers only need to include basic details such as database authentication, source and destination processes, etc., and Sqoop can handle the rest. Sqoop employs a connector-based structure that enables modules to link to new external entities.</p>
------------------------------
Question: What is Apache Flume?
Answer: <p><br></p><p>Apache Flume is a data ingestion platform for HDFS. It is an integral part of the Hadoop Ecosystem. It <span style="color: rgb(0, 71, 178);">collects and aggregates massive amounts of streaming data</span> ( such as log files, events from various sources like social media messages). Flume is fault-tolerant and has a stability feature for failure recovery.</p>
------------------------------
Question: Which Hadoop component is best for importing structured data into HDFS?
Answer: <p><strong>Sqoop</strong> is the best Hadoop component for importing structured data into Hadoop Distributed File System. Structured data usually resides in relational databases. Sqoop is used to import data into Hadoop from relational databases and data repositories, and these are the primary repositories of structured datasets.</p>
------------------------------
Question: Which is better for processing streaming data: Spark or Storm?
Answer: <p>Only stream processing is possible with Storm. But the market needs a standardized approach that enables in the same framework different forms of processing, such as batch processing, stream processing, digital processing, and iterative processing. Apache Spark serves exactly this need. It also provides a general-purpose computing engine. As a result, Apache Spark is very easy for developers to use. We can also combine it very well with Hadoop. Hence, <span style="color: rgb(0, 71, 178);">Spark Streaming is better than Storm</span>.</p>
------------------------------
Question: What is Apache Flink?
Answer: <p class="ql-align-justify"><strong>Apache Flink</strong> is just the next generation engine for stream processing after Apache Spark. It is comparable to 4G for Big Data collection as it is an open-source platform designed specially for <span style="color: rgb(0, 71, 178);">fast stream processing</span>. Flink's stream processing functionality allows it to process rows upon rows of data in real time. It enables us to develop software with low latency and high throughput. Flink has a large number of libraries for graph analysis and machine learning. Application state of Flink is rescalable , which ensures that resources can be added when the app is running.</p>
------------------------------
Question: What is the difference in functionality between Sqoop and Flume?
Answer: <p class="ql-align-justify">Both Sqoop and Flume are utilities to load data into HDFS. However,</p><ul><li class="ql-align-justify"><span style="color: rgb(0, 71, 178);">Sqoop</span> is used to load data from relational databases into HDFS, whereas <span style="color: rgb(0, 71, 178);">Flume</span> is used to capture large amount of streaming data.</li><li class="ql-align-justify">Flume works well with streaming data sources created continuously in the Hadoop environment, such as log files from multiple servers, while Apache Sqoop is configured to work with any kind of relational database system.</li><li class="ql-align-justify">Apache Flume has an agent-based architecture, which means that the code written in Flume is known as an agent, and it is responsible for fetching data, while Apache Sqoop has a connector-based architecture. Sqoop's connectors understand how to bind to multiple data sources and retrieve data.</li></ul>
------------------------------
Question: What is data science?
Answer: <p>According to the definition by <a href="https://en.wikipedia.org/wiki/Data_science#:~:text=Data%20science%20is%20an%20interdisciplinary,broad%20range%20of%20application%20domains." rel="noopener noreferrer" target="_blank">Wikipedia</a>, "Data science is an interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data". To explain what it actually is, we must be aware of the domains which are interlinked with it. Data Science is a vast field and is one of the most widely sought after fields by professionals all across the world, in the last decade. The attached diagram can illustrate the roles of a data scientist. <span style="color: rgb(74, 74, 74);">Data Science can be rightfully explained as a blend of various tools, algorithms, machine learning principles, and domain knowledge with the goal to discover hidden patterns from raw big data. </span></p><p class="ql-align-center"><br></p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/6677612ae1c22f471e35f5a13013b369.png"></p><p class="ql-align-center"><a href="https://www.ironhack.com/en/data-analytics/data-science-data-analytics" rel="noopener noreferrer" target="_blank">SOURCE</a> explains the domain data science</p><p>With the advancement of technology, data is the most valuable resource in today's world and all organizations are trying to use the raw data, that they collect through their product and services to serve the customers more so that they would purchase their services. Data Science is a field that encompasses the following :</p><ul><li><strong>Data Extraction</strong>: Extracting the data from sources in a readable format.</li><li><strong>Data Wrangling</strong>: Pre-processing and making sense of raw, unstructured data.</li><li><strong>Data Analysis</strong>: Using tools like (Tableau, PowerBI, etc) and also predictive analytics tools/clustering(Machine Learning algorithms and principles) to analyze the processed data and generate understandable reports.</li></ul>
------------------------------
Question: How does regularization minimize the risk of overfitting?
Answer: <p>A model which fits the training data too well i.e, when the model is aware of all noise and details of the training data it becomes unable to perform on any data, except the training data as the model becomes unable to generalize having picked up noise and details of fluctuations. Overfitting is more likely to occur in non-linear and non-parametric models like a Decision Trees. To put it into perspective, the attached figure represents overfitting on training data as compared to appropriate fitting.</p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/60a490c8b6c98774946c4a8cdda1672c.png"> </p><p class="ql-align-center"><a href="https://analyticsindiamag.com/regularization-in-machine-learning-a-detailed-guide/" rel="noopener noreferrer" target="_blank">SOURCE</a></p><p><span style="color: rgb(64, 64, 64);">Regularization can be loosely defined as a technique that adds information to a model to prevent the occurrence of overfitting. This technique discourages the model to learn more complex, in-depth information such as noise, hence making it more flexible to changes in the data. It brings uniformity or rather regularizes the parameters of the model to r</span><span style="color: rgb(36, 39, 41);">educe the variance by simplifying the function to ultimately increase the bias of the model hence reducing the expected error. In a general case, models with lots of features are at a greater risk to overfit, owing to the high complexity of the model, regularization also discourages using highly complex features with multiple intricate parameters, to obtain simpler models which can perform well in a general scenario, irrespective of the dataset used.</span></p>
------------------------------
Question: what is the random forest method in machine learning
Answer: <p>Random forest is a supervised machine learning algorithm, based upon the concept of ensemble learning. Ensemble learning is a technique that utilizes multiple weakly supervised models to perform the predictive task, parallelly on the same dataset in groups, to improve the overall performance of single models. The random forest algorithm as the name suggests uses multiple decision trees and combines their results based on the majority vote of prediction, to decide the final result. Since the random forest combines multiple decision trees to predict the final result, it is highly likely that some decision trees predict the correct output, while others fail at the task. The final algorithm utilizes the concept of majority vote such that as long as the majority number of trees have predicted the correct output, it will be chosen as the actual output of the random forest.</p><p><u>Some of the major advantages of this algorithm, over many of its counterparts, are:</u></p><ul><li>Random Forest although suitable for classification, is capable of performing both classification and regression tasks.</li><li>The algorithm is useful for handling large datasets with high dimensionality.</li><li>The accuracy of the model is increased owing to using a higher number of simultaneous decision trees and overfitting is thereby prevented.</li></ul><p><br></p>
------------------------------
Question: What are sensitivity and specificity as metrics in machine learning
Answer: <p><strong>Sensitivity </strong>is the measure of the proportion of actual positive cases that got predicted as positive or rather the true positive rate and is often referred to as the Recall of the model. High sensitivity values indicate a higher value of the true positives and a lower value of the false negatives, and vice-versa. Models with high sensitivity are preferred in healthcare and financial domains, owing to the high true positive rate.</p><p>Let <strong>TP</strong> = True Positive, <strong>FP</strong> = False Positive, <strong>TN</strong>= True Negative, and <strong>FN</strong>= False Negative. Then,</p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/4be8adc4b2ae0717f58d4902f4581fc8.png"></p><p class="ql-align-center"><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" rel="noopener noreferrer" target="_blank">Sensitivity Formula</a></p><p><strong style="color: rgb(34, 38, 53);">Specificity</strong><span style="color: rgb(34, 38, 53);"> is the</span> proportion of actual negatives, which got predicted as the negative or rather the true negative rate of the model. A higher value of specificity represents a high true negative rate and low false-positive rate, and conversely, low specificity represents a low true negative rate and high false-positive rate.</p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/280da8479f7d7502a52ea6a6055b6943.png"></p><p class="ql-align-center"><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" rel="noopener noreferrer" target="_blank" style="background-color: rgb(255, 255, 255);">S</a><a href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity" rel="noopener noreferrer" target="_blank">pecificity Formula</a></p><p><span style="background-color: rgb(255, 255, 255);">Depending on the domain, and the requirements of the model either of the two can be maximized, at the cost of the other value to meet the requirements. </span>Prioritizing sensitivity of an algorithm over specificity is important when the goal is identifying particular features in a population, however, when the goal is to improve the property of the model to include a vast sample of data and its generalizing capability, model specificity is more important.&nbsp;</p>
------------------------------
Question: What is the functionality of Apache Storm?
Answer: <p class="ql-align-justify">The <span style="color: rgb(0, 71, 178);">primary function of Apache Storm</span> is to receive massive amounts of data coming in very fast from multiple sources, interpret it, and publish real-time reports to a UI or any other location, without retaining any individual data. It is a fault-tolerant distributed real-time computing system. <span style="color: rgb(0, 71, 178);">Fault tolerant </span>as it can run on a consistent stream of data and restore data even though the connection is lost. <span style="color: rgb(0, 71, 178);">Distributed</span> since it operates in a similar way to the MapReduce model, where processing is distributed through several nodes that run in parallel to achieve high throughput. Since it can handle streaming data, it is capable of <span style="color: rgb(0, 71, 178);">real-time computation</span>. And even if the data processing is distributed through hundreds of nodes, Apache Storm ensures that each incoming message is still properly analyzed.</p>
------------------------------
Question: What is the function of Apache ZooKeeper?
Answer: <p class="ql-align-justify">Zookeeper is a centralized coordination service for- <strong>Naming service (</strong>Using names to identify nodes in a cluster. It's like DNS, but for nodes), <strong>Configuration management (</strong>The most recent and up-to-date device configuration information for a joining node), <strong>Cluster monitoring</strong> (maintains the status of a node joining or exiting the cluster as well as the node status in real-time), <strong>Leader election</strong> ( selects a node to serve as a leader for the purposes of coordination), <strong>Service for locking and synchronization</strong> (locks the data while it is being updated. It aids in the automated fail recovery of other distributed software such as Apache HBase). ZooKeeper operates on a basic client-server model, with clients being nodes (i.e., machines) that use the service, and servers being nodes that deliver the service. Each ZooKeeper server can support a large number of simultaneous client connections.</p><p class="ql-align-justify">In a nutshell, Apache ZooKeeper is a distributed collaboration service for handling a wide number of hosts. Apache Zookeeper, with its basic architecture and API, addresses the problem of distributed environment management. The ZooKeeper architecture offers a comprehensive mechanism for addressing all of the problems that distributed systems face.</p>
------------------------------
Question: What is the difference between Ambari and ZooKeeper?
Answer: <p class="ql-align-justify">1) Apache Ambari is an open platform for provisioning, handling, and tracking Apache Hadoop clusters, while Zookeeper is a structured service for configuring, identifying, and delivering distributed synchronization.</p><p class="ql-align-justify">2) Apache Ambari is a web interface, while Zookeeper is a server that is open source.</p><p class="ql-align-justify">3) The Apache Ambari status is maintained via APIs, while the Zookeeper status is maintained via znodes.</p><p class="ql-align-justify">4)Ambari falls under the "Monitoring Tools" division of the software stack, while Zookeeper falls under "Open Source Service Discovery."</p>
------------------------------
Question: How do Spark and Kafka interact?
Answer: <p>Spark streaming may benefit from a messaging and integration tool like Kafka. Kafka serves as a central hub for real-time data sources, which are analyzed using complex algorithms in Spark Streaming. After the data has been processed, Spark Streaming can post the results to another Kafka topic or store them in HDFS, databases, or dashboards. The overall process is like:- Data can be ingested from a variety of sources, including Kafka -&gt; Spark Stream constantly collects input data -&gt; breaks the data into batches -&gt; Spark Engine processes the data -&gt; results in a stream of batches -&gt; Finally, stored data can be sent to filesystems, databases, and live dashboards.</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/2f5fac5f9fbcd2baa8cbdbc78726cafd.png"></p>
------------------------------
Question: What is LDA in machine learning
Answer: <p>LDA stands for <strong style="color: rgb(73, 84, 94);">Linear Discriminant Analysis </strong><span style="color: rgb(73, 84, 94);">is one of the most commonly used dimensionality reduction techniques. There exists something called the "Curse of Dimensionality" which in simple terms states that which higher-dimensional data, the chance of overfitting the model vastly increases as excess features are not good for model performance. LDA is a </span>supervised classification algorithm that aims at reducing the dimensionality i.e, the number of features, and chooses only the best classes related to the dependent variable to significantly improve model performance and overcome the problem of overfitting. LDA is the popular choice specifically for classification problems, owing to its simplicity, adaptability, and better performance compared to its dimensionality reduction counterparts.</p><p><u>Some of the practical use cases of LDA are:</u></p><ul><li>Face recognition is carried out by representing faces using large amounts of pixel values. LDA can be used in trimming down the number of features to reduce the chance of model overfitting.</li><li>Linear discriminant analysis (LDA) is widely used to classify a patient's disease state based upon multiple parameters and the medical treatment he is subjected to. LDA maximizes performance to give doctors an idea about the current state of the patient and carry out the necessary treatment.</li></ul>
------------------------------
Question: What is k-fold cross validation?
Answer: <p>In building a machine learning model it is usual to randomly choose a subset of the labelled data as a test set (or a dev set). Even though the test set is randomly chosen, it is possible that this set may be unintentionally biased, i.e., it may be a poor choice of data in terms of which to test the model. One strategy for avoiding such a bias is to use K-fold cross-validation.</p><p>K-Fold Cross Validation is a widely used data partitioning strategy that effectively splits the datasets to aid in building a more generalized model.  It is used to estimate the effectiveness of an ML model on unseen data to have a proper test of a model's effectiveness in practice. A limited sample is utilized to estimate how the model is expected to perform in a general scenario using unseen data. The K-fold cross-validation technique has a parameter called K which is utilized to specify the number of groups the training data is to be divided in. A poorly chosen value for the parameter 'K' may result in misrepresentation of the model effectiveness, i.e, an overestimation of the model's actual skill. Generally, it is recommended to keep the K value as 10 and optimize it using evaluation metrics to find the perfect number of subsets.</p>
------------------------------
Question: What are support vectors in SVM?
Answer: <p>According to the definition from Wikipedia, "Support-vector machine (SVM) is a supervised learning model with associated learning algorithms that analyze data for classification and regression analysis."</p><p>Support vectors are the data points that are the most critical elements of the training set. They lie the closest to the decision surface (or hyperplane) and are the ones that are the most difficult to classify. The hyperplane used to separate the two classes(in case of a binary classification problem), depend on the support vectors i.e., these data points directly influence the position of the hyperplane. The main challenge of finding the correct hyper-plane, although dependent on the Support Vectors, is an optimization problem with the main goal being to find the optimal line to divide the two classes, such that the margin between the two classes is as large as possible and no data points directly lie between the decision boundary of the hyperplane.</p><p><br></p><p><br></p><p><br></p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/9c7fb83d21c032f2d7597b4b1fbf887f.png"></p><p class="ql-align-center"><a href="https://web.mit.edu/6.034/wwwbob/svm.pdf" rel="noopener noreferrer" target="_blank">SOURCE</a></p><p>The above diagram is a visual representation of the SVM algorithm in practice, and the circled points are the <u>SUPPORT VECTORS</u> which are very close to the hyperplane but still not completely within the margin with distance 'd'.</p>
------------------------------
Question: What is a restricted Boltzmann machine?
Answer: <p>Restricted Boltzmann machine (RBM) is a special class of 2-layer generative neural network that is commonly used for dimensionality reduction, classification, regression, collaborative filtering,  and topic modeling.&nbsp;A Boltzmann machine has a visible input layer and a hidden layer. It's a two-layer neural net that makes stochastic decisions as to whether a neuron should be on or off. RBMs have a restricted number of connections between the hidden and the visible layers and each node in the hidden layer is connected to each node in the visible layer. The learning procedure of an RBM is simpler compared to a general Boltzmann machine owing to the visible units in RBM being conditionally independent of the hidden units, and vice-versa. RBMs can be defined as a symmetrical bipartite graph. <em>Symmetrical</em> means that each visible node is connected to each hidden node, and <em>Bipartite </em>means that it has two parts/layers. Therefore, while RBMs have many uses, proper initialization of weights to facilitate later learning and classification is one of their chief advantages. The main disadvantage of RBM is that they don't produce the most stable results compared to other feed-forward networks. In most cases, a dense-layer autoencoder outperforms an RBM.</p>
------------------------------
Question: What is the relation between AUC and ROC curves?
Answer: <p>The receiver operating characteristic curve (ROC) curve is a plot that graphically represents the effective performance of a binary classifier as a function of its cut-off threshold. ROC is a graphical representation of the <strong>true positive rate (TPR) (Y-axis)</strong>, otherwise known as the sensitivity of the model, and the&nbsp;<strong>false positive rate (FPR) </strong>on the<strong> (X-axis).</strong></p><p>The main advantage of the ROC curves is that they are insensitive to changes in the&nbsp;distribution of the class, and won't be affected by changes in the proportion of the positive and negative instances.&nbsp; It is mainly utilized to choose the perfect classification threshold for the classifier.</p><p><u>The mathematical formula to calculate TPR and FPR:</u></p><p class="ql-align-center"><span style="color: rgb(41, 41, 41);"><img src="https://lh6.googleusercontent.com/rXRlwq4ereaYH9o8qbz2S2bTbjVWInz5e6-f5xD9cH6D2EfWCq9sh66XhPZvKXy-f5rM7YGRgNqaZ7eIPKClMbW6tsG_FR9NM7QGPMJSEuTAyw6ttjm8nTHU8JeDsIEhOj7Q_D9N" height="124" width="194"></span></p><p><span style="color: rgb(41, 41, 41);">ROC is a 2-D representation of classifier performance and cannot be used as a one-stop numerical value of model performance. To compare different classifiers, the </span><strong style="color: rgb(41, 41, 41);">Area under the Curve (AUC</strong><span style="color: rgb(41, 41, 41);">) metric is popularly used, and based on it, the parameters are tuned to improve classifier performance. Simply speaking, AUC is a numerical representation of the area under the ROC curve. </span>The AUC metric, provides an aggregate measure of performance and model effectiveness, across all possible classification thresholds and is invariant of the classification threshold utilized, i.e, it measures the quality of a model, regardless of the chosen threshold for classification.</p>
------------------------------
Question: What is cloud computing?
Answer: <p class="ql-align-justify">The cloud is just a location where you may store your data remotely. Cloud computing is the provision of on-demand computer services through the internet (cloud) on a 'pay-as-you-go' basis. You generally only pay for the cloud services you use. Rather than managing data on local storage devices, cloud computing allows them to be saved and accessed through the internet. Cloud computing provides computing, networking, and storage resources on demand. Hence, about any service that does not need you to be physically close to the computing hardware you are using, can now be provided via the cloud.</p>
------------------------------
Question: Is there a good way to select k for the k-means algorithm?
Answer: <p>The K-means algorithm is an unsupervised learning algorithm. It utilizes unlabeled structured data, to form a pre-user-defined (k) number of clusters from the data. These clusters are unique, depending on the features of the data points in each cluster.</p><p class="ql-align-center"><br></p><p>Since the K value is a crucial parameter, having a way to identify the correct number of clusters is important. There is a popularly used method known as the ELBOW METHOD, which tackles just this problem. The steps are as follows:</p><ul><li>Multiple values of K are chosen and the cluster distance to the centroid is calculated for each value of K.</li><li>A graph is plotted between the different values of K (in the X-axis) and the within-cluster distance to the centroid ( in the Y-axis).</li><li>Find the point on the curve at which the average within-cluster distance begins to plateau. This is the 'elbow' of the curve.. </li><li>The optimum minimum value for K is the point on the X-axis where the 'elbow' occurs. </li></ul>
------------------------------
Question: What is an operating system?
Answer: <ul><li><span style="background-color: transparent;">An operating system is a collection of software that handles computing resources including the central processing unit, storage devices, and memory.</span></li><li><span style="background-color: transparent;">The operating system serves as a bridge between the user and the computing hardware.</span></li><li><span style="background-color: transparent;">The operating system (OS) does a variety of functions, including detecting keyboard input, submitting output to the computer, and keeping track of files and folders.</span></li><li><span style="background-color: transparent;">An operating system aims to have a comfortable and effective environment in which a user can execute programs.</span></li><li><span style="background-color: transparent;">An operating system is stored on the hard drive.</span></li><li>Every computer system must have at least one operating system to run other programs.</li><li><span style="background-color: transparent;">Some examples of OS are - Windows, Linux, macOS, Android, iOS.</span></li></ul>
------------------------------
Question: What is internet of things?
Answer: <p class="ql-align-justify">The <strong>Internet of Things</strong> is all about connecting "things" to the internet so that they can function intelligently. Things are actual items such as instruments, work equipment, buildings, clothes,&nbsp;and construction equipment, etc. IoT applies to the billions of physical devices that are currently wired to the internet and storing and transmitting data all over the globe. The internet of things (IoT) is represented by the combination of three components: a device, an embedded system, and the internet.</p><p class="ql-align-justify"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/6ead30be3655202cba711e1196516cce.png"></p><p class="ql-align-justify"><strong>Consider smart refrigerator example:</strong> A refrigerator that can search the internet for the best prices depending on its location, order food, and order a water filter when necessary. It has the ability to request routine repairs or cleaning services. It may send specific details to the owner by email, for example.</p>
------------------------------
Question: What is the difference between primary memory and secondary memory?
Answer: <p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/4daa6b550ceae2236d408a44cc7cf66e.png"></p>
------------------------------
Question: What is a database?
Answer: <ul><li>A database can be defined as an ordered collection of data electronically saved in a computer so that it will be easy for the user to access or manage the data. The important point is that it is a form of persistent storage.</li><li>A database facilitates the electronic collection and processing of records.</li><li>The database is generally controlled by a software called DBMS (Database Management System).</li><li>In a relational database, the data is stored in the form of tables.</li></ul><p><br></p>
------------------------------
Question: What is meant by a client server architecture?
Answer: <p>Client-server architecture is a computer network architecture which consists of clients and server. In this, there are many clients in the network and there is a server that fulfills the requests of the clients. The clients send the request to the server for the resource they need (files, web pages, emails, etc.) and then the server manages the request and provides them these resources. This architecture is also considered a distributed computing system. The clients are known as remote systems and the server is known as host systems. Client-server architecture is of 4 types: 1-tier architecture, 2-tier architecture, 3-tier architecture, N-tier architecture. Some examples are Web servers, Mail servers, File servers, etc.</p>
------------------------------
Question: What is computer networking?
Answer: <p class="ql-align-justify">A computer networking is a group of computers that uses a set of common protocols for communication over digital interconnections. In other words, we can say computer networking is a process for connecting two or more computers either by cables or WiFi, for sharing data, files, providing technical help, and communicating for the business purpose. And the technology which is used to connect multiple computer systems at multiple different geographical locations for serving this purpose is the Internet.</p><p class="ql-align-justify">A computer network is mainly of 4 types :</p><p class="ql-align-justify">1)LAN(Local Area Network)</p><p class="ql-align-justify">2)PAN(Personal Area Network)</p><p class="ql-align-justify">3)MAN(Metropolitan Area Network)</p><p class="ql-align-justify">4)WAN(Wide Area Network)</p>
------------------------------
Question: what is a file server?
Answer: <p class="ql-align-justify">A file server is a type of server which provides access to the files saved on the server. A file server can fetch and store all types of files such as images, videos, an audio file, a database, and many other types of documents. Any client machine linked to a file server over a network can retrieve these files. Since the files are transferred over a network, users do not require any physical medium for transferring files such as  pen-drives floppy disks, etc. Any computer of our choice can be set up as a host and plays the role of a file server. File servers are typically used in schools and workplaces where users link their client computers through a local area network.</p>
------------------------------
Question: What is a compiler
Answer: <p class="ql-align-justify">A compiler is a computer program that is used to translate source code from a high-level programming language to a low-level programming language in order to create an executable form of the source code that can be easily understood by a computer, or to translate the source code to machine code (0's and 1's form). A compiler has the common feature of being a huge program with error checking and other capabilities. The key difference between a compiler and an interpreter is that the interpreter translates the code line by line and will not move further until the error is corrected, while the compiler translates the entire source code at once. Compilers are also divided into 3 major parts: Single-pass Compiler, Two-pass Compilers, and Multi-pass Compilers.</p>
------------------------------
Question: What is tf-idf?
Answer: <p><em>Term Frequency-Inverse Document Frequency (TF-IDF)</em> is used to statistically measure the weight or importance of a word in a particular corpus or document.</p><p><strong>TERM FREQUENCY </strong>can be simply explained as the frequency of a particular word with respect to the document/corpus it belongs to. It is not the best method to calculate the effective importance of a word as terms such as <em>"the", "it", "to" etc</em>, will gain more relevance if only the frequency of each term is calculated. They are commonly occurring words in a sentence and do not convey important characteristic information, unlike words such as <em>"brown", "dog", etc</em> which can be potentially valuable for the document to make sense (although less in frequency).</p><p>The<strong> INVERSE DOCUMENT FREQUENCY</strong> is the numerical representation of how much information a particular word provides across all documents in a corpus. A high TF-IDF value can be obtained by getting a high term frequency in the particular document and a low document frequency of the term in the whole collection of documents. When TF*IDF is high, the frequency of the term is less and vice versa.</p><p><u>A simple method to calculate the TF*IDF score is:</u></p><p>TF(X) = (Frequency of X in a document)/(The total number of terms in the document)</p><p>IDF(X) = log_e(The total number of documents/The number of documents having the term X)</p><p>Therefore,</p><p>TF(X)*IDF(X) = Weight of term X in a document.</p>
------------------------------
Question: What is polymorphic association in Ruby on Rails?
Answer: <p>A polymorphic association is one `belongs_to` association that enable a class to belong to more than one model through this single association. In so far as the database columns are concerned this is represented through two fields, a foreign key field and a string field that specifies the name of the model it belongs to. See https://guides.rubyonrails.org/association_basics.html#polymorphic-associations for more details.</p>
------------------------------
Question: Is Ruby dynamically typed or statically typed?
Answer: <p>Ruby is a dynamically-typed language. This is why you can change, on the fly, the value of a variable from one to another even if the new value is of a different class.</p><p>E.g. `a = 1; a = false;` will work without error.</p><p>Some might argue that Ruby is duck-typed, which means that the type (or class, in the case of Ruby) is less important than the methods it defines. E.g. the String class defines a method `chars` like so `"123".chars=`[1,2,3]`. This would not work if "123" was not a string. If we try `123.chars` we get `NoMethodError: undefined method `chars' for 123:Integer`.</p>
------------------------------
Question: Can Apache Spark support real-time data processing?
Answer: <p class="ql-align-justify">Yes, Apache spark supports real-time data processing  Spark lets you run programs up to 100 times faster than Hadoop. It enables distributed processing of large amounts of data. But, Spark does not completely support real-time data stream processing in the sense of processing incoming piece of data as it comes in. Rather, the live data stream is partitioned into micro batches and processed batch by batch. As a result, it does not allow complete real-time computation but comes close. </p>
------------------------------
Question: What is the difference between an application program and an operating system program?
Answer: <ul><li><span style="background-color: transparent;">The main difference between an operating system program and an application program is that an operating system is a framework that is the interface between the client and the hardware while application program are used to perform certain dedicated tasks.</span></li><li><span style="background-color: transparent;">An operating system runs in the background and acts as a platform whereas application programs run in the foreground and interact with the user.</span></li><li><span style="background-color: transparent;">Applications programs depend on operating systems to run for a user. Operating System is required if we want to use applications.</span></li></ul><p><br></p><p><span style="background-color: transparent;">Examples of operating systems are Windows, Linux, macOS etc, and examples of application programs are Gmail, MS Word, notepad, paint, etc.</span></p>
------------------------------
Question: What is artificial intelligence?
Answer: <p class="ql-align-justify">Although there are many definition of artificial intelligence, the one we prefer is one of the oldest, attributed to Patrick Winston, an AI pioneer. He defined Artificial Intelligence as the branch of computer science that is aimed at getting computers to do tasks and produce behavior that would require cognition/intelligence if done by humans. Some examples are recognizing images, navigating across a room or a street, responding meaningfully to linguistic utterances, translating from one language to another, etc. Note that this definition doesn't presuppose that computers should produce intelligent behavior using cognitive processes similar to human cognitive processes. Indeed, as far as this definition is concerned, the computer does not have to use any process that even resembles a human cognitive process. </p><p class="ql-align-justify">Although artificial intelligence is a branch of computer science, it also draws from subjects such as psychology, linguistics, mathematics, philosophy, etc.</p>
------------------------------
Question: What is the difference between a web client and a web server?
Answer: <ul><li><span style="background-color: transparent;">Web server is the platform that helps the different servers to communicate with different Clients and these clients are the web clients which use servers. Means a </span>web client is a person who pays to be served, and a web server is a service provider who runs a website on a server.</li><li><span style="background-color: transparent;">Web clients communicate to web servers through Hyper Text Transfer Protocol(HTTP) and the web servers will send the HTTP responses to the clients.&nbsp;</span></li><li><span style="background-color: transparent;">Web servers give a place to store and coordinate the pages of the site and the clients use these pages to get information.&nbsp;</span></li><li><span style="background-color: transparent;">The web server is the one who hosts a website on the internet and the web client is the one who uses that website.</span></li><li><span style="background-color: transparent;">The simplest example to understand the server-client difference is that our browser is a web client and that machine that gives us the results is a web server.</span></li></ul>
------------------------------
Question: What is distributed computing?
Answer: <p><strong>Distributed computing</strong> system is one in which multiple software components run on multiple machines but seem like running on a single system. In this, the machines running the software components are connected through a local area network when the systems are physically close, and connected through a wide area network when the systems are geographically distant. Distributed computing is far better than a centralized computing system in reliability, scalability, and flexibility.</p><p>An example that we consider for distributed computing is multi-player online gaming. We play the same game but at a distance on the same network which will show that we are running different programs on different systems but will act as a single system.</p>
------------------------------
Question: What is computer graphics?
Answer: <p class="ql-align-justify">Computer graphics is the method of making computer-generated images with the aid of programs. Graphics are made up of several pixels (smallest graphical unit). <span style="background-color: transparent;">Computer graphics is the use of computers to make and control pictures on a displaying device. It involves programming procedures to make, store, change, address pictures.</span><strong style="background-color: transparent;">&nbsp;</strong><span style="background-color: transparent;">The biggest example of computer graphics is GAMES, as different things are displayed while playing games and those can only be done with the help of graphics. </span></p><p class="ql-align-justify"><span style="background-color: transparent;">Types of computer graphics:- </span><strong>Interactive computer graphics</strong> contains two-way communication between the computer and the user. By supplying the observer with an input system, he is given some power over the image. When the machine receives signals from the input unit, it will change the displayed image. In <strong>non-interactive computer graphics</strong>, also known as passive computer graphics, the image is merely the product of a static stored - program and will operate in accordance with the instructions given. Screen savers are an example.</p>
------------------------------
Question: What is human action recognition?
Answer: <p>Human Action Recognition (HAR) is an ongoing sub-domain of computer vision, which utilizes sensors to collect data and utilizes machine learning algorithms (SVM, Decision Tree, etc.), and Neural Networks (CNN, RNN, GCN, etc) to train the model with the goal of enabling machines to analyze, understand, and uniquely identify human actions. The data is generally accumulated using sensors, there are mainly two types of widely used sensors for HAR, they are:</p><ul><li><strong>RGB Sensor</strong> - This is the most commonly used method, which makes use of the frames, extracted from images and videos captured using RGB cameras. Since RGB sensors are visual in nature, common challenges such as low lighting, large distance, etc, can severely harm the models' performance and at the same time cause a breach in privacy.</li><li><strong>Depth Sensor</strong> - Depth sensors such as the Microsoft Kinect have enabled the task of perceiving detailed information to differentiate similar tasks from one another, traditional RGB cameras are unable to perceive the 3-dimensional data of human action, making it unfit to differentiate between similar actions. Depth sensors, although more complex and computationally expensive, overcome the problems of traditional HAR.</li></ul><p>This task is challenging owing to large variations in the model performance due to changes in background settings, differences in physique and techniques of people performing the actions, and also similarity between two separate actions such as (running vs jogging).</p><p>Human Action Recognition is used in many spectrums of our day-to-day lives, for e.g., Banks use HAR to detect suspicious humans, HAR is also used in modern-day VR games to make the character move according to the human's motion among many real-life use cases.</p>
------------------------------
Question: What is the difference between 2d and 3d human action recognition?
Answer: <p>The main difference between 2D and 3D HAR lies in the type of sensor used to collect the data.</p><p><strong>RGB sensors</strong> are simple cameras that are used to capture <strong>2D</strong> images and videos of the subjects for action recognition. These sensors are unable to capture the information regarding the depth of field and lead to loss of potentially valuable information to classify human action. It depends directly on the visual data and isn't suitable in certain background settings, such as dimly light environments, high exposure, high distance from the subject, etc. The inability of these cameras to capture high-quality data makes them unfit for use in many practical scenarios. Compared to 2D data(RGB-based), the presence of higher-dimensional data can inherently capture a larger volume of information, overcoming the limitations of vision-based 2D-HAR and making the model more accurate.</p><p><u>The most commonly used sensors for capturing 3-dimensional data are:</u></p><ul><li><strong>Depth Sensor</strong>- Utilizing a 3-D Depth sequence of videos has gained widespread popularity due to fewer quality issues as it is not dependent on background settings as it is a non-visual approach. It can capture data even in total darkness, sudden changes in lighting conditions, and color gradient. The main disadvantage of depth sensor-based HAR is its sensitivity to minor faults in the sensor, and its nature to highly vary with the presence of noise. Depth data can further be used to extract skeletal information of the subject using various positions of body joints to make the model highly accurate, and able to distinctly classify human action based on joint movement.</li><li><strong>Inertial Sensor- </strong>Historically, the price of Depth sensors has been paramount. To overcome the challenges of 2D HAR at a lower cost, wearable inertial sensors are used. Although more accurate than traditional&nbsp;RGB&nbsp;sensors, and contain more information, they are highly susceptible to fluctuations&nbsp;in&nbsp;the&nbsp;sensor when in&nbsp;use over long periods of time.&nbsp;These are directly attached to the subject's body and therefore are highly use-case specific and cannot be used for security and surveillance.</li></ul>
------------------------------
Question: What is XGBoost in machine learning?
Answer: <p>XGBoost stands for <strong>eXtreme Gradient Boosting </strong>and is an advancement that overcoming the limitations of the gradient boosting <span style="color: rgb(85, 85, 85);">tree algorithm, i.e, its nature to sequentially compute data</span>. It is highly efficient, flexible, and portable making it widely popular in the applied machine learning community. XGBoost is very fast when compared to other implementations of gradient boosting and is highly memory efficient. It is based on the <u>g</u><u style="color: rgb(85, 85, 85);">radient&nbsp;boosting algorithm</u><span style="color: rgb(85, 85, 85);"> which is an ensemble approach where the </span>gradient descent algorithm is used to minimize the loss when adding new models. The<span style="color: rgb(85, 85, 85);"> new models created, attempt to predict the errors of prior models and then are finally added together to make the final prediction. </span>The XGBoost algorithm, although based on gradient boosting, increases the model’s performance by utilizing parallel computations on decision trees to improve speed and efficiency. It works very well on categorical data i.e, data that can be divided into groups and is also highly efficient and can learn from small quantities of labeled data.</p><p><br></p>
------------------------------
Question: What is gradient boosting?
Answer: <p>Boosting is an ensemble learning technique that combines multiple weak learners and converts them into strong ones. It uses multiple weak models, to fuse with each other with the goal of obtaining a much stronger model. </p><p>The <u>g</u><u style="color: rgb(85, 85, 85);">radient&nbsp;boosting algorithm</u><span style="color: rgb(85, 85, 85);">&nbsp;is a sequential ensemble approach where the&nbsp;</span>gradient descent algorithm is used to minimize the loss when adding multiple new models. The <span style="color: rgb(85, 85, 85);">models created, attempt to predict the errors of prior models, and then are finally added together to obtain a strong model and make the final prediction. The </span>performance of the model improves over each iteration as a new model is created every time a weak learner is added to the existing model, which is more precise than its previous iteration.</p><p><span style="color: rgb(85, 85, 85);">.</span>The gradient boosting algorithm learns faster than many traditional machine learning models and works very well on categorical data i.e, data that can be divided into groups. It is very efficient and can learn from small quantities of labeled data.</p><p><u>The gradient boosting algorithm requires these 3 parameters to function, they are:</u></p><p><strong>Loss function</strong>: It is required to reduce errors in the prediction and describe the model performance.</p><p><strong>Weak Learners</strong>: They are required to make predictions, one step at a time. Fusing multiple weak-learners, at each iteration the final model is created.</p><p><strong>Additive model</strong>: In the case of <u>gradient boosting</u>, we aim to reduce the loss function by adding multiple decision trees which are weak on their own however after fusion can become efficient and a strong model.</p>
------------------------------
Question: What is openCV?
Answer: <p>Open Source Computer Vision (OpenCV) as the name suggests in an open-source library developed with the sole aim of accelerating the development of computer vision products, by having an easy-to-use one-stop solution to all classic as well as state-of-the-art computer vision and machine learning algorithms. The library is well-documented and developer-friendly and has more than 2500 optimized algorithms to aid in development along with C++, Python, Java, and MATLAB interfaces. It is flexible and supports multiple operating systems, mainly Windows, Linux, Android, and Mac OS. </p><p>Some of the main modules of the library are:</p><ul><li>Image Processing</li><li>Video analysis</li><li>Object Detection</li><li>Computational Photography</li><li>Camera construction and 3D calibration</li></ul><p>The detailed documentation regarding the functions and its applications can be found <a href="https://docs.opencv.org/master/" rel="noopener noreferrer" target="_blank">here</a>.</p>
------------------------------
Question: What is the difference between multi-label classification and multi-class classification?
Answer: <p><strong>Multiclass classification</strong> is a classification problem where the task is to classify data, between two or more possible classes. Binary classification problems are simple owing to the data point definitely belonging to either of the two possibilities, class 1 or 2. However, with the increase in the number of classes, it becomes increasingly difficult to classify the data. For example, Classifying the types of animals(dog, cat, tiger, etc.) from a dataset containing images of different animals, where each type of animal is a uniquely identifiable class.</p><p><strong>Multilabel classification</strong> is another type of classification problem in machine learning, where the main task is classifying the labels i.e. the tags associated with each instance in data. These labels can be in the range of&nbsp;0 to n, depending on the total number of possible classes. For example, a multi-label classification of textual data can be employed to find the labels associated with each category that a movie belongs to. Depending on a detailed summary of its plot, labels can be associated based on age range(PG, PG-13, R, etc.), on the basis of genre (comedy, thriller, action, etc.), etc.</p><p><br></p><p>In a multiclass classification problem, the classes are <u>mutually exclusive</u> i.e. a data point can belong to any 1 of the multiple classes. Whereas, in a multilabel classification problem, each label represents a different classification task that is somehow related. When each instance has to be placed in one of several (more than two) classes, it is a multi-class classification; whereas when each instance has to be assigned one or more labels, then it is multi-label classification. </p><p><br></p>
------------------------------
Question: What is Box Cox transformation?
Answer: <p>A Box-Cox transformation is a tool commonly used in Multiple Regression Analysis. The main assumptions made by Box-Cox is that:</p><ul><li>The data must be positive (must not contain negative values).</li><li>The data should be continuous.</li></ul><p>Box-Cox is a transformation method, commonly used when the assumption that the data is normally distributed is violated or when the relationship between the dependent and the independent variables (in the case of linear models) is not linear in nature. In such situations, Box-Cox helps the dataset follow a Gaussian distribution by normalizing it.&nbsp;Although it aids in normalizing the data by performing data transformation, there is no guarantee that transformed data follows normal distribution as it does not check for normality after transformation.</p><p>The Box-Cox method checks the standard deviation of the data, to confirm if it is the smallest. Therefore, it is advised to check the transformed data for normality using techniques like probability plot or Q-Q(Quantile-Quantile) plot. Some of its other benefits include reduction in skewness i.e., high symmetry in data distribution, and aids in maintaining a linear relationship between the dependent and independent variables.&nbsp;</p>
------------------------------
Question: What is structured streaming in Spark?
Answer: <p class="ql-align-justify"><strong style="color: rgb(68, 68, 68);">Structured Streaming</strong><span style="color: rgb(68, 68, 68);"> is Apache Spark’s streaming engine that can be utilized for real-time analytics. In Structured Streaming, the main idea is to consider a live data stream as a table, that is appended to continually. It leads to a novel stream processing model. In&nbsp;Structured streaming, there is no concept of a batch (but later versions of Spark, starting with 2.0, have micro-batch processing). In addition, our streaming calculation may be expressed as a normal batch-like query on a static table. Furthermore, Spark executes it as an incremental query on the unbounded input table. Structured Streaming is an effective method for ingesting huge amounts of data from a number of sources.</span></p>
------------------------------
Question: What are the data transformations supported in Spark structured streaming?
Answer: <p class="ql-align-justify">Data transformations supported in Spark structured streaming can be grouped into two types: <strong>Stateless transformations</strong> and <strong>Stateful transformations</strong>.</p><ul><li class="ql-align-justify">The processing of each micro-batch of data in Stateless transformations is independent of the processing of prior batches of data. As a result, this is a stateless transformation, with each batch processing independently .</li><li class="ql-align-justify">In Stateful transformations, the processing of each micro-batch of data is fully or partially dependent on the processing of prior batches of data. As a result, this is a stateful transformation, with each batch taking into account what happened before it and then utilizing that knowledge to compute the data in this batch.</li></ul>
------------------------------
Question: What are the steps in defining a streaming query in Spark?
Answer: <p><strong style="color: rgb(0, 71, 178);">There are 5 steps to define a streaming query:</strong></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">1) </span><strong style="color: rgb(68, 68, 68);">Define Input Sources:</strong><span style="color: rgb(68, 68, 68);"> There are a few built-in sources-</span></p><ul><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">File source</span><strong style="color: rgb(68, 68, 68);">&nbsp;</strong><span style="color: rgb(68, 68, 68);">- Reads files written in a directory as a stream of data.</span></li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Kafka source&nbsp;- Reads data from Kafka.</span></li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Socket source (for testing)&nbsp;- Reads UTF8 text data from a socket connection.</span></li></ul><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">2) </span><strong style="color: rgb(68, 68, 68);">Transform the data:</strong><span style="color: rgb(68, 68, 68);"> Two types of data transformations are there - stateful transformation and stateless transformation.</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">3) </span><strong style="color: rgb(68, 68, 68);">Define Output mode:</strong><span style="color: rgb(68, 68, 68);"> The “Output” is defined as what gets written out to the external storage. There are three types of output modes - Append mode, Complete mode, Update mode.</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">4) </span><strong style="color: rgb(68, 68, 68);">Specify processing details: </strong><span style="color: rgb(68, 68, 68);">The final step before starting the query is to specify details of how to process the data.</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">5) </span><strong style="color: rgb(68, 68, 68);">Start the query:</strong><span style="color: rgb(68, 68, 68);"> Once everything is specified, final step is to start the query using start() method.</span></p>
------------------------------
Question: What is time-series analysis?
Answer: <p>A <em>time series</em> is a quantity measured sequentially in time over a specific interval. Time series analysis, is a statistical approach to analyze the <em>time-series</em> data to understand the trends from the past and predict what is likely to happen in the future with reference to time, based on the trends in existing data. The image below highlights a time-series graph that compares military expenditure and federal debt over a time period. </p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/2049ff082b79c90c7acb7c250db83977.png"></p><p class="ql-align-center"><a href="https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781788290227/1/ch01lvl1sec8/different-types-of-data" rel="noopener noreferrer" target="_blank">Example of time-series data and analysis</a></p><p><u>Some applications of time-series analysis are:</u></p><ul><li><strong>Field of Economics:</strong> Popularly used in Economic Forecasting, census Analysis, etc.</li><li><strong>Field of Finance:</strong>&nbsp;Used largely in Stock Market Analysis, yield management, studying market volatility, etc.</li><li><strong>Social Science:</strong> Birth rates and death rates overtime periods are studied to understand societal effects.</li><li><strong>Healthcare:</strong> For example, researchers are analyzing the trends in people, affected by the coronavirus over a period of time. Moreover, blood pressure and vitals traced over a period of time are often used to evaluate a drug, etc.</li></ul>
------------------------------
Question: What is the difference between univariate, bivariate, and multivariate analysis?
Answer: <p>Analysis techniques in the field of statistics can be classified into three major types, depending on the number of variables present in the data. They are:</p><ul><li><strong>Univariate analysis</strong> is the most basic among the three types. It does not concern with causes or relationships as there is only one input variable, and the main purpose of the analysis is to describe the data and find existing patterns within it. For example, studying the trend in the weight distribution in a class of students.</li><li><strong>Bivariate analysis</strong> is slightly more analytical as it deals with two input variables and the relationship between the two is analyzed to find a correlation. It ensures that the correlations between the two variables are quantified and often involves comparisons, relationships, causes, and explanations. For example, there exists a relationship between the temperature and ice cream sales in the summer season.&nbsp;&nbsp;</li><li><strong>Multivariate analysis</strong> is complex due to multiple relationships among the variables and is used when dealing with 3 or more variables. It is similar to bivariate but can contain more than one dependent variable. For example, crop growth is the dependent variable when analyzing farming data, and the different factors affecting it, i.e. rainfall, temperature, amount of sunlight, amount of fertilizer, etc., are the independent variables.</li></ul>
------------------------------
Question: What is blockchain?
Answer: <p class="ql-align-justify">A blockchain is a series of blocks in which data is kept in set structures known as 'blocks.' It is a <strong>distributed ledger</strong> (record) of all transactions that take place over a <strong>peer-to-peer network</strong> (network containing many computers/nodes connected to each other).  It is a decentralized system since each user has a copy of the ledger. There isn't a central server. Blockchain is immutable because it assures that no one can penetrate the system or change the data stored to the block. It is therefore nearly impossible to hack it.</p>
------------------------------
Question: What are blockchain ledgers?
Answer: <p class="ql-align-justify">The blockchain ledger <span style="color: rgb(0, 102, 204);">permanently records transactions</span> in a public or private peer-to-peer network. A blockchain is a distributed ledger that allows data to be added and modified in real time through the consensus of the network's nodes running the software. However, unlike a database, once data is put to the ledger, it cannot be deleted or changed.</p>
------------------------------
Question: What is meant by proof-of-work in blockchain technology?
Answer: <p class="ql-align-justify">The Proof of Work is a <strong>consensus algorithm</strong> that involves solving a computationally difficult puzzle (mathematical problem) in order to construct new blocks in the Bitcoin blockchain. The method is known as mining. Whenever a miner discovers the correct answer, the node communicates it to the whole network at the same time. Then the miner will get a cryptocurrency reward offered by the PoW protocol. It is utilized to battle against <strong>double spending</strong> (problem empowering users to spend a similar digital money more than once). There are primarily two characteristics that have led to the widespread success of this consensus protocol, and they are as follows:</p><ul><li class="ql-align-justify">It is difficult to come up with a solution to the mathematical puzzle.</li><li class="ql-align-justify">It is simple to check the solution's correctness.</li></ul>
------------------------------
Question: What is a block in a blockchain?
Answer: <p class="ql-align-justify">A block is a single entity in a blockchain which constitutes a chain part. The blocks are placed one after the other, representing the public blockchain. The block is a collection of data and information that includes data, hash, and the previous block's hash. All blocks must be validated by network nodes through a consent procedure, once the block confirmation is obtained it is appended to the end of the chain and it may be added to the next chain. Block 0 (first block of the blockchain) is known as genesis block.</p>
------------------------------
Question: What is proof-of-stake in blockchain technology?
Answer: <p class="ql-align-justify">The Proof of Stake or PoS is a consensus method. According to the Proof of Stake (PoS) principle, a person can create or validate block transactions based on the number of coins they own. It is used to validate transactions on a blockchain. Proof of Stake is not the same as Proof of Work. In Proof of Stake, nodes authenticate transactions by pointing to the network's native tokens to validate the block's contents. A network of this type gives improved security. To reach consensus, participants "stake" their coins on the network. PoS networks are proven to be more efficient, scalable, and energy efficient.</p>
------------------------------
Question: What is meant by a consensus algorithm in blockchain technology?
Answer: <p class="ql-align-justify">Once a transaction is recorded in the blockchain, the same copy should be made available to each and every node. This mechanism is called Consensus algorithm. It is a procedure that allows all the peers in the Blockchain network to agree on the current state of the public ledger. The consensus protocol ensures that a new block stored on the Blockchain seems to be the only version of the truth that all nodes in the Blockchain agree on. While there are several consensus algorithms, the two main ones are of these two types: -</p><ul><li class="ql-align-justify"><strong>Proof of Work:</strong> The main concept behind this consensus algorithm is to solve a complicated mathematical problem and quickly have a solution.</li><li class="ql-align-justify"><strong>Proof of Stake:</strong> In this consensus algorithm, validators invest in the system's coins by staking their own coins.</li></ul>
------------------------------
Question: What are some popular consensus algorithms?
Answer: <p class="ql-align-justify">Some famous consensus algorithms are-</p><ul><li class="ql-align-justify"><strong>PROOF OF WORK (PoW)</strong>: In this type of consensus algorithm, a block is mined by the miners. Miners have to solve a puzzle-like problems and each miner is in a race with each other. The one who solves the puzzle first, gets the right to mine the next block in Blockchain.</li><li class="ql-align-justify"><strong>PROOF OF STAKE (PoS):</strong> <span style="color: rgb(45, 52, 54);">This consensus algorithm does not involve the process of mining. It just involves the process of validating the blocks in a blockchain. Generally, the participants "stake" their coins on the network to achieve consensus.</span></li><li class="ql-align-justify"><strong>DELEGATED PROOF OF STAKE (DPOS):</strong> In this algorithm, stakers will elect their representatives to validate the block on their behalf. As a result, DPoS is quicker than PoS.</li><li class="ql-align-justify"><strong style="color: rgb(45, 52, 54);">PRACTICAL BYZANTINE FAULT TOLERANCE (pBFT)</strong><span style="color: rgb(45, 52, 54);">: </span>It is a consensus system in which all nodes are self-sufficient ( honest or dishonest). All nodes are arranged in a chain, with the "Leader" node at the top. Both nodes consult with each other during the consensus process, and the end aim is to reach an agreement on the current state of the system by a majority vote.</li></ul>
------------------------------
Question: What is a smart contract?
Answer: <p class="ql-align-justify">A smart contract is an agreement between two persons where the terms of the contract are written into lines of computer code. Because smart contracts work on the blockchain, they are stored in a public database and cannot be modified or altered. Transactions are handled via blockchain in smart contracts, which simply means that they may be  executed  automatically even without the need of any third party such as a bank or government. So there is nobody to rely on. Transactions occur once the parameters of the agreement are met, therefore there are no trust difficulties.</p>
------------------------------
Question: What are the key features of blockchains?
Answer: <p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Blockchain's main features are -</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">1) </span><strong style="color: rgb(68, 68, 68);">Immutable</strong><span style="color: rgb(68, 68, 68);"> - Blockchain is immutable because it assures no one can penetrate the system or change the data stored in the block.</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">2) </span><strong style="color: rgb(68, 68, 68);">Decentralized</strong><span style="color: rgb(68, 68, 68);"> - The network is decentralized, which means there is no governing authority or one individual in charge of the framework. The network is instead maintained by a collection of nodes, making it decentralized.</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">3) </span><strong style="color: rgb(68, 68, 68);">Security</strong><span style="color: rgb(68, 68, 68);"> - The use of cryptographic hashes (all information on the blockchain is cryptographically hashed) provides the system's increased security. Blockchain uses SHA-256 (Secure Hash Algorithm), which makes it secure.</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">4) </span><strong style="color: rgb(68, 68, 68);">Distributed ledgers</strong><span style="color: rgb(68, 68, 68);"> - The network ledger is maintained by all other system users. Each computer has the copy of ledger.</span></p><p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">5) </span><strong style="color: rgb(68, 68, 68);">Consensus</strong><span style="color: rgb(68, 68, 68);"> - In the absence of a central authority guaranteeing the integrity of transactions in the network, the consensus algorithm provides this guarantee in a decentralized manner. Nodes may not trust one another, but they may trust the consensus algorithm. Consensus makes sure that there will be no tampering with the blockchain data.</span></p>
------------------------------
Question: What are cryptocurrencies?
Answer: <p class="ql-align-justify">Cryptocurrency is a type of decentralized money or we can say it is digital or virtual currency that is kept, generated, and processed independently of a central bank or government. There is no physical form of cryptocurrency. The term "cryptocurrency" refers to currency that is protected by cryptography (encoding and decoding information). Cryptocurrencies operate utilizing a technology known as blockchain. Bitcoin (BTC), Litecoin (LTC), Ethereum (ETH), BitTorrent (BTT), etc, are examples of cryptocurrencies.</p>
------------------------------
Question: What is a 51% attack?
Answer: <p class="ql-align-justify">When a Bitcoin owner confirms a transaction, it is added to the Bitcoin network's pool of unconfirmed transactions. Miners integrate transactions from the pool to create a block of transactions. They must solve a difficult mathematical challenge in order to add this block of transactions to the blockchain. A malicious miner may attempt to reverse existing transactions. When a miner discovers a solution, it is assumed that it will be broadcasted to all other miners for verification; the block is then added to the blockchain. In contrast, a dishonest miner will not broadcast the solutions to the rest of the network. The corrupt miner will now strive to add blocks to his fake blockchain quicker than the other miners (the truthful one). As soon as the compromised miner builds a longer blockchain, he abruptly transmits it to the rest of the network. The rest of the network will now notice that this (fake) version of the blockchain is longer than the one they were working on, and the protocol will compel them to shift to it. The infected miner may now spend all of his Bitcoins on the true version of the blockchain, which is being worked on by all other miners.&nbsp;The malicious miner will require greater hashing power than that of the rest of the network (51% of the hashing power) to add blocks to his version of the blockchain quicker, allowing him to create a longer chain. This is known as 51% attack.</p>
------------------------------
Question: What is a dApp?
Answer: <p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">A DApp is a decentralized application. It is a distributed network software application that operates on a peer-to-peer network. This signifies that the network is not under the authority of a single person or group. Or we can say that a DApp is an interface that allows users to interact with smart contracts that are hosted on a distributed and peer-to-peer network.</span></p><ul><li class="ql-align-justify"><strong style="color: rgb(68, 68, 68);">Resistant to censorship:- </strong><span style="color: rgb(68, 68, 68);">It is extremely difficult to control the network since there is no single point of failure.</span></li><li class="ql-align-justify"><strong style="color: rgb(68, 68, 68);">There will be no downtime:- </strong><span style="color: rgb(68, 68, 68);">Using a peer-to-peer approach ensures that the DApps continue to function even if computers of the network fail or go down.</span></li><li class="ql-align-justify"><strong style="color: rgb(68, 68, 68);">Open-source:- </strong><span style="color: rgb(68, 68, 68);">This promotes general growth of the DApp ecosystem, allowing developers to create better DApps with more useful or interesting functionality.</span></li></ul>
------------------------------
Question: What is a hyperledger?
Answer: <p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Hyperledger is simply a project. This project was hosted by Linux Foundation in December 2015 for the purpose of helping developers and businesses to communicate on collaborative blockchain initiatives. It is a group of open-source blockchains and tools that anybody may use to build their own distributed ledgers. Hyperledger provides the structure, rules, principles, and tools required to develop open source blockchain. Simply explained, Hyperledger uses blockchain technology for business. Consider Hyperledger to be a hub for numerous different blockchain-based projects including big names such as IBM, Samsung, Microsoft, Hitachi, American Express, JP Morgan.</span></p>
------------------------------
Question: What are the differences between a dApp and a smart contract?
Answer: <p class="ql-align-justify"><span style="color: rgb(68, 68, 68);">When two people or parties agree on the terms of a contract, they can do it legally or through a Blockchain smart contract, thus we call it a digital contract or a paperless contract. Smart contracts are immutable and therefore safe from tampering. Whereas, </span><strong style="color: rgb(68, 68, 68);">Decentralized application</strong><span style="color: rgb(68, 68, 68);">, or DAPP, is a block-chain software that stores and saves data in an untrusted manner. It runs autonomously, and codes may be generated and improved on a regular basis. DAPP functions on a peer-to-peer network of computers. To function, a DApp requires smart contracts. </span><strong style="color: rgb(68, 68, 68);">As a result</strong><span style="color: rgb(68, 68, 68);">, the distinction between Smart Contracts and DApps is that Smart Contracts are systems for digitally verifying, storing, controlling, and facilitating data and financial transactions, whereas DApps are Decentralized Applications linked to the blockchain via Smart Contracts.</span></p>
------------------------------
Question: What are the different types of blockchains?
Answer: <p class="ql-align-justify">There are 4 types of Blockchain:</p><ul><li class="ql-align-justify"><strong>PUBLIC BLOCKCHAIN</strong> - A public blockchain is a distributed ledger system. Hence, anybody may join and use public blockchain as there is no need for permission. It is an open version in which each peer has a copy of the ledger. Anybody can conduct decentralized transactions with an internet connection.</li><li class="ql-align-justify"><strong>PRIVATE BLOCKCHAIN </strong>- A private blockchain operates in a closed network. It is a permissioned blockchain (needs permission) controlled by an enterprise. Hence, ideal for usage within a privately held organization that wishes to use it for internal purposes. They are generally fast because the network requires less time to establish consensus, resulting in speedier transactions.</li><li class="ql-align-justify"><strong>CONSORTIUM BLOCKCHAIN</strong> - A consortium blockchain is also known as Federated blockchains. It is a novel method used in order to meet the demands of companies that want both public and private blockchain functionality. Some features of the organizations are made public in a consortium blockchain, while others are kept private.</li><li class="ql-align-justify"><strong>HYBRID BLOCKCHAIN </strong>- A hybrid blockchain is one that combines a private and public blockchain. It has applications in organizations who do not want to implement either a private or public blockchain and instead want to implement the best of both world.</li></ul><p><br></p>
------------------------------
Question: What are Merkle trees?
Answer: <ul><li class="ql-align-justify">A Merkle tree is a data structure produced by hashing the pairings of nodes, and is therefore used to encode Blockchain data. This procedure is repeated until only one hash is left. i.e. Merkle Root A hash of transactional data is a leaf node. A non-leaf node is a hash of the hashes before it.</li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Merkle tress&nbsp;are binary trees, hence they must have an even number of leaf nodes. In case there are odd number of leaf nodes, then last hash will be copied once to make an even number of leaf nodes.</span></li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Merkle Root is stored in the block header ( part of blockchain Block structure)</span></li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">It is used for safe data verification as well as data synchronization.</span></li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">It employs hash functions to ensure the data integrity.</span></li><li class="ql-align-justify"><span style="color: rgb(68, 68, 68);">Merkle trees are useful in distributed systems where the same data must be present in several places.</span></li></ul>
------------------------------
Question: What is Byzantine Fault Tolerance objective?
Answer: <p class="ql-align-justify">BFT (<strong>Byzantine Fault Tolerance</strong>) is a distributed network feature that allows it to attain consensus (agreement on the same value) even when up to  33% of the nodes in the network fail to reply or respond with inaccurate information. The goal of a BFT mechanism is to protect against system failures by utilizing collective decision-making (both right and faulty nodes) with the goal of reducing the effect of faulty nodes. Byzantine fault tolerance can be obtained if the network's correctly functioning nodes agree on their values. Missing messages can be assigned a default value, i.e., we may suppose that a message from a certain node is ‘faulty' if it is not received within a specified timeframe. Simply put, BFT's purpose is to guard against system failures by minimizing the impact of malicious nodes.</p>
------------------------------
Question: What are the security features of blockchain technology?
Answer: <p class="ql-align-justify">Some of the key security features of blockchain are:</p><p class="ql-align-justify"><strong>Decentralized:</strong> Blockchain is a decentralized network that means no one has the ability to change any form of data within the blockchain. In this network, all nodes or persons have the same privileges. This is the initial layer of blockchain security.</p><p class="ql-align-justify"><strong>Distributed p2p network:</strong> For data exchange, blockchain employs a peer-to-peer network.&nbsp;If a malicious party wishes to modify any form of data from blockchain, it must modify the data of more than half of the nodes in the p2p network in a short period of time.</p><p class="ql-align-justify"><strong>Immutable</strong>: Blockchain is immutable, means no one can modify or alter the blockchain data. So it is next to impossible to hack the blockchain data. This provides enhanced security to Blockchain.</p><p class="ql-align-justify"><strong>Hashing</strong>: One of the most essential security elements of blockchain is cryptographic hash. It can be compared to a block's fingerprint. Each transaction gets signed with a private key, which may then be validated further.<span style="color: rgb(77, 81, 86);"> </span>Blockchain uses Secure hash algorithm (SHA 256).</p><p class="ql-align-justify"><strong>Mining process:</strong> Bitcoin miners safeguard the bitcoin payment network by confirming transaction details by solving computational math puzzles. During the process, nonce is generated (a number used only once). It gives an extra layer of protection to the network. Because calculating the nonce value takes a lot of computational power.</p>
------------------------------
Question: How does blockchain technology prevent double spending?
Answer: <p class="ql-align-justify">When the same digital money is spent several times during a transaction, this is known as double-spending. The thief would transmit a copy of the money transaction to make it appear real, or he or she would erase the transaction entirely. By timestamping groups of transactions and then broadcasting them to all nodes in the bitcoin network, blockchain eliminates double-spending. It avoids double spending by requiring confirmation from  many participants to authenticate a transaction before it is recorded on the ledger. Transactions on the blockchain are irreversible and hard to manipulate since they are time-stamped and mathematically tied to prior ones.</p>
------------------------------
Question: What is the difference between proof-of-work and proof-of-stake?
Answer: <p class="ql-align-justify">Proof of Work algorithm achieves agreement through mining. All of the nodes compete with one another to solve the mathematical objective. The struggle results in the correct solution, and the block is successfully uploaded to the network. The miner who discovers the solution first wins a reward. As a result, we can simply state that PoW assists in the proof of the sequence of events. Proof of Stake, on the other side, is all about verifying blocks rather than mining them. Consensus is reached by the staking of their currencies or tokens to ownership. This approach chooses nodes at random to serve as validators for each new block. Proof-of-work involves the use of an external resource (mining hardware), but proof-of-stake does not require any external source.</p>
------------------------------
Question: How is it decided that someone has solved the mathematical puzzle in proof-of-work?
Answer: <p class="ql-align-justify">The procedure is solved using a difficult mathematical puzzle known as proof of work.&nbsp;&nbsp;All of the miners are competing amongst themselves to mine a specific transaction. The reward is given to the miner who solves the puzzle first. The solved puzzle is visible to all nodes within the network, and its proof of work can be tested. This is referred to as consensus. The problem is solved by changing a nonce that results in a hash value less than a predetermined criteria known as a target. When a transaction is confirmed and validated by other users, a miner confirms it by solving the problem and adding the block to the blockchain. Bitcoin miners implement the SHA-256 hashing technique to compute the hash and define the hash value. If it is less than the specified condition (the target), the puzzle is considered solved. If not, they keep altering the nonce value and repeating the SHA-256 hashing procedure to create the hash value until they receive a hash value that is less than the target.&nbsp;</p>
------------------------------
Question: What is the RSA algorithm?
Answer: <p class="ql-align-justify">The RSA algorithm was created by Ron Rivest, Adi Shamir, and Len Adleman. RSA is a public key cryptography technique that uses two distinct keys to encrypt and decrypt messages. Encryption uses public key, whereas decryption uses private key. As a result, it is also known as an asymmetric key algorithm. It provides a way to ensure integrity, and authenticity. What makes RSA secure? The difficulty of factoring enormous integers that are the product of two enormous prime numbers (p and q). The modulus, n, is calculated by multiplying p and q together (n=p*q). This number is shared by both the public and private keys and acts as a bridge between them. The public key is composed of the modulus(n) and a public exponent(e). The private key is made up of the modulus (n) and the private exponent (d).</p>
------------------------------
Question: What is a nonce in blockchain technology?
Answer: <p class="ql-align-justify">A Nonce is a random whole number. Nonce can only be used only once. It is a 32-bit (4 byte) field that is adapted by miners during the process of mining. When the perfect Nonce is obtained, it is applied to the hashed block. This number, along with the hash value of that block, is rehashed, resulting in a complex algorithm. The miners test and discard millions of Nonce per second before discovering a genuine or valid nonce called "<strong>Golden Nonce</strong>". Golden nonce refers to a hash value that is lower than the target value (threshold value). Miners compete against one another by using hashing power to complete the verification faster than other miners. After they discover the Golden Nonce, they will complete the Block and add it to the Blockchain. And thus resulting in winning the Block reward.</p>
------------------------------
Question: What is an asymmetric encryption algorithm?n
Answer: <p class="ql-align-justify">The asymmetric key algorithm is often referred to as the public key algorithm. They are used to solve two problems that symmetric key algorithms are incapable of solving: key distribution (privacy concerns) and nonrepudiation (authentication issues). These goals are accomplished by public key algorithms that act asymmetrically. A key is composed of two parts: a public key and a private key. The public key is used to encrypt data, while the private key is used to decode it.</p><p class="ql-align-justify"><strong>Most popular asymmetric encryption algorithms are -</strong></p><ul><li class="ql-align-justify"><strong>RSA algorithm</strong> - The RSA algorithm provides asymmetric encryption by factorizing the product of two prime integers.</li><li class="ql-align-justify"><strong>Diffie Hellman Algorithm</strong> - A secure communication algorithm that is commonly used to create a symmetric key by using a private-public key pair.</li><li class="ql-align-justify"><strong>Elliptic-curve cryptography </strong>-<strong> </strong>ECC is an asymmetric encryption technique that employs more complex mathematical concepts based on elliptic curves over a limited field.</li></ul>
------------------------------
Question: What is the difference between private and public key?
Answer: <ul><li>In simple terms, the public key is used to encrypt the plain text for converting it to cipher text, while the recipient uses the private key to decipher the plain text. The private key is symmetrically encrypted, whereas the public key is asymmetrical.</li><li>Private key is also known as secret key, and is only shared with the generator of the key, making it extremely safe.</li><li>Public keys are widely known and used for identification, and private keys are kept hidden and used for authentication and encryption.</li><li>Private keys are more efficient and faster than public keys.</li></ul>
------------------------------
Question: What is the difference between a consortium blockchain and a hybrid blockchain?
Answer: <p class="ql-align-justify">The terms hybrid blockchain and consortium blockchain are nearly synonymous. They change slightly depending on scalability and who uses the network. A hybrid blockchain is one that combines a private and public blockchain (<strong>hybrid = public + private</strong>). As a result, it employs a multi-layer design to fulfil a given use case.&nbsp;For example, it requires creating hashed data blocks on the private network and storing them on the public network.&nbsp;Consortium blockchain, on the other hand, is just a <strong>semi-private</strong> blockchain where a group of people makes choices. Unlike hybrid blockchain, which may be both private and public in certain ways, Consortium blockchain is only open to the group<span style="color: rgb(77, 81, 86);"> of approved individuals </span>or consortium. At the same time, it is a recommended blockchain for use cases requiring sensitive data and processes.</p>
------------------------------
Question: What are the main components of the blockchain ecosystem?
Answer: <p>The blockchain ecosystem's key components are:&nbsp;</p><ul><li class="ql-align-justify"><strong>Node Application</strong> - It is a specific application that must be downloaded and installed on every internet-connected workstation in order to participate in the blockchain ecosystem. Once deployed, they become a member of the blockchain network. It is the foundation of the Blockchain ecosystem.</li><li class="ql-align-justify"><strong>Distributed Ledger</strong> - A distributed ledger is a ledger that is shared by everyone on the network. It&nbsp;is a database that is shared, and synchronized by network peers. The distributed ledger's fundamental characteristic is that it is decentralized (as there is no central authority).</li><li class="ql-align-justify"><strong>Consensus Algorithm </strong>- The consensus algorithm was developed for blockchain technology to provide stability in a network of many nodes. As a consequence, every incoming block in the network is thoroughly validated and secure. Consensus algorithms are classified into various types such as&nbsp;proof of work and proof of stake.</li><li class="ql-align-justify"><strong>Virtual Machine </strong>- A virtual machine is a machine that a computer software creates. It is essentially an imaginary machine contained within a real machine. The virtual machine used in Ethereum blockchain ecosystem is EVM (Ethereum virtual machine).</li></ul>
------------------------------
Question: How is Ethereum different from Bitcoin?
Answer: <p class="ql-align-justify">Both Bitcoin (BTC) as well as Ethereum (ETH) are types of cryptocurrencies. Both are digital money. The main distinction between Ethereum and Bitcoin is that Bitcoin is merely a currency, whereas Ethereum is a ledger technology. Bitcoin has just one smart contract, but Ethereum is a peer-to-peer network that can process any sort of smart contract, which can be simply generated with a few code lines and without the need for your own special-purpose blockchain infrastructure. Bitcoin and Ethereum both use blockchain technology, but Ethereum is significantly more powerful. Ethereum 2.0 enables the development of decentralized applications on top of it. In Ethereum, smart contracts are written in Solidity, which is Turing complete. As a result, the internal code of Ethereum is Turing complete. Bitcoin, on the other hand, is not capable of complicated calculation and is not Turing complete.</p>
------------------------------
Question: Under which circumstances is a new block created in a blockchain?
Answer: <p class="ql-align-justify">In the creation of the block, the mining process plays a major role. Let us consider an example: A wants to send money to B via Blockchain technology. A will digitally sign the transaction using the RSA algorithm. Then Mining process will happen, which means miners will process the transaction and record it. For this purpose, Miners will compete with each other and solve a mathematical puzzle. After guessing the correct solution to the puzzle, the miner will broadcast it to the network, and then a block gets created and added to the network. Hence, every time a transaction happens, the same process will repeat and result in grouping of transactions. The Miners usually generate a new block every 10 minutes that contains all the transactions. To summarize, the transactions that happen over a specified period of time are stored in a file known as a block, which is the foundation of the blockchain network.</p>
------------------------------
Question: What is the Solidity language?
Answer: <p class="ql-align-justify">Solidity is an object-oriented programming language. It is basically designed to power the Ethereum blockchain network. It is primarily intended to enable the creation of "smart contracts" on the network. The Solidity-compiled applications are designed to operate on the Ethereum Virtual Machine (EVM). Solidity was created with the goal of being simple to learn for programmers who are already familiar with one or more current programming languages such as C++, javascript, and python. Solidity is a turing complete language, which means that it may be used to build contracts that solve any reasonable computational problem.</p>
------------------------------
Question: What is the connection between blockchains and cryptocurrencies?
Answer: <p class="ql-align-justify">Blockchain is an unchangeable, secure, distributed, and decentralized technology. Cryptocurrency, on the other hand, is a type of digital or virtual money. The existence of bitcoin is made possible by blockchain. Because Blockchain is a decentralized technology that operates without the need for a government or bank, cryptocurrencies (such as Bitcoin, Ethereum, Litecoin, and BitTorrent) may exist without the need for a central authority. One person may effortlessly transfer money in the form of cryptocurrency to another person via Blockchain. A Blockchain is used by cryptocurrency to keep transactions in a public ledger.</p><p class="ql-align-justify">In a summary, Blockchain may be thought of as the data structure that underlies cryptocurrencies. Cryptocurrencies are built on top of the Blockchain. The cryptocurrencies on the market have a technological backbone thanks to blockchain.</p>
------------------------------
Question: What is the role of Merkle trees in blockchains?
Answer: <p class="ql-align-justify">First of all, the Merkle tree is a hash tree. It uses the concept of hashing and hence used to encode the data of Blockchain in a very secure way. Merkle tree is used in Blockchain for verification purposes. This verification is done sequentially via solving the pairs of hashes until we get one hash value (Merkle Root). So, with the help of Merkle trees we can easily verify whether a transaction is included in a block or not, if added, then in what order? Merkle trees make blockchain data in such a way that very little processing power is required for sharing &amp; verification of data.</p>
------------------------------
Question: Which operations are supported by a Spark RDD?
Answer: <p class="ql-align-justify">Types of operations supported by Spark RDD -</p><ul><li class="ql-align-justify"><strong>Transformation</strong> - RDD Spark Transformations are functions that accept an RDD as input and return one or more RDDs as output. There are two types of transformations: narrow transformations (data from a single partition) and wide transformations (data from several partitions)&nbsp;</li><li class="ql-align-justify"><strong>Action</strong> - The ultimate result of RDD computations is returned by an Action in Spark. It uses a lineage graph to start the execution (dependency graph of all parallel RDDs). The value of an action is stored in one of two places: the drivers or the external storage system.</li></ul>
------------------------------
Question: Is proof-of-stake vulnerable to a 51% attack?
Answer: <p class="ql-align-justify">Yes, Proof of Stake is vulnerable to 51% attack. A 51% attack is when an attacker or a group of attackers obtain control of 51% or more of the processing power or hash rate on a blockchain. Hence, to carry out a 51% attack with a Proof of Stake mechanism, the attacker would need to collect 51% of the cryptocurrency which is not easy. Proof of Stake makes it difficult for a miner with a 51% share in a cryptocurrency to attack the network. So we can say that Proof of Stake is seen as risky for attackers to attack the network.</p>
------------------------------
Question: What is micro-batch stream processing in Spark?
Answer: <p class="ql-align-justify"><strong>Micro-batch Stream processing </strong>- Micro-batch stream processing refers to the approach of gathering data in tiny "batches" for processing stream data. Using this method, the streaming query will perform precisely one micro-batch. After processing all new data in a single batch, it then exits. Small deterministic jobs manage each batch in the Spark cluster. In other words, it is a mechanism for describing streaming computing as a continuous succession of batch processing jobs. The basic idea behind micro-batch processing is to obtain exactly-once semantics, which is not attainable in standard processing systems. Suppose you want to create a streaming app that takes a vast stream of words and computes the top most encountered terms.&nbsp;Micro-batch processing can handle this task.</p>
------------------------------
Question: How do partitions work in Apache Spark?
Answer: <p class="ql-align-justify">The basic idea behind partition is that Resilient Distributed Datasets are so large that they cannot fit into a single node. Hence, RDDs must be partitioned among many nodes. So,&nbsp;Apache Spark automatically partitions these RDDs and distributes them across multiple nodes. Partitioning is the logical partition of data held on a cluster node. Partitioning helps to reduce data shuffling across network nodes. Thereby reducing network latency, and completion time. RDDs with specified partitioning may be constructed in two ways: one by supplying an explicit partitioner by invoking the "partitionBy" method on an RDD, and the other by executing transformations that yield RDDs with specified partitioners.</p>
------------------------------
Question: What is executor memory in a Spark application?
Answer: <p class="ql-align-justify">An executor in Spark is a process that is launched on a worker node for a Spark application. For a spark executor, every spark application has a fixed heap size. The heap size refers to the amount of memory used by the Spark executor. So we can say that an executor memory is nothing more than measure of just how much memory the application will need on the worker node. Spark executor memory is necessary to conduct your spark jobs in accordance with the instructions provided by your driver program.</p>
------------------------------
Question: What is the relationship between Spark and Hadoop?
Answer: <p class="ql-align-justify">Spark is an in-memory distributed processing engine. It is not required to be used in conjunction with Hadoop because it can work in a stand-alone mode. But, because Hadoop is one of the most common large data processing technologies, Spark is designed to work with the Hadoop infrastructure. Hadoop, for example, stores its data in the HDFS (Hadoop Distributed File System), therefore Spark may read data from HDFS and write results to HDFS.</p>
------------------------------
Question: How exactly does a proof-of-work solve the double spending problem?
Answer: <p class="ql-align-justify">The Blockchain network timestamps the transactions by hashing them into a continuous chain of <strong>hash-based proof-of-work</strong>, yielding a record that can't be changed without repeating the proof-of-work. Hashes, which are lengthy sequences of integers that serve as proof of work, are commonly employed to detect tampering such as double spending.&nbsp;The longest chain indicates not only the sequence of events seen, but also proves&nbsp;that it originates from the largest pool of CPU power. A transaction involving double spending is not uploaded to the blockchain since it would be discovered during the block mining process. To be added to the blockchain, a transaction must be included in a block, and miners will conduct Proof of Work on the block. To be included in the block, the outputs from which the transaction spends must not have been spent by any other transaction on the blockchain. This is how Proof of Work helps to prevent double spending.&nbsp;</p>
------------------------------
Question: How can proof-of-work block a 51% attack?
Answer: <p class="ql-align-justify">When a single dishonest miner controls more than 51% of the processing power on a blockchain network, he or she can manipulate the system to insert fraudulent transactions. The approach is known as a 51% attack. Proof Of Work is used by blockchains like Bitcoin and Ethereum. And the risk of a 51% attack on these blockchains is really minimal. In basic terms, PoW prevents this attack since an attacker would need processing power or hashing power larger than that of millions of miners throughout the world, which is extremely difficult. To carry out such an attack, one would have to spend a significant amount of money on mining hardware that could compete with the rest of the network. Even the most powerful computers cannot compete against a pool of millions of other computers, making such an attack extremely difficult. Furthermore, the amount of power necessary to propagate such an attack would make the process unfeasible. And this is the reason that PoW makes it difficult for attacker to launch 51% attack.</p>
------------------------------
Question: Can Apache Spark handle streaming data
Answer: <p class="ql-align-justify">Yes, Apache Spark is capable of processing streaming data. It's because of spark streaming.&nbsp;To begin with, streaming data is data that is continuously created by one or more data sources. Spark Streaming is an extension of the main Spark API that supports real-time data processing from a variety of sources. Apache Spark divides a continuous stream of data into micro-batches known as Dstream or Discretized Streams. Spark Structured streaming is another fundamental concept for treating a live data stream as a table that is constantly being added. As a result, a new stream processing model is created that is quite similar to a batch processing model. Your streaming calculation will be expressed as a conventional batch-like query on a static database, and Spark will perform it as an incremental query on the unbounded input table.</p>
------------------------------
Question: What is the difference between lemmatization and stemming?
Answer: <p>Stemming and lemmatization share the same goal, i.e. to reduce inflection and sometimes derivation-related forms of a word to a common base form. For example, <em>eating, eats, eaten</em> can all be derived from "eat". Having multiple instances of the same word is not ideal, as often they share the same meaning.&nbsp;</p><p><u>Stemming</u> is a technique that extracts the base form of the word by removing all affixes to them. For e.g., eat is the stem of eat-ing. eat-s, eat-en. Indiscriminate removal of affixes to obtain the stem works at times, however, in cases where different words have similar stems, the noisy data can adversely affect the end result.</p><p><br></p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/1b819866466dedac5c9cb47b4f17850e.png"></p><p class="ql-align-center">Stems of two completely different words having the same stem. (<a href="https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/" rel="noopener noreferrer" target="_blank">Telegu Language</a>)</p><p class="ql-align-center"><br></p><p><u>Lemmatization</u> utilizes the morphological information of each word (i.e. it is context-aware) to extract proper lemma's from documents. Although more accurate in cases where stemming fails, it requires detailed dictionaries of the language, for the algorithm to link the derived words back to their origin.</p><p><br></p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/1b819866466dedac5c9cb47b4f17850e.png"></p><p class="ql-align-center"><a href="https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/" rel="noopener noreferrer" target="_blank">Deriving lemma's</a> using the morphological information from the dictionary.</p><p class="ql-align-center"><br></p>
------------------------------
Question: What are recommender systems?
Answer: <p>Recommender systems are one of the most widely used applications of machine learning, in the modern world. From e-commerce giants (Amazon, Alibaba, etc.)&nbsp;to social-networking platforms (Facebook, Instagram, etc.) and even video streaming platforms (Netflix, Youtube, etc.) most small to large-scale companies use recommender systems in their products to attract potential customers. The purpose of a recommender system is to suggest relevant items to users. There exist two major types of recommender systems, they are collaborative and content-based recommender systems.</p><ul><li>The content-based method works on the principle of "similar content". If a user is watching a movie, then the system will check for movies of a similar genre, and similar storyline to recommend to the user.</li><li>Collaborative filtering makes suggestions based on how multiple users rated an item and not just the information on the product itself. It recommends new items, based on the pattern of a user's ratings after comparing it to similar patterns of other users.</li></ul>
------------------------------
Question: What is the difference between Euclidean distance and Manhattan distance?
Answer: <p><em>Euclidean distance</em> is the shortest distance between two points in space. Irrespective of the number of dimensions, the Euclidean formula of distance holds true. It is the "default" distance used in classification and clustering algorithms due to its simplicity.</p><p><u>For example,</u> the flight patterns when traveling from Point A to Point B in an airplane. The shortest route between point A and point B is chosen as there is no traffic or lanes in the sky. The Euclidean Distance gives the best possible result for it.</p><p><br></p><p>On the other hand,<em> Manhattan distance</em> works only if the points are arranged in the form of a grid and the problem gives more priority to the distance between the points along with the grids, and not the geometric distance. In a 2 dimensional plane, it is the sum of the horizontal and vertical distance between two points.</p><p><u>For example,</u> w.r.t a chess dataset, the use of Manhattan distance is more appropriate owing to a grid representation. Similarly, the distance between house A and house B in a block system can be calculated using Manhattan distance.</p>
------------------------------
Question: What are IoT enabling technologies?
Answer: <p class="ql-align-justify">Some of the various IOT enabling technologies are -</p><ul><li class="ql-align-justify"><strong>Cloud Computing: </strong>Cloud computing and its three service models – Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS) are important to the Internet of Things because they allow anybody with a browser and an Internet connection to turn smart object data into actionable knowledge. It provides a virtual infrastructure for monitoring devices, storage, analytics tools, visualization platforms, and client delivery, allowing businesses and consumers to utilize IoT-enabled apps whenever, wherever, and however they want.</li><li class="ql-align-justify"><strong>Big Data Analytics: </strong>Since more intelligent devices are connected to the Internet of Things, more information is collected from them to analyze them. The term "big data" means the huge quantity of information that has to be gathered, preserved, researched, analyzed and handled in order to deliver on the promise of the IoT.</li><li class="ql-align-justify"><strong>Embedded Systems: </strong>The term "embedded system" is software operating as part of hardware (i.e., embedded in hardware). Embedded systems play a significant role in the Internet of Things due to its unique capabilities, such as real-time computing, low power consumption, low maintenance, and high availability.</li></ul>
------------------------------
Question: What are the fundamental components of an IoT system?
Answer: <p><span style="color: rgb(51, 51, 51);">There are 4 fundamental components of IoT-</span></p><ul><li><strong style="color: rgb(51, 51, 51);">Devices</strong><span style="color: rgb(51, 51, 51);">: Devices such as various types of sensors (temperature sensor, humidity sensor, ultrasonic sensor, motion sensor) collect the data from the surrounding environment. </span></li><li><strong style="color: rgb(51, 51, 51);">Connectivity</strong><span style="color: rgb(51, 51, 51);">: Data acquired by sensors must be delivered to a cloud infrastructure. There are various communication and transport media, such as mobile networks, satellite networks, Wi-Fi, Bluetooth, low power wide area networks, etc., to connect sensors to the cloud.</span></li><li><strong style="color: rgb(51, 51, 51);">Data processing</strong><span style="color: rgb(51, 51, 51);">: Once the data has been captured and sent to the cloud, now it needs to be processed. This  data is processed with the help of advanced analytics and other processing systems.</span></li><li class="ql-align-justify"><strong style="color: rgb(51, 51, 51);">User Interface</strong><span style="color: rgb(51, 51, 51);">: The information is now available to the end user in some way after processing.</span></li></ul>
------------------------------
Question: What is the Internet of Things?
Answer: <p class="ql-align-justify"><span style="color: rgb(51, 51, 51);">The Internet of Things (IoT) is the process of linking "things" to the internet, where "things" can be anybody or anything. Information can be accessible from anywhere, at any time, and on any device because of the Internet of Things. The Internet of Things helps to automate jobs by removing the need for human involvement. The Internet of Things is all about making things "smart," including smart homes, smart watches, smart cities, smart speakers like Google Home, and self-driving cars, among other things. Sensors collect the data from the surroundings and then this data is sent to the cloud, then this data get processed and then interacted with end-users. This is how IoT works. </span></p>
------------------------------
Question: What is an IoT cloud?
Answer: <p class="ql-align-justify">Companies can collect large amounts of data from devices and apps with the help of IoT. There are different data gathering tools available that can efficiently gather, process, manage, and store data in real time.&nbsp;&nbsp;All of this is handled by a single system, which is known as an <strong>IoT Cloud</strong>. In addition, an IoT cloud comprises the services and standards required for connecting, managing, and protecting various IoT devices and applications. Simply, we can say that IoT cloud is a network needed for the purpose of storing and processing data collected by sensors.</p>
------------------------------
Question: What are sensors and actuators in an IoT system?
Answer: <p class="ql-align-justify"><strong>SENSOR</strong> - Any physical device that transfers physical energy into electrical energy is referred to as a sensor. In general, sensors are classified into two types: active and passive. Passive sensors, for example, do not require an external power source to operate, but active sensors must. Sensors are classified into mechanical, thermal, electric, and chemical categories based on the sensing mechanism used in them.&nbsp;Different sensors have different properties, such as , temperature sensor measures the temperature, ultrasonic sensors detects distance, and&nbsp;Gas sensor detects gas leakage in the air.</p><p class="ql-align-justify"><strong>ACTUATOR</strong> - Actuators work in the opposite way of sensors, converting electrical signals into physical quantities. It receives input from the system and gives the output to the environment. Motors and heaters are some of regularly used actuators.</p>
------------------------------
Question: What is a WSN?
Answer: <p class="ql-align-justify">WSN simply means <strong>Wireless Sensor Network</strong>. It is made up of a network of distributed sensors nodes&nbsp;that are responsible for covering a geographic region for collecting data. WSN can be used for data processing, analysis, storage, and mining. In WSN, the sensor takes analog&nbsp;data from the physical environment, which is then converted to digital data by an Analog to Digital Converter. The base station (an interface between users and the network) provides instructions to the sensor nodes, and the sensor nodes collaborate to complete the task. After gathering the relevant data, the sensor nodes transmit it to the base station. A base station can also serve as a gateway to other networks through the internet. A base station receives data from sensor nodes and does minimal data processing before sending the updated information to the user through the internet.</p>
------------------------------
Question: What is LoRaWan?
Answer: <p class="ql-align-justify">LoRaWAN is an acronym for Long Range Wide Area Network, which has a range of up to 10 kilometers in rural regions and 2 kilometers in urban areas. The LoRa Alliance, a non-profit organization, created the low-power wireless network protocol for communication on the Internet of Things. This protocol is intended to link battery-powered 'things' to the internet wirelessly.</p>
------------------------------
Question: What are tree learning algorithms in machine learning
Answer: <p>Tree-Based models are built by recursively splitting a training sample, using different features from a dataset at each node. These models use a series of if-then rules to generate predictions from one or more decision trees. Decision trees are the foundation of all tree-based models, and can be used for either regression (predicting numerical values) or classification (predicting categorical values) modeling. The root note is divided into sub-nodes that represent the attributes, and each sub-node is further divided until finally, the leaf node (terminal node) corresponds to a class label. After removing the unnecessary sub-nodes the final output or the leaf node is chosen as the output of the decision tree.</p><p><u>The main advantage of tree-based methods are:</u></p><ul><li>Tree-based methods, for e.g. decision trees are easy to understand due to their intuitive nature.</li><li>They are useful for data exploration and can be used to identify the most significant features and their correlations.</li></ul>
------------------------------
Question: What is the F measure in machine learning
Answer: <p>Precision and Recall of a model are inversely proportional to one another, however, they don't serve as a one-stop score to evaluate model accuracy. The <em>F-score</em> also called the<em> F1-score</em>, is used to evaluate model performance by combining the precision and recall of the model and is defined as the harmonic mean of the model’s precision and recall. It overcomes the limitations of Precision and Recall by taking into account both the False positives(FP) and the False negatives(FN) of the model and numerically quantifies the model performance. A high value of F1 Score indicates a good balance between the Precision and Recall of the</p><p>model, which is a characteristic of a good, generalized model.</p><p><u>The formula to calculate F-score is as follow:</u></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/c82c3ffaaa6e4e203920b05834e91a1c.png">, where <strong>tp = True Positive , fn = false negative &amp; fp = false positive.</strong></p>
------------------------------
Question: What are decision trees?
Answer: <p>Decision trees are a type of supervised learning algorithm, and can be used for either regression (predicting numerical values) or classification (predicting categorical values) modeling. It is a flowchart-like tree structure, making it highly intuitive and easy to understand. Each internal node in the chart denotes a test on the attributes, each branch represents an outcome of the test, and each leaf node(terminal node) holds a class label. The extra sub-nodes are eliminated by pruning the tree and the label associated with the final leaf node is considered as the output of the decision tree. Decision trees can be very useful tools for individuals, investors, and other business professionals that need a visual way to break down a complex decision.</p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/e8796c976f9432d858e35394b95d520f.png"></p><p class="ql-align-center"><a href="https://towardsai.net/p/programming/decision-trees-explained-with-a-practical-example-fe47872d3b53" rel="noopener noreferrer" target="_blank">Example</a> of a decision tree algorithm to classify animals based on attributes.</p>
------------------------------
Question: What is pruning in a decision tree?
Answer: <p>In Decision Trees, <em>pruning</em> is performed to remove the extra branches by removing parts of the tree that do not provide power to classify instances. Pruning enables the decision tree to overcome model overfitting and reach an output for the model. It reduces the complexity of the final classifier and improves the predictive accuracy by reducing model overfitting. A large/deep tree risks overfitting and poor generalization, whereas a small tree risks underfitting and being unable to capture important information about the sample space. Pruning aims to reduce the size of a decision tree without reducing predictive accuracy as measured by cross-validation to obtain the final leaf node. </p>
------------------------------
Question: What are the different IoT protocols?
Answer: <p class="ql-align-justify">Devices can interact with one another using protocols known as IoT protocols. Among the several IoT protocols are:</p><ul><li><strong style="color: rgb(51, 51, 51);">HTTP</strong><span style="color: rgb(51, 51, 51);">: Hyper Text Transfer Protocol has been used to transmit data over the internet. It is the most often used protocol for IoT devices when a big volume of data has to be released.</span></li><li class="ql-align-justify"><strong style="color: rgb(51, 51, 51);">ZigBee</strong><span style="color: rgb(51, 51, 51);">: ZigBee is an Internet of Things (IoT) protocol that allows smart objects to interact with one another. It's frequently used in security and home automation systems. Apps that manage low-rate data transfer over short distances use ZigBee, which is more well-known in industrial situations.</span></li><li class="ql-align-justify"><strong>MQTT</strong>: MQTT means Message Queuing Telemetry Transport. It is a protocol that collects the data from a variety of electrical devices and also enables remote device monitoring. It's a TCP-based publish/subscribe protocol. MQTT is used for event-driven message exchange across wireless networks.</li><li class="ql-align-justify"><strong>CoAP</strong>: CoAP is Constrained Application Protocol. Using this protocol, the client may submit a request to the server, and the server may respond via HTTP to the client. It uses UDP (User Datagram Protocol) for lightweight implementation and saves space.</li><li class="ql-align-justify"><strong>M2M</strong>: M2M stands for machine-to-machine communication protocol. It is an open industry standard designed to offer remote application administration for Internet of Things (IoT) devices. M2M communication protocols are low-cost and rely on public networks. It establishes a setting in which two devices may communicate and share data.</li></ul>
------------------------------
Question: What are the different types of WSN?
Answer: <ul><li class="ql-align-justify"><strong>Terrestrial WSN</strong>: Terrestrial WSNs are made up of hundreds to thousands of wireless sensor nodes that are placed in an unstructured (ad hoc) or organized (preplanned) manner and are capable of efficiently communicating with base stations. In an unstructured mode, the sensor nodes are randomly spread across the target region, which is dropped from a fixed plane. Although the battery power of this WSN is limited, it is equipped with solar cells as a backup power source.</li><li class="ql-align-justify"><strong>Underground WSN</strong>: WSN networks are made up of a large number of sensor nodes placed in the ground that are used to monitor underground conditions. To transport data from the sensor nodes to the base station, additional sink nodes are positioned above ground.</li><li class="ql-align-justify"><strong>Underwater WSN</strong>: Several sensor nodes and vehicles are embedded underwater in these networks. These sensor nodes provide data to autonomous underwater vehicles. Underwater communication is hampered by long propagation delays, as well as bandwidth and sensor problems. WSNs are fitted with a restricted battery that cannot be refilled or replaced underwater.</li><li class="ql-align-justify"><strong>Multimedia WSN</strong>: It has been proposed that multimedia wireless sensor networks be used to track and monitor multimedia events such as pictures, video, and audio. Low-cost sensor nodes with microphones and cameras form the backbone of these networks. For data compression, and retrieval, these nodes interact with one another over a wireless network.</li><li class="ql-align-justify"><strong>Mobile WSN</strong>: Sensor nodes in these networks may move freely and interact with the physical world. The mobile nodes are capable of computation, sensing, and communication.</li></ul>
------------------------------
Question: What is identity management in IoT?
Answer: <p class="ql-align-justify">Providing unique IDs to devices and objects (things) as well as providing authority to communicate, share data, and transact through pre-approved relationships is what Identity management includes. The goal of identity management is to identify people and limit their access to various types of data (like sensitive data, non-sensitive data, or device data). Device identification is supported by identity management, which protects against breaches and malicious activity. Identity management is used in IoT architecture to keep data safe.</p>
------------------------------
Question: What is the HLSA IoT framework?
Answer: <p class="ql-align-justify">HLSA is an abbreviation for "high-level M2M system architecture". The HLSA is made up of three domains: the device and gateway domain, the network domain, and the applications domain.</p><ul><li class="ql-align-justify"><strong>Devices and Gateway domain</strong>: This domain contains M2M devices (devices that execute M2M applications using M2M service capabilities), M2M gateways (gateways that run M2M applications using M2M service capabilities), and an M2M area network that connects M2M devices and M2M gateways.</li><li><strong>Network domain</strong>: The Access network (which allows M2M devices and gateway domains to interface with the core network) and the Core network (a network that provides interconnection with other networks) makes a network area.</li><li class="ql-align-justify"><strong>Application domain</strong>: This domain contains apps that conduct service logic and take use of M2M service capabilities made accessible through an open interface. It also includes features for network administration and M2M management.</li></ul><p><br></p>
------------------------------
Question: What is the difference between IoT and WSN?
Answer: <p class="ql-align-justify">A WSN (Wireless Sensor Network) is a network of wireless sensor nodes that captures the data from diverse locations throughout the world, whereas the Internet of Things is about trying to make things smart through internet connection while eliminating human interaction. In an IoT system, all sensors send data to the internet immediately. For example, a temperature sensor may be used to track the temperature of a body. The data will be sent directly to the internet (IoT cloud), where it will be processed by a server and interpreted on a front-end interface. A WSN, on the other hand, does not have a direct internet connection. The various sensors are instead connected to a router or central node. After that, the data from the router or central node may be routed as needed. To that end, an IoT device can connect to its router and use a wireless sensor network to gather data.</p>
------------------------------
Question: What are the various IoT levels?
Answer: <ul><li class="ql-align-justify"><strong>Level-1</strong>: It consists of a single node/device that conducts sensing and actuation, stores data, analyses it, and hosts the application. Level-1 IoT systems are appropriate for modelling low-cost, low-complexity solutions in which the data involved is little and the analysis needs are not computationally heavy. As an example, consider smart homes.</li><li class="ql-align-justify"><strong>Level-2</strong>: A single node in a level-2 IoT system handles sensing, actuation, and local analysis. Data is often kept in the cloud, and the application is also cloud-based. Level-2 IoT systems are appropriate for solutions involving large amounts of data; nevertheless, the major analytical required is not computationally expensive and may be performed locally. As an example, consider smart irrigation.</li><li class="ql-align-justify"><strong>Level-3</strong>: A single node constitutes a level-3 IoT system. The application is cloud-based, and data is saved and analyzed on the cloud. Level-3 IoT systems are appropriate for applications with large amounts of data and computationally heavy analytic needs.</li><li class="ql-align-justify"><strong>Level-4</strong>: Multiple nodes in a level-4 IoT system do local analysis. The application is cloud-based, and the data is saved on the cloud. Level 4 contains both local and cloud-based observer nodes that can subscribe to and receive data collected in the cloud from IoT devices. Level-4 IoT systems are appropriate for solutions that need several nodes, large amounts of data, and computationally expensive analysis.</li><li class="ql-align-justify"><strong>Level-5</strong>: Multiple end nodes and one coordination node comprise a level-5 IoT system. The end nodes are responsible for sensing and/or actuation. Data is collected from the end nodes and sent to the cloud via the coordinating node. The application is cloud-based, and data is saved and processed on the cloud. Level-5 IoT systems are appropriate for wireless sensor network-based solutions with large amounts of data and computationally intense analytical needs.</li><li class="ql-align-justify"><strong>Level-6</strong>: Multiple independent end nodes in a level-6 IoT system execute sensing and/or actuation and transfer data to the cloud. Data is saved on the cloud, and the application runs on the cloud. The analytics component analyses the data and records the findings in a cloud database. With the cloud-based application, the findings are visualized. The centralized controller monitors the state of all end nodes and issues control directives to them. As an example, consider a weather monitoring system.</li></ul><p class="ql-align-justify"><br></p>
------------------------------
Question: Can there be IoT without the Internet?
Answer: <p class="ql-align-justify">Yes, there can be IoT without internet. It's not like all Internet of Things devices need to be connected to the internet to function properly. They do, however, require a network connection to automate particular processes, interface with it via direct commands, or change its configuration. You may control it from outside the local network by connecting it to the internet. As a result, transmitting data to the cloud requires access to the internet. However, internet is not always essential since it is not always available, such as in airplanes where the Internet is not always available or in farming where 4G is not available. In such cases, IoT ought to work without internet.</p>
------------------------------
Question: What are the main IoT frameworks?
Answer: <p class="ql-align-justify">An IoT framework includes all of the necessary capabilities for cloud support as well as other requirements for IoT technology. Some IoT frameworks are -</p><ul><li class="ql-align-justify"><strong>THINGSPEAK</strong>: Users can use the ThingSpeak framework in MATLAB to not only analyze but also visualize data. The user is not required to purchase a license in order to perform services. ThingSpeak, an open source sensor data collection and storage system, allows for the collection and storage of sensor data even in private channels. This open-source also allows sensor data to be shared in public channels.</li><li class="ql-align-justify"><strong>ARDUINO</strong>: Arduino is an open source IoT framework that is highly recommended, especially if you want to own a computer that can perceive and control the world. The Arduino open source is a combination of IoT hardware and software systems that makes it extremely simple and straightforward to use.</li><li class="ql-align-justify"><strong>SAP IoT</strong>: The SAP IoT is an IoT framework. It provides facilities to build as well as manage an IoT application. This platform makes it easy to monitor all the IoT system's connected devices from a distance. Remote devices can be directly connected to the SAP Platform or via a cloud service.</li><li class="ql-align-justify"><strong>MICROSOFT AZURE IoT</strong>: The Microsoft Azure IoT Suite includes pre-configured solutions and the ability to modify and develop new ones to meet the requirements of projects.&nbsp;It also offers the best security, scalability, and ease of integration with your existing or future systems.</li></ul>
------------------------------
Question: what is pooling in deep learning
Answer: <p>A pooling layer is a new layer added after the convolutional layer, specifically after a nonlinearity (e.g. ReLU) has been applied by the convolutional layer. <span style="color: rgb(79, 93, 115);">The pooling layer in the CNN architecture downscales the image obtained from the previous layers. It can be compared to shrinking an image to reduce its pixel density. Its main </span>function is to progressively reduce the spatial size of the representation to reduce the number of parameters and computation in the network. A 2x2 pooling layer will always reduce the size of each feature map by a factor of 2, i.e. each dimension is halved, reducing the number of pixels or values in each feature map to one quarter the size.&nbsp;For example, a pooling layer applied to a feature map of 8x8 (64 pixels) results in an output pooled feature map of 4×4 (16 pixels).</p>
------------------------------
Question: What is the difference between precision and recall
Answer: <p><em>Precision</em> is defined as the ratio of correctly predicted positive observations to the total number of predicted positive observations. The greater the number of FPs a model returns, the lower its precision score.&nbsp;</p><p>Mathematically, <strong>Precision = TP/TP+FP, Where TP = True Positive &amp; FP = False Positive</strong></p><p>In situations in which we want to minimize the chance of the model returning an FP judgment, we should use a model with high precision.</p><p>In contrast, <em>Recall</em> is defined as the ratio of correctly predicted positive observations to the total number of</p><p>positive observations (whether predicted or not). Mathematically, Recall = TP / (TP+FN)<strong> , Where TP = True Positive &amp; FN = False Negative. </strong>The greater the number of FNs a model returns, the lower its recall score. In situations in which we want to minimize the chance of the model returning an FN judgment, we should use a model with high recall.</p>
------------------------------
Question: What are kernels in SVM?
Answer: <p>Support vector machines (SVM) is a linear model, which uses a hyperplane to classify data. Kernel functions transform the training set of data so that a non-linear decision surface is able to be transformed to a linear equation in a higher dimension space. The function of a kernel is to take data as input and transform it into the desired form. Different SVM algorithms use differing kinds of kernel functions, for instance, linear, nonlinear, polynomial, radial basis function (RBF), and sigmoid. The RBF is most preferred as it's localized and has a finite response along the complete x-axis, however, the choice of the kernel function is use-case specific.</p>
------------------------------
Question: What are feature vectors?
Answer: <p>Feature vectors are n-dimensional vectors containing numerical or symbolic characteristics of an object known as features. Feature vectors are the vectors of explanatory variables used in algorithms such as linear regression. It is often combined with weights using a dot product in order to construct a linear predictor that can be used to determine a score for making a prediction. For instance, in speech recognition, features can be sound lengths, noise level, noise ratios, and more. The feature vectors of different objects can be compared to gain a better understanding of different classes of objects represented by the data.&nbsp;</p><p>An example of a feature vector is RGB (red-green-blue) color descriptions. A color can be described by how much red, blue, and green there is in it. A feature vector for this would be color = [R, G, B].</p>
------------------------------
Question: What is a p-value?
Answer: <p>A p-value is a measure of the probability that an observed difference could have occurred just by random chance. A very small p-value means that such an extreme observed outcome would be very unlikely in hypothesis testing and that there is stronger evidence in favor of the alternate hypothesis. Therefore, the smaller the p-value, the more important (“significant“) are the results. P-values are mostly used by researchers to determine whether a certain pattern they have measured is statistically significant. A p-value less than 0.05 (typically ? 0.05) is statistically significant. Its main limitation is that it can only tell us whether the null hypothesis is supported and cannot tell whether the alternative hypothesis is true or not.</p>
------------------------------
Question: What is latent semantic indexing (LSI)?
Answer: <p>Latent Semantic Indexing (LSI) is an indexing &amp; information retrieval technique that utilizes the singular value decomposition (SVD) to identify patterns between terms and concepts. The LSI technique conducts a semantic analysis, in this way. Firstly, ‘keyword A’ i.e. the user input is identified and the algorithm “understands” that when it finds 'Keyword A', it is also likely to find the related keywords ‘B,’ ‘C,’ and ‘D’ nearby. LSI helps  increasing recall of information retrieval engines by grouping together conceptually related terms thereby overcoming the constraints of other techniques like the 'Boolean keyword search'. It works best in applications where there is little overlap between queries and documents and can be used to perform cross-linguistic concept searching and example-based categorization. However, the multiple use-cases come at the cost of high computational performance and memory requirement, in comparison to other information retrieval techniques.</p>
------------------------------
Question: What is the difference between a skewed and an uniform distribution?
Answer: <p>A skewed distribution occurs when one tail of the graph has more data points in comparison to the other end. Skewness is a coefficient that can be positive, negative, or neutral. A positively skewed graph is skewed towards the left and a negative skew is skewed towards the right tail of the distribution. Using skewed data for training a machine learning model makes the model's output biased towards the skew of the distribution (left or right), which could be  unsuitable for modeling a real-life scenario.</p><p>A uniform distribution occurs when all the observations in the dataset are equally spread across the range of distribution. There exist no clear peaks in this distribution, since each data point appears the same number of times in the set. The presence of uniform instances for each point in data enables the training of a generalized model, which is not biased towards any particular instance in data.&nbsp;</p>
------------------------------
Question: What is computational photography?
Answer: <p>Computational Photography is a field of science that utilizes computer processing capabilities to enhance or extend the capabilities of digital photography which overcomes the limitations of the camera or the lens. It is a subfield of computer vision which, has become pre-dominant especially in the smartphone camera scene where the presence of a more powerful lens is not possible due to the constraint in size and space. In computational photography, a number of pictures are often taken for cross-referencing each shot. After cross-referencing images, the software automates many of the settings a photographer might carefully set to improve overall image quality and allow the production of sharper images with better details thereby overcoming the limitations of traditional lenses.</p>
------------------------------
Question: What is Arduino UNO?
Answer: <p class="ql-align-justify">The Arduino UNO is a microcontroller board made by the Arduino corporation. The term "UNO" means "ONE" in Italian, and it was selected to represent the first release of Arduino Software. It is based on the ATmega328P (datasheet). The Arduino UNO has a total of 28 pins, 14 of which are digital I/O pins and 6 of which are analog pins. 6 PWM (pulse width modulation) pins are marked with the symbol '~' among the digital pins. Arduino boards can read inputs such as a light on a sensor, a finger on a button, or a Twitter tweet and convert them into outputs such as operating a motor, turning on an LED, or posting anything online. As a result, the Arduino UNO is the most often utilized microcontroller board in IoT applications.</p>
------------------------------
Question: What is industrial IoT?
Answer: <p class="ql-align-justify">The term IIoT, as the name suggests, refers to the use of the Internet of Things in industries. The Industrial Internet of Things (IIoT) is a data-monitoring, data-gathering, data-exchange, and data-analysis system comprised of a network of intelligent devices linked together. IIoT is all about the use of smart sensors and actuators to improve manufacturing and industrial processes. IIoT&nbsp;is critical for use cases involving connected ecosystems, such as how cities become smart cities and factories are becoming smart factories. Another example of IIoT is the agricultural industry, where industrial sensors collect data regarding soil nutrients, moisture, and other factors, allowing farmers to grow the best crop possible. IIoT devices are used in the automotive sector throughout the production process.</p>
------------------------------
Question: What is the role of IoT in building smart cities?
Answer: <p class="ql-align-justify">Smart cities install advanced linked roadways, smart buildings, smart parking, smart lighting, and other transportation technologies. IoT helps in making cities "smart" by using different types of sensors. These sensors helps to collect and analyze data. Object Tracking Sensors are key technologies in the Smart Parking solution, which is redefining how vehicles in malls and city centers identify accessible parking spaces. Various IoT sensors, such as proximity sensors, smoke detectors, water level sensors, and so on, make a building smart. A smart city leverages data from people, cars, buildings, and other sources to not only enhance citizens' lives but also to reduce the city's environmental effect, continuously enhancing efficiency.</p>
------------------------------
Question: What is the difference between IoT and SCADA?
Answer: <p class="ql-align-justify">SCADA means Supervisory Control and Data Acquisition. Devices/Sensors in the data acquisition section acquire data from the entire manufacturing system and gives it to the supervisory control system. Then next step is to examine the data and see whether it requires further management or not. If it requires further management, then data is simply input into the system as a time-related data point, giving a baseline for further choices and management. This is what SCADA is. Whereas, the internet of things (IoT) is all about making things smarter by connecting them to the internet. </p><p class="ql-align-justify">To distinguish between SCADA and IoT, we may simply state that the, the Internet of Things (IoT) is the data collection, and control mechanism for the SCADA system in some cases. The IoT devices do not have to make a decision on what to do with the information it gathers. It doesn't bother to look into it most of the time. Any SCADA system, according to logic, is made up of IoT devices, however not all IoT devices are part of a SCADA system. </p>
------------------------------
Question: What is OpenHAB IoT framework?
Answer: <p class="ql-align-justify">OpenHAB is an acronym that stands for "<strong>Open Home Automation Bus</strong>." It is a modular open-source home automation <strong>software</strong> that is free to use and can be installed on any computer running any operating system. Cloud servers aren't necessary. Java is the programming language that was utilized to create it. OpenHAB platform uses a lightweight framework. The most strong point about openHAB is that if a user prefers to keep his or her information private, then user can use openHAB software offline. If not required, remote access can also be disabled . </p>
------------------------------
Question: What role can IoT play in smart transportation?
Answer: <p class="ql-align-justify">IoT has several applications in transportation. Smart transportation system includes - smart vehicles, smart tolls, smart traffic management, smart parking, accident prevention system. Vehicles may be tracked using IoT for their speed, position, whether they are in danger, and so on. In most cases, trucks are employed for transportation or to move huge goods. Sensors monitor the inside conditions of the vehicle, such as humidity, temperature, and lighting. A transportation company can establish the smart payment service at tolls or parking tickets using IoT. Vehicle guiding and navigation control systems are also possible with IoT. The usage of IoT allows the monitoring and control of transportation. A car may be detected up to a kilometer away from a tolling station, correctly identified, and the barrier lowered to allow the car to pass through using IoT technology such as RFID.&nbsp;IoT technology enables transport companies to monitor progress in real-time and adjust for unexpected situations such as accidents, road construction, emergencies, and so on, allowing them to re-route and make travels highly efficient.</p>
------------------------------
Question: What is the role of IoT in oil and gas industry?
Answer: <p class="ql-align-justify">The key benefit of Internet of Things in Oil and Gas industry is that administrators can monitor and run the whole plant using real-time data from IoT endpoints. Data may be consumed from a variety of sources due to IoT devices placed in various areas. As a consequence, oil and gas production from refineries to smart stations, are improved. Pipeline data like as temperature, flow, and pressure are monitored by IoT sensors, which assists in real-time control. IoT smart valves are also put at different locations along the pipeline to allow for remote control of the flow. IoT in Oil and Gas industry allows you to monitor data from anywhere using IoT devices.&nbsp;Distributors in oil and gas companies can obtain accurate information about container levels and utilize this information to enhance container refilling to avoid shortages. In Oil and gas industry, IoT sensors that monitor the inventory levels of onshore oil tanks inform transporters when the tanks need to be emptied. Furthermore, companies can use IoT to monitor machinery and determine when they need to be shut down automatically when required.</p>
------------------------------
Question: What are eigenvalues and eigenvectors?
Answer: <p>Dimensionality Reduction is one of the major challenges of data science and Principal Component Analysis (PCA) is one of the most popular techniques. <strong>Eigenvalues &amp; Eigenvectors</strong> are used in PCA to reduce the dimensionality of the data, resulting in a simpler model which is computationally efficient and provides greater generalization accuracy.&nbsp;<em>Eigenvectors</em> when multiplied with a matrix (linear combination or transformation) results in a new vector having the same direction but scaled (scaler multiple) in forward or reverse direction. This scaling is done by a magnitude of the scaler multiple which is known as Eigenvalue. Therefore, the eigenvalue is the scaling factor for eigenvectors and the magnitude of scaling is dependent on the eigenvalue. These help in transforming the data to a set of most important dimensions (principal components) resulting in faster processing of the data.</p>
------------------------------
Question: What are some IoT applications in health care?
Answer: <p class="ql-align-justify">Some applications of internet of Things in healthcare are-</p><ul><li class="ql-align-justify"><strong>Medical Alert Devices</strong>: Wearable devices whose function is to inform relatives or friends in the event of an emergency. For example, if a person wearing a medical alert bracelet falls out of bed in the middle of the night, the individuals they choose to assist in an emergency would be notified promptly on their cellphones that their assistance was required.</li><li class="ql-align-justify"><strong>Sensors that operate wirelessly</strong>: Wireless sensors are being utilized in labs and hospital freezers to guarantee that blood samples, cold medicines, and other biological items are kept at the right temperatures at all times.</li><li class="ql-align-justify"><strong>Anti-Depression Wearables</strong>: Apple has created an app for the Apple Watch that assists manic depressive sufferers in dealing with their mental health and depression. The app monitors cognitive and emotional functions and keeps track of a patient's episodes.</li></ul>
------------------------------
Question: Can IoT make buildings energy efficient?
Answer: <p class="ql-align-justify">Smart sensors can be used to monitor energy utilization, ensuring that energy is not wasted while some areas of building are unused. Buildings may use IoT platforms to manage light sensors and identify the best time of day to consume electricity. In order to make buildings more energy efficient, smart lighting solutions play a vital role. Buildings may also use smart lighting to change the brightness level of LED lights in certain areas at various times, based on use. When environmental factors deviate from their ideal range, smart alarms can notify you. This way, any wasted energy concerns may be resolved promptly and efficiently.</p>
------------------------------
Question: What is statistical inference?
Answer: <p>Statistical inference is the process through which assertions are made about a population (usually with a certain margin of error) based on observations of a sample and certain techniques of statistical analysis. Typically, it involves forming a hypothesis about the population and a null hypothesis and a statistical threshold (a p-value) for rejecting the null hypothesis and accepting the alternate hypothesis.</p>
------------------------------
Question: what is the difference between a type 1 and type 2 error?
Answer: <p>A type 1 error is when you <strong>reject</strong> the null hypothesis when it is in fact true.</p><p>A type 2 error is when you <strong>reject</strong> the alternate hypothesis when it is fact true. In other words, keeping the null hypothesis when it should in fact be rejected.</p>
------------------------------
Question: What is the power of a test to determine whether the null hypothesis is true.
Answer: <p>Power of a test is the probability of rejecting the null hypothesis when in fact it is false. Or, equivalently, it is the probability of not committing a Type 2 error. </p>
------------------------------
Question: What is meant by the effect size in statistics?
Answer: <p>An effect size is defined as the magnitude of the difference between the point estimate and the null value for a population statistic.</p>
------------------------------
Question: What is the difference between practical significance and statistical significance?
Answer: <p>The practical significance of a test is determined by the effect size (i.e., the difference between the point estimate and the null value), whereas statistical significance is determined by the <em>z-value</em> and the standard error (SE). However, the <em>z-value</em> depends on the sample size; the larger the sample-size, the larger the <em>z-value</em> and, thus, the smaller the<em> p-value</em>. The smaller the p-value, the easier it is to reject the null-hypothesis. Thus, an experimenter can manipulate the p-value, thereby making it easier to reject the null-hypothesis even in cases where the difference between the point estimate and the null value is deemed to be of no practical significance.</p>
------------------------------
Question: What purpose does a large sample serve?
Answer: <p>The purpose served by a large sample can be explained thus. As long as the observations are independent and the distribution is not highly skewed, a large sample ensures that the sampling distribution of the mean is nearly normal, and the estimate of the standard error is reliable.</p>
------------------------------
Question: What is the difference between a t-distribution and a normal distribution?
Answer: <p>A t-distribution like a normal distribution is bell-shaped (uni-modal) but has thicker tails (more observations fall under the tails than in a normal distribution). This means that more observations are likely to fall beyond the 2 standard deviation region. This is helpful for mitigating the effect of less reliable estimates of the standard error of the sampling distribution, as in when the sample size is small. A &lt;t-distribution like the normal distribution is always centered at 0 and has only one parameter, the degrees of freedom, unlike the normal distribution which has two parameters (the mean and the standard deviation).</p>
------------------------------
Question: What is supervised learning?
Answer: <p>Supervised machine learning consists in developing a machine learning model (a function) based on a training set. Each instance of the training set is a data  instance, such as an image or a piece of text (usually, represented by a vector, such as pixels or word embeddings) and a target value such as a label or a numerical value.</p>
------------------------------
Question: What is regression?
Answer: <p><span style="color: rgb(49, 59, 63);">Regression is a supervised machine learning method which predicts a numerical value (output value) for a vector of input values, unlike the classification systems which predict a categorical label. The point of a regression model is to infer a function (generally, polynomial) which maps the values of the set of input (or independent) variables to a value for the output (or dependent) variable. This function is inferred on the basis of a training data, where each instance of the training data consists of a vector of values for the independent variables (also called the features of the model) and a scalar value for dependent variable. A regression model can be interpreted as an equation of the form y = MX + c, where y is a scalar and X is a vector of (feature) values and M is a vector of coefficients and c is some constant. These equations are usually polynomial and can be univariate (X consists of a single variable) or multivariate (X consists of more than one variable)..</span></p>
------------------------------
Question: What are some evaluation metrics for regression?
Answer: <p>Some of the most common evaluation metrics for regressions are:</p><ul><li>Mean Squared Error (MSE) over a set of input instances is just the average of the square of the errors for this set, where the error for an input is the difference between the actual output for the input and the predicted value for the input.</li><li>Root Mean Squared Error (RMSE), which is just the square root of MSE.</li></ul><p><br></p><ul><li>R-Squared&nbsp; is defined as a statistical measure of the model fitness in terms of how much variation in the dependent variable (Y) is explained by the independent variable(s) in a regression model.&nbsp;</li></ul><p>A detailed examination of the most common evaluation metrics for regression can be found in our   <a href="https://knowhowangels.com/blog/regression-evaluation-metrics/" rel="noopener noreferrer" target="_blank">blog post</a>.</p>
------------------------------
Question: what is the vanishing gradient problem?
Answer: <p>The Vanishing Gradient Problem is encountered while training Neural Networks with gradient-based methods (e.g. Back Propagation). It occurs when the backpropagation algorithm moves back through all of the neurons of the neural net to update their weights and is caused by the multiplicative nature of the algorithm. Due to the multiplicative nature, gradients calculated at a deep stage can have too small of an impact due to a small value of the actual factor being multiplied by the RNN in the backpropagation algorithm. As more layers are added to the neural networks, the gradients of the loss function approach zero, making the network harder to train and unable to learn from datasets. One of the many ways to overcome the Vanishing Gradient Problem is by using batch normalization, which reduces this problem by simply normalizing the input.</p>
------------------------------
Question: What is the difference between parametric and non-parametric methods?
Answer: <p>Parametric and nonparametric are two broad classifications of statistical procedures. The basic idea behind the parametric methods is that there exists a set of fixed parameters that determine a probability model. These are the methods for which we know that the population is approximately normal, or we can approximate using a normal distribution. In contrast, non-parametric methods are statistical techniques for which we do not have to make any assumption of parameters for the population we are studying.&nbsp;</p><p>A parametric method involves the calculation of a margin of error using formulae, and the estimation of the population mean, whereas nonparametric methods calculate confidence mean, generally involving the use of bootstrapping.</p>
------------------------------
Question: What are the main differences between ELMo and BERT?
Answer: <p>At its core, BERT uses transformers whereas ELMo uses LSTMs. Besides the fact that these two approaches work differently, using transformers enables the parallelization of training which is an important factor when working with large amounts of data, to reduce training time. The ELMo (Embeddings from Language Models) design uses a deep bidirectional LSTM language model for learning words and their context. BERT (Bidirectional Encoder Representations from Transformers) builds on top of the bidirectional idea from ELMo, using the transformer architecture to compute word embeddings to take proper advantage of both left and right contexts simultaneously. Moreover, BERT is also designed to be fine-tuned easily and can be used in a classifier without having to do much network building or customization, i.e., it is developer-friendly.&nbsp;</p>
------------------------------
Question: What are the main advantages of Transformers over LSTM?
Answer: <p>Transformers are generally more efficient, but they usually require to be deeper or bigger than the corresponding LSTM models. A 1-layer LSTM can go very far, however that's not the case for transformers. LSTMs are a&nbsp;good choice if we are decoding a lot during training, for instance; transformers tend to be really slow at inference. The main advantage of the transformers is their ability to handle long-range interactions, which is something that LSTMs struggle with due to the problem of vanishing gradients. In practice, Transformers are faster to train than LSTMs and are very easy to parallelize and improve training time.&nbsp;</p>
------------------------------
Question: What are transformers in natural language processing?
Answer: <p>A transformer is a deep learning model that adopts the mechanism of attention to process sequential data (primarily in the field of natural language processing) without using recurrence or convolutional networks. Transformers have enabled researchers to capture how a particular word relates to other words, including long-term dependencies among words. It has now become possible to describe words with varied dimensions representing the closeness of the words to the meanings and use of other words.</p><p>1) Transformers can understand the relationship between sequential elements that are far from each other.</p><p>2) Since the transformer model facilitates parallelization during training, it has enabled training on larger datasets than were once impossible.</p><p>3) It pays equal attention to all the elements in the sequence and improves the accuracy of the system.</p><p><br></p>
------------------------------
Question: What are conditional random fields
Answer: <p>Conditional Random Field(CRF) is a probabilistic graphical model used mostly for sequence labeling tasks. In CRFs, our input data is sequential, and we take the previous context into account when making predictions on a data point. CRFs are discriminative, and model P(y|x). They do not require explicitly modeling P(x) and depending on the context, might yield higher performance because they need fewer parameters to be learned. In general, CRFs are more powerful than HMMs due to their application of feature functions.&nbsp;&nbsp;</p><p>Some examples where CRFs are used:&nbsp;Labeling or parsing of sequential data for NLP or biological sequences, POS tagging, shallow parsing, named entity recognition, etc.</p>
------------------------------
Question: What is the difference between a population and a sample?
Answer: <p>A population includes all members from a specified group, all possible outcomes or measurements that are of interest. The exact population will depend on the scope of the study. In contrast, a sample is a specific group that we collect the data from i.e. it's a part/subset of the population. It is the group of elements who actually participated in the study. The size of the sample is always less than the total size of the population.</p>
------------------------------
Question: What is the dying ReLu problem?
Answer: <p>The "Dying ReLU" refers to a neuron that outputs 0 for the data in the training set. This happens because the sum of (weight * inputs) in a neuron (also called activation) becomes &lt;= 0 for all input patterns. This causes ReLU to output 0. As the derivative of ReLU is 0 in this case, no weight updates are made and the neuron is stuck at output = 0. Implementing the Leaky ReLU solves the problem by providing a very small gradient for negative values allowing the neuron to recover. The Leaky ReLU is based on a ReLU, but it has a small slope for negative values instead of a flat slope (setting all negative values to 0). The slope coefficient is determined before training, i.e. it is not learned during training.&nbsp;</p>
------------------------------
Question: What is Central Limit Theorem in statistics?
Answer: <p>The Central Limit Theorem(CLT) states that the sampling distribution of the sample means approaches a normal distribution as the sample size gets larger, no matter the shape of the population distribution. This fact holds especially true for sample sizes over 30. The theorem holds regardless of the shape of the distribution from which the samples are taken.  One of the most common applications of CLT is in election polls, where the percentage of persons supporting a candidate which are reported with confidence intervals.</p>
------------------------------
Question: What are residual networks in computer vision?
Answer: <p>According to Wikipedia, "A residual neural network (ResNet) is an artificial neural network (ANN) of a kind that builds on constructs known from pyramidal cells in the cerebral cortex".  As we design increasingly deeper networks it becomes more important to understand how adding layers can increase the complexity and expressiveness of the network. ResNet tries to learn the difference in learned features and if the learned features deemed to be not useful in the final decision then the weight will become zero hence it will not overfit. Taking advantage of its powerful representational ability and deep networks, the performance of many computer vision applications has been boosted significantly using ResNets such as object detection and face recognition. </p>
------------------------------
Question: What are confidence intervals in statistics?
Answer: <p>The confidence interval is the range of values that an estimated value is expected to fall within for a certain percentage of the time. It is not a probability measure; rather it provides a range of values that is likely to contain the population parameter with confidence. Interval estimates are often desirable because the estimate of the population parameter, say, mean varies from sample to sample. Instead of a single estimate i.e. a point estimate for the mean, a confidence interval generates a lower and upper limit for the mean. The interval estimate gives an indication of how much uncertainty there is in our estimate of the true mean. The narrower the interval, the more precise is our estimate. The more accurate the sampling strategy, or the more realistic an experiment, the greater the chance that the confidence interval includes the true value of our estimate.</p>
------------------------------
Question: what is the difference between machine vision and computer vision
Answer: <p>Computer vision(CV) is a broad area of study, referring to the automation of capture and processing of images, with an emphasis on image analysis. The main goal of CV is not only to see but also to process and provide useful results based on the observation. Traditionally, Machine Vision(MV) has focused more on the use of cameras and video in industrial settings where light and motion are controlled and where the objects to be viewed are already known and almost all observed events are predictable, making it a subcategory of computer vision. Computer vision includes MV but also deals with aspects of 2D and 3D images and vision that are uncontrolled, often unpredictable, (unlike MV) and where objects, their activities, and the surrounding world extends much further into the unknown.</p>
------------------------------
Question: What is regression towards the mean?
Answer: <p>The phenomenon called regression towards the mean consists in when a group of participants score far from the mean on the first measurement of scores, but on a second or third measurement the group on an average scores closer to the mean.</p>
------------------------------
Question: What is a two-group experimental design?
Answer: <p>In a two-group experimental design the experimental subjects are randomly put into the experimental group and the control group. The experimental group is exposed to the hypothesized cause condition or treatment whereas the control group is not. At the conclusion of the treatment both groups are tested for the effect condition.</p>
------------------------------
Question: what is two group pre-test post-test experimental design?
Answer: <p>In a two-group pre-test post-test experimental design the experimental subjects are randomly divided into two groups: experimental and control. As in simple two-group design, the experimental group is subjected to the hypothesized cause condition or treatment and the control group is not. But before they are subjected to this treatment a pre-test is administered to both groups to measure the degree of effect condition. Then after the experimental group is exposed to the hypothesized cause treatment, both groups are administered the same test (now it is called the post-test). This would provide the experimenter a measure how much difference was created by the administration of the treatment</p>
------------------------------
Question: What are quasi-experimental designs?
Answer: <p>Quasi-experimental designs attempt to discover causal relationships between the independent variable(s) and the dependent variable, as in the case of experimental designs. But unlike experimental design, in quasi-experimental design the independent variable cannot be manipulated or compared or random assignments are not possible for a variety of reasons.</p>
------------------------------
Question: How do you estimate the population mean using the t-distribution?
Answer: <p>The population mean is estimated using the t-distribution in more or less the same way it is estimated using the normal distribution except that you would use t-star for the critical value instead of z-star. Thus, it would be estimated as:</p><p>x-bar plus or minus t*_df times SE.</p><p> Here x-bar is the point estimate (i.e., sample mean), SE is the standard error which is sample standard deviation divided by the square root of the sample size, and df in t*_df is the degrees of freedom, which is typically sample size minus 1.</p>
------------------------------
Question: What is a causal-predictive study?
Answer: <p>A causal-predictive study attempts to predict an effect on one variable by manipulating another variable while holding all other variables constant. For example, researchers using a causal-predictive study might be interested in whether installation of video surveillance cameras on the receiving dock and in stockrooms would reduce employee theft in mall stores.</p>
------------------------------
Question: What is singular value decomposition (SVD)?
Answer: <p>The singular value decomposition(SVD) of a matrix A is the factorization of A into the product of three matrices&nbsp;</p><p>A = (UDV)^T where the columns of U and V are orthonormal and the matrix D is diagonal with positive real entries. It generalizes the eigendecomposition of a square normal matrix to any m × n matrix via an extension of the polar decomposition. Eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. SVD with a reduced number of singular values can closely approximate the original matrix and is widely used for data compression by storing the truncated forms&nbsp;and also for variable reduction.</p>
------------------------------
Question: What is a blind experiment?
Answer: <p>An experiment or study is said to be blind when the experiemental subjects do not know if they are receiving the treatment (as opposed to a placebo).</p>
------------------------------
Question: What is a double blind experiment?
Answer: <p>An experiment or study is said to be double blind when the experimenters do not know if they are giving the treatment to the experimental group or to the control group and the experimental subjects do not know which group they belong to.</p>
------------------------------
Question: What is Pearson correlation?
Answer: <p>Pearson's Correlation coefficient is a value ranging from -1 to 1 that acts as a measure of the direction and strength of the linear relationship between two quantitative variables. A value of 1 signals a strong positive linear correlation, and a value of -1 signals a strong negative linear correlation. A value of 0 signals no strong correlation between the two quantitative variables.</p><p>The Pearson coefficient correlation looks at the relationship between two variables and seeks to draw a line through the data of two variables to show their relationship. The relationship of the variables is measured &amp; the Pearson’s correlation coefficient provides a way to evaluate how well two sets of data are related to each other, X vs Y on a graph.</p>
------------------------------
Question: What is the difference between a blind study and a double blind study?
Answer: <p>In a blind study experimental subjects do not know whether they belong to the experimental group or the control group whereas in a double blind study even the experimenters do not know which experimental subjects belong to which group.</p>
------------------------------
Question: What is the chi-squared test?
Answer: <p>The chi-square test usually refers to Pearson’s chi-square and is<span style="color: rgb(40, 40, 41);"> mostly</span><strong style="color: rgb(40, 40, 41);"> </strong>used to find the “goodness of fit” or to “test independence” of categorical variables.&nbsp;This test is used, when there are two categorical variables and we want to determine whether there is a significant association between the two variables. The main advantage of the chi-squared is that it is non-parametric, it makes no assumptions about the distribution being tested, particularly that it approaches normality. However, it only tests whether two individual variables are independent in a binary, “yes” or “no” format and does not provide any insight into the degree of difference between the categories</p><p><br></p>
------------------------------
Question: What is a two-sided hypothesis test?
Answer: <p>A two-tailed test in statistics looks at both the highest and the lowest standard deviation of a statistical curve. It is used in null-hypothesis testing and testing for statistical significance and shows a more balanced picture of the data, unlike the one-tailed test. The test examines both sides of a specified range of data as designated by the probability distribution involved and tests whether a sample is greater than or less than the range of values.</p><p>A two-tailed test signifies the hypothesis test in which the region of rejection appears on both sides of the sampling distributions. If the population parameter is characterized by the words equal to or not equal to, then we commonly use a two-tailed test. For example, suppose the return on all stocks on NYSE is equal to 3%, then it is a two-tailed test.</p>
------------------------------
Question: What is ANOVA
Answer: <p>ANOVA is used to test for differences among the means of the population by examining the amount of variation within each of these samples, relative to the amount of variation between the sample. It stands for analysis of variance and compares 3 or more means based on 1 or more factors. It does this by comparing the variance within groups to the variance between groups. If the latter is much larger than the former, then it concludes that there is a difference in means between groups.&nbsp;</p><p><u>The Formula for ANOVA test:</u></p><p><strong>F= MSE/MST</strong></p><p>where:</p><p>F= ANOVA coefficient</p><p>MST= Mean sum of squares due to treatment</p><p>MSE= Mean sum of squares due to error</p><p><br></p>
------------------------------
Question: What is the concept of entropy in the context of a decision tree algorithm?
Answer: <p>A decision tree is built top-down from a root node and involves partitioning the data into subsets that contain instances with similar values (homogenous).  Node splitting is used to divide a node into multiple sub-nodes to create relatively homogeneous nodes. Information Gain is the metric, used to measure the reduction of entropy i.e the disorder, and is crucial for node splitting. It is used for splitting the nodes when the target variable is categorical and works on the concept of entropy. The main function of entropy, w.r.t decision trees is for numerically representing how homogeneous the node is. The lower the value of entropy, the higher is the homogeneity of the node. If the sample is completely homogeneous the entropy is zero and if the sample is equally divided into multiple segments, it has an entropy of one.</p>
------------------------------
Question: How are encoders different from decoders in machine learning?
Answer: <p>An encoder is a neural network that takes an input, and outputs a feature map/vector/tensor or an embedding vector. These feature vectors hold the information, i.e. the features, that represents the input. The decoder is again a network (usually the same network structure as encoder but in opposite orientation) that takes the feature vector from the encoder as an input and gives the best/closest match to the actual input or intended output.</p><p>For example: Whenever a user types a text in English and wants to convert it to let's say, French, the input will undergo folds ( encoding ) to retain only important information which in turn is passed to a DECODER. The DECODER helps translate from English to French with the help of input from ENCODER.</p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/006fb747015a7924a526d291469a15a9.png"></p><p class="ql-align-center"><strong>Encoder-Decoder model in practice</strong></p>
------------------------------
Question: What is the difference between syntactic and semantic analysis of a sentence?
Answer: <p>At its core, Semantics cares about whether or not a sentence has a valid meaning. Syntax refers to the structure of a language and tracing its roots to study how everything is put together.</p><p>Semantic analysis is the task of ensuring that the declarations and statements of a program are semantically correct, i.e, their meaning is clear and consistent with the way in which control structures and data types are supposed to be used. In contrast, syntactic analysis is the process of analyzing a string of symbols either in natural language, computer languages, or data structures to check if it's following the rules of formal grammar. While syntactic analysis takes the tokens as input and generates a parse tree as output in the second phase of the compilation process, semantic analysis checks whether the parse tree generated abides by the rules of the language, in the third phase of the compilation process.</p>
------------------------------
Question: What is the sigmoid activation function?
Answer: <p>The sigmoid function is an activation function in terms of underlying gate structured in co-relation to neuron firing, in Artificial Neural Networks(ANNs). The sigmoid activation function transforms linear inputs to nonlinear outputs and is mostly used in classification problems. Its outputs are bound between <em>0 and 1</em> so, it is generally interpreted similarly to probability. Most learning algorithms, unlike the sigmoid function, involve higher-order differentiation and require more computational resources, adding to model complexity and increasing training time. Therefore, choosing an activation function like sigmoid, that is computationally inexpensive to handle is helpful in specific scenarios to minimize training time.</p>
------------------------------
Question: When should the sigmoid activation function be used instead of ReLu?
Answer: <p>The sigmoid function is a logistic function that helps to normalize the output of any input in the range between 0 to 1. The biggest advantage of the sigmoid activation function is that it squashes the input to [0,1], unlike the RELU activation, thereby aiding in increasing the efficiency and accuracy of the model. This is sometimes desirable; for example, if we want an output or intermediate layer of your net to represent the probability of something happening. In contrast, ReLu is a favored advanced activation function right now because all the drawbacks like Vanishing Gradient Problem faced by Sigmoid and Tanh&nbsp;are completely removed in it; however, it is computationally expensive. The sigmoid function is useful since learning algorithms involve lots of differentiation, thus choosing a function that is computationally cheaper to handle is helpful.</p>
------------------------------
Question: What is pooling in convolutional neural networks
Answer: <p>A pooling layer is one of the major building blocks of a Convolutional Neural Network (CNN) which is performed after the Convolution &amp; Activation Layer. It is used to reduce the spatial dimension of an image and can be compared to shrinking an image to reduce its pixel density. Max pooling is the most common pooling methodology and it works by only keeping the pixel with the highest value. Pooling is done to reduce the computational load on the system. By reducing the quality of the image, we can increase the “depth” of the layers, making the NN look for more features in the reduced image.&nbsp;Pooling also allows the convolution layer to look at the image as a whole instead of focusing on small areas of the image, aiding in the capture of higher-level features.</p>
------------------------------
Question: What is the difference between Minkowski and Hamming distance?
Answer: <p>Hamming distance is a set of positive integers that represent the number of pieces of data that would have to be changed to convert one data point into another. While comparing two binary strings of equal length, Hamming distance is the number of bit positions in which the two bits are different i.e. it measures the similarity between two strings of the same length. The Hamming distance between two strings, a and b is denoted as d(a,b).</p><p>In comparison, the Minkowski distance is a distance measurement between two points in the normed vector space (N-dimensional space) and is a generalization of the Euclidean and Manhattan distances. The p parameter of the Minkowski Distance metric represents the order of the normalization. When the order(p) is 1, it represents Manhattan Distance and when the order is 2, it represents Euclidean Distance.</p>
------------------------------
Question: What is the difference between Dataframes vs Series in Pandas?
Answer: <p>A series and data frame are both objects of the pandas library. A series is a one-dimensional object that can store values from any data type. It is somewhat like a list, but here we can define our index, instead of starting the indexing from 0 to n-1 (as in a list). We can define the indexing like a, b,c which is similar to keys in the dictionary.</p><p>DataFrame can be represented as a collection of multiple series or even a single series. It is a two-dimensional object having columns and rows. Therefore, a single column DataFrame can have a name for its single column but a Series cannot have a column name. Moreover, each column/row of a DataFrame can be converted to a series.</p>
------------------------------
Question: What is meant by reshaping a Pandas DataFrame?
Answer: <p>One of the key features of the pandas library is to reshape the dataframe. Reshaping can be thought of as transforming the dataframe so that the resulting structure makes it more suitable for data analysis or our specific needs. Reshaping is not so much concerned with formatting the values that are contained within the dataFrame, but more about transforming the shape of the entire dataframe to meet the needs of the specific task. There are three major ways of reshaping a pandas dataframe, they are pivoting, stacking and unstacking, and melting.&nbsp;They are as follows:</p><ul><li><strong>PIVOT():</strong> The pivot() function is used to create a new derived table out of the original one with the desired shape.</li><li><strong>MELT():</strong> The Melt() method is considered to be very useful when the data has one or more columns that are identifier variables, while all other columns are considered as measured variables.</li><li><strong>STACK() &amp; UNSTACK():</strong> The stack() method makes the dataframe taller by moving the innermost column index to become the innermost row index. The unstack() function is used after stacking to move the innermost row index to become the innermost column index according to the requirements of the user.</li></ul>
------------------------------
Question: What are the different ways a DataFrame can be created in pandas?
Answer: <p>The different ways a dataframe can be created in the Pandas are as follows:</p><p>1)<strong><u> Creating the Dataframe using Dictionary of Arrays:</u></strong> The arrays must be of the same length. The length of the index should be equal to the length of the array to maintain continutiy. For e.g.</p><p><span class="ql-font-monospace">data={'Name':['John', 'Edward'], 'Age':[19, 20]}&nbsp;</span></p><p><span class="ql-font-monospace">df=pd.DataFrame(data)&nbsp;</span></p><p><strong><u>2) Creating the DataFrame using Lists from the Dictionary:</u></strong> We can use, “pandas.DataFrame”, to transform the dictionary of a list to a DataFrame. For e.g.&nbsp;</p><p><span class="ql-font-monospace">dict={'Name':["John", "Edward"].&nbsp;</span></p><p><span class="ql-font-monospace">&nbsp;&nbsp;&nbsp;'Course':["B.Sc", "M.Tech"],&nbsp;</span></p><p><span class="ql-font-monospace">&nbsp;&nbsp;&nbsp;'CGPA':["8.6", "9.2"]}&nbsp;</span></p><p><span class="ql-font-monospace">df=pd.DataFrame(dict)</span></p><p><strong><u>3) Creating the DataFrame using Lists of Lists/Single Lists:</u></strong> Creating lists or multiple lists of lists to convert to a dataframe.&nbsp;For e.g.</p><p><span class="ql-font-monospace">list=['Today's', 'World', 'Is', 'Driven', 'by', 'Technology']&nbsp;</span></p><p><span class="ql-font-monospace">df=pd.DataFrame(list)&nbsp;</span></p>
------------------------------
Question: How to retrieve a row in a Pandas DataFrame?
Answer: <p>Pandas use a unique method to retrieve rows from a dataframe. There are 2 ways that can be used to retrieve a row from a pandas dataframe. One way is by label-based locations using the<em> loc() </em>function and the other way is by index-based locations using the <em>iloc() </em>function.</p><p>The <em>DataFrame.loc[]</em> method takes only index labels as parameters and returns rows of the dataframe if the index label exists in the caller dataframe. For e.g. In case of a dataframe object with columns, 'X', 'Y', 'Z' and rows, 'A', 'B', 'C', 'D'. The command, <em>dataframe.loc['A']</em> can be used to get a row with the label A.</p><p>Another way to retrieve a row is through index-based locations using the <em>dataframe.iloc()</em> function. Let's say we create a dataframe object whose rows are labeled, 'A', 'B', 'C', and 'D'. With index-based locations, we reference each row with an index. These indexes always begin at the location of 0. Therefore, the first row, 'A', can be referenced using the statement, <em>dataframe1.iloc[0].</em></p>
------------------------------
Question: What is Data Aggregation?
Answer: <p>Data aggregation is often used to provide statistical analysis and create useful summaries of data for business analysis. Data aggregators are tools that summarize data from multiple sources and provide capabilities for multiple aggregate measurements, such as sum, average, and count. Aggregate data does not need to be numeric, e.g. counting the number of any non-numeric data element.</p><p>The main task of Data Aggregation is to apply an aggregation function to one or more columns, to summarize the data, and draw answers from the collected data. Some of the methods commonly used are:</p><ul><li><strong>sum:</strong> Used to return the sum of the values for the requested axis.</li><li><strong>min:</strong> Used to return a minimum of the values for the requested axis.</li><li><strong>max:</strong> Used to return maximum values for the requested axis.</li></ul><p>There are other aggregation tools and techniques, but the above are the most commonly used aggregation functions to summarize data.</p>
------------------------------
Question: What Are The Most Important Features Of The Pandas Library?
Answer: <p>The Pandas library is one of the most popular libraries for data augmentation due to its ease and functionality. It has multiple features and some of them are mentioned as follows:</p><ul><li><strong>Data Alignment- </strong>Pandas provide extremely streamlined forms of data representation and alignment, thereby helping to analyze and understand the data better.</li><li><strong>Memory Efficiency- </strong>It is highly efficient in memory consumption, and can be used for ease of access and storing. Pandas also help to save a lot of time by importing large amounts of data very fast.</li><li><strong>Reshaping-</strong> Reshaping the data to fit the user's needs is one of the major advantages of Pandas.</li><li><strong>Data flexibility-</strong> Pandas provides a huge feature set to apply to the data so that we can customize, edit and pivot it according to our own will and desire.</li><li><strong>Time Series-</strong> Pandas contains extensive capabilities and features for working with time series data for all domains, due to its vast features and good documentation.</li></ul>
------------------------------
Question: How can we sort a DataFrame?
Answer: <p>Using the <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html" rel="noopener noreferrer" target="_blank"><strong>sort_values</strong></a> function from the Pandas library we can sort the dataframe according to our needs.</p><p>If the dataframe is named as df and we want to sort on a column called ‘<em>Col 1’</em>, then the syntax is <em>df.sort_values(‘Col 1’)</em>. By default, the function will sort the column and it will look to sort the column named <em>'Col 1'</em>. The function has multiple parameters which can be used to additionally specify multiple arguments to meet the user's needs.&nbsp;Some of the arguments are, as follows:</p><ul><li>The<strong> 'inplace' </strong>argument (if the sorted dataframe must be overwritten in df i.e. treated as a permanent change in the original dataframe). </li><li>The <strong>'axis' </strong>argument (to sort by row or column axis=1 signifies a column and axis=0 signifies a row). The&nbsp;'<strong>ascending'</strong> argument (True to sort in ascending order, else false for descending) among some other useful parameters.</li></ul>
------------------------------
Question: How to append new rows to a pandas DataFrame?
Answer: <p>The<em> </em><a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html" rel="noopener noreferrer" target="_blank"><em>dataframe.append()</em></a><em> </em>function in the Pandas library can be used to append desired rows to the end of the specified dataframe, returning a new dataframe object. Columns that are not in the original dataframe are added as new columns and the new cells are populated with NaN values.</p><p><u>Example of adding new rows to the specified dataframe:</u></p><p>df1 = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB')) // Creating Dataframe1</p><p>df1</p><p>&nbsp;&nbsp;<strong>A&nbsp;B</strong></p><p><strong>0</strong>&nbsp;1&nbsp;2</p><p><strong>1</strong>&nbsp;3&nbsp;4</p><p>df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB')) // Creating Dataframe2</p><p>df1.append(df2) // Final result after appending df2 to the end of df1</p><p>&nbsp;&nbsp;<strong>A&nbsp;B</strong></p><p><strong>0</strong>&nbsp;1&nbsp;2</p><p><strong>1</strong>&nbsp;3&nbsp;4</p><p><strong>0</strong>&nbsp;5&nbsp;6</p><p><strong>1</strong>&nbsp;7&nbsp;8</p>
------------------------------
Question: What is Reindexing in Pandas?
Answer: <p>Reindexing changes the row labels and column labels of a DataFrame and is one of the most important features of the Pandas library. To reindex means to transform the data to match a given set of labels along any particular axis. One can reindex a single row/column or multiple rows/columns by using the<em> reindex()</em> method. Default values in the new index that are not present in the dataframe are assigned 'NaN'.</p><p><u>Example of reindexing a dataframe's(df1), row index:</u></p><p>column=['a','b','c','d']</p><p>index=['A','B','C','D']&nbsp;</p><p><strong># creating a dataframe of random values</strong></p><p>df1 = pd.DataFrame(np.random.rand(4,4),</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;columns=column, index=index)</p><p>&nbsp;<strong># Reindexing the dataframe df1 using the reindex() method to change row index</strong></p><p>df1.reindex(['B', 'D', 'A', 'C'])</p>
------------------------------
Question: What are categorical values in Pandas?
Answer: <p>The categorical data type is non-numeric or qualitative data which represents characteristics such as gender, state, zip code, status, etc. Any data element that helps summarize metrics or measures would fall under categorical data. We can create bins of continuous data to make them categorical elements.</p><p>It can take on numerical values (such as “1” indicating male and “2” indicating female), but those numbers don’t have mathematical meaning i.e they cannot be added or subtracted, they are only labels to determine a specific category. Categorical values in Pandas can also take only a limited and fixed number of possible values, and are very useful in classification problems such as Cats and Dogs classification among many others.</p><p><br></p>
------------------------------
Question: What Are The Different Types Of Data Structures In Pandas?
Answer: <p><strong><u>The different types of data structures available in Pandas are, as follows:</u></strong></p><p><strong>Series – </strong>It is immutable in size and homogeneous, one-dimensional array data structure.</p><p><strong>DataFrame – </strong>It is a two-dimensional, tabular data structure that comprises rows and columns. Here, data and size are mutable, and multiple different data types can be represented via different columns in the dataframe.</p><p><strong>Panel –</strong> It is a three-dimensional data structure, used to store heterogeneous data .</p>
------------------------------
Question: What is the procedure to add a column to a Pandas Dataframe?
Answer: <p>Columns of a Pandas Dataframe are a type of Series. Therefore, a Dataframe is composed of one or more Pandas Series, which are wrapped around NumPy ndarrays (and associated with indices for rows and columns). There are multiple ways to add a new column to an existing dataframe, one of the simplest methods for adding a new column to an existing Dataframe is as follows:</p><p><br></p><p><strong># Creating a simple 4 column matrix:&nbsp;</strong></p><p>a = np.arange(100).reshape((25,4))&nbsp;</p><p><strong># Creating a DataFrame there from:&nbsp;</strong></p><p>df = pd.DataFrame(a, columns=['a','b','c','d'])&nbsp;</p><p><strong># Creating a new column:&nbsp;</strong></p><p>z = pd.Series(np.arange(25,0,-1))&nbsp;</p><p><strong># Adding the new column named 'new' to the DataFrame&nbsp;</strong></p><p>df['new'] = z&nbsp;</p><p><strong><u>OUTPUT:</u></strong></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/c8587c36cbd3766223acabbab1d3eb38.png"></p>
------------------------------
Question: Why are dropouts needed in training a deep learning model?
Answer: <p>The idea of dropout is such that, during the forward-pass, the network just randomly sets some of the neurons to zero to make the network independent of particular nodes. Dropouts are implemented per layer in a neural network. It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers(LSTM layers). With dropouts, the weights of the nodes learned through backpropagation become somewhat more insensitive to the weights of the other nodes and learn to decide the outcome independent of the other neurons. Dropouts help prevent the network from relying on one node in the layer too much, thereby eliminating the problem of overfitting. A large network with more training and the use of a weight constraint is suggested when using dropout regularization to prevent underfitting due to the dropping of nodes.</p>
------------------------------
Question: What is the difference between lasso regression versus ridge regression?
Answer: <p>The basic difference between the two is that Ridge regression performs “L2 Regularization” whereas Lasso performs “L1 Regularization”.</p><p>In Ridge regression, the coefficients are estimated by trying to minimize the sum of squares of errors (focuses on prediction accuracy) with the constraint that the sum of the squares of the coefficients is less than a certain value (forming the L2 Norm and keeping the coefficients from growing too large). Therefore, Ridge regression is useful in reducing or preventing overfitting, but may not really help with model parsimony or with feature selection.</p><p>In contrast, Lasso regression changes the constraint to keep the sum of the absolute values of the coefficients less than a certain value. Just like in Ridge regression, the constraint penalizes the coefficients and keeps them from growing large, thus reducing or preventing overfitting. However, compared to Ridge, it can actually push the coefficients to zero, thereby removing variables/features from the model, thus achieving model parsimony(simple yet high predictive powers) and feature selection as well.</p>
------------------------------
Question: How can we convert a Series to DataFrame?
Answer: <p>The <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.to_frame.html" rel="noopener noreferrer" target="_blank"><em>to_frame()</em></a> function can be used to convert a Pandas Series to a DataFrame. For e.g.</p><p><br></p><p><strong>#Creating the series</strong></p><p>s = pd.Series(["p", "q", "r"],</p><p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name="vals")</p><p><strong>#converting the series to a dataframe with default indexes</strong></p><p>s.to_frame()&nbsp;</p><p><br></p>
------------------------------
Question: What is the difference between semantics and pragmatics in natural language processing?
Answer: <p>Semantics has to do with meaning and has a number of sub-branches. Meanwhile, pragmatics has to do with the usage of language in various contexts.</p><p>For example, the sentence "Our professor was not drunk today" literally means a certain person denoted by "our professor" was not in a certain state (being drunk) on a certain day (denoted by "today"). This is the semantic meaning of this sentence, and semantics is a systematic exploration of such meanings. On the other hand, this sentence implies that on other days the professor was drunk. This is the pragmatic meaning of this sentence, and pragmatics studies how such meanings are created.</p>
------------------------------
Question: What are degrees of freedom in statistics?
Answer: <p>In Layman's terms, the DoF is the difference between the number of unknowns and the number of constraints. If a proper degree of freedom is not used to calculate an estimate, then the resultant estimate will be a biased one, which is undesirable.&nbsp;For e.g. Let the mean be 9 for a set of 3 no's, At best, we can choose any two of the three numbers freely but as soon as we do that, we lose the freedom to choose the last number: <strong>(4+12+x)/3 = 9; x</strong> can only be <strong>11</strong>. Therefore, DoF can be defined as the minimum number of independent coordinates that can specify the position of a system correctly.</p>
------------------------------
Question: What is the difference between autoencoding and autoregression?
Answer: <p><em>Autocorrelation </em>is defined as the correlation of a signal with a delayed copy of itself as a function of delay. It is the similarity between observations as a function of the time lag between them. It is the actual correlation between a data point and its lag (some period prior).</p><p><br></p><p><em>Autoregression</em> is a time series model that uses observations from previous time steps as input to a regression equation to predict the value at the next time step. It is a very simple idea that can result in accurate forecasts on a range of time series problems. It is used to describe the process/model when one value may be affected by prior values.</p><p><br></p>
------------------------------
Question: What is the Haar cascades algorithm?
Answer: <p>A Haar cascade classifier is an ML object detection program that identifies objects in an image and video. They were used in one of the first real-time face detectors. One of its major drawbacks is that this algorithm requires a lot of positive images of faces and negative images of non-faces to train the classifier i.e. it is dependent on the quality and quantity of the dataset. The entire algorithm can be described in terms of these 4 major steps:</p><ol><li><strong>Calculating Haar Features:</strong> Calculations performed on adjacent rectangular regions at a specific location in a detection window.</li><li><strong>Creating Integral Images: </strong>Integral images are sub-rectangles with array references to avoid computing at each pixel and compute Haar features. These images speed up the calculation of the Haar features.</li><li><strong>Using Adaboost:</strong> Adaboost is an ensemble learning method, which chooses the best features and trains the classifiers to use them.</li><li><strong>Implementing Cascading Classifiers:</strong> These classifiers, made up of a series of stages, where each stage is a collection of weak learners are implemented with tuned hyperparameters.</li></ol>
------------------------------
Question: 1.	How can we calculate the standard deviation from the Series in Pandas?
Answer: <p>The Pandas <strong>std()</strong> function is used for calculating the standard deviation of any given set of numbers, DataFrame, column, and rows. The function returns sample standard deviation over the requested axis, normalized by N-1 by default. The arguments of the function are explained in greater detail in the official Pandas<a href="https://pandas.pydata.org/docs/reference/api/pandas.Series.std.html" rel="noopener noreferrer" target="_blank"> documentation</a>.</p><p><u>The syntax to calculate the standard deviation of any series is as follows:</u></p><p><strong>Series.std(axis=None, skipna=None, level=None, ddof=1, numeric_only=None)&nbsp;&nbsp;</strong></p><p>For a dataframe named <em>df</em>, we can calculate the standard deviation of a particular column, which is a series, as <strong>df['low_var'].std() </strong>where <em>'low_var'</em><strong><em> </em></strong>is the name of the column.</p>
------------------------------
Question: What is the advantage of performing dimensionality reduction before fitting an SVM?
Answer: <p>Both <em>Principal Component Analysis (PCA)</em> and <em>Singular Value Decomposition (SVD)</em> help in reducing the number of dimensions(features) in the data space and improving the discriminative power of the SVM classifier. However, since the original data is lost forever after decompression, it may affect the performance of the <em>Support Vector Machine (SVM)</em> by changing the data-space drastically. The drawback however is that, since we ignore dimensions with lower variance, applying PCA makes it prone to information loss, which is dangerous if applied blindly without knowledge of the data. Therefore it is best to run the SVM classifier on data where PCA has been performed and simultaneously on data where it has not been performed to compare the tradeoff between accuracy and performance and decide accordingly, keeping in mind the advantages and disadvantages.</p>
------------------------------
Question: What are the different types of gradient descent?
Answer: <p>Gradient Descent is an optimization algorithm used for minimizing the cost function in various ML algorithms and is mainly used for updating the parameters of the learning model.</p><p><br></p><p><u>There are mainly 3 types of gradient Descent:</u></p><p><br></p><p><strong>Batch Gradient Descent:</strong> This is a type of gradient descent that processes all the training examples for each iteration of gradient descent.&nbsp;</p><p><strong>Stochastic Gradient Descent: </strong>This is a type of gradient descent that processes 1 training example per iteration.&nbsp;</p><p><strong>Mini Batch gradient descent:</strong> It is a type of gradient descent that works faster than both batch gradient descent and stochastic gradient descent by processing data in batches simultaneously making it fast and effective.</p>
------------------------------
Question: Are there any techniques for setting the learning rate in a machine learning algorithm?
Answer: <p>While setting an optimal learning rate, there is a trade-off between the rate of convergence and overshooting.</p><p>Unfortunately, we cannot analytically calculate the optimal learning rate for a given model on a given dataset. Instead, a suitable learning rate must be discovered via trial and error. Smaller learning rates require more training epochs, in contrast, larger learning rates require fewer training epochs. Therefore, diagnostic plots are often used to investigate the learning rates impact on the models learning and dynamics. For e.g. a line plot of the loss over training epochs during training can be used to determine a good learning rate. Alternatively, a sensitivity analysis of the learning rate for the chosen model can be performed to highlight a good learning rate.</p>
------------------------------
Question: How can two arrays be merged using numpy?
Answer: <p>Two numpy arrays can be merged using the <a href="https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html" rel="noopener noreferrer" target="_blank">numpy.concatenate</a> function provided by numpy library. The following image highlights how to merge two numpy arrays using the function:</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/c67918f3d2bb6ad38014245b230b5e6d.png"></p><p>The function is mainly used to join a sequence of arrays along an existing axis. The list of arguments to join the arrays can be found in the documentation page in more detail.</p>
------------------------------
Question: What is the seed() function in numpy?
Answer: <p>The <em>numpy.random.seed() </em>function is used to set the seed for the pseudo-random number generator algorithm. The pseudo-random number generator performs predefined operations on the seed and produces a pseudo-random number in the output. A pseudo-random number is a number that appears random but actually isn’t. Computers are incapable of generating a truly random number due to their deterministic nature. It also consistently follows a given set of instructions, leading us to always get the same set of random numbers for the same seed on any machine.&nbsp;</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/3c948be15b94d0a81b1d06ac001f61df.png"></p><p><br></p><p>The main benefit of using different seeds will cause NumPy to produce different pseudo-random numbers as the output will be dependent on the seed we are choosing.</p>
------------------------------
Question: How to reshape a numpy array?
Answer: <p>The <a href="https://numpy.org/doc/stable/reference/generated/numpy.reshape.html" rel="noopener noreferrer" target="_blank"><em>numpy.reshape()</em></a> function is used to gives a new shape to an array without changing its data. The new shape should be compatible with the original shape. More information regarding the arguments can be found in the official documentation.</p><p><br></p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/0e858b75ced40cc631d8c543c30c45a6.png"></p><p class="ql-align-center"><strong>CONVERTING 1-D ARRAY TO 2-D</strong></p><p class="ql-align-center"><br></p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/0e858b75ced40cc631d8c543c30c45a6.png"></p><p class="ql-align-center"><strong>CONVERTING 1-D ARRAY TO 3-D</strong></p>
------------------------------
Question: How to stack two arrays vertically in numpy?
Answer: <p>The <a href="https://numpy.org/doc/stable/reference/generated/numpy.vstack.html" rel="noopener noreferrer" target="_blank"><em>numpy.vstack()</em></a> function, which is a part of the numpy library is commonly used to stack the sequence of input arrays vertically, thereby forming a single array. This is equivalent to concatenation along the first axis(row-wise).&nbsp;</p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/d31b433b3eae0d90793a89dd3cbeb3d9.png"></p><p class="ql-align-center"><strong>Stacking 3 numpy arrays vertically</strong></p>
------------------------------
Question: How to stack two arrays horizontally in numpy?
Answer: <p>The <a href="https://numpy.org/doc/stable/reference/generated/numpy.hstack.html" rel="noopener noreferrer" target="_blank"><em>numpy.hstack()</em></a> function is used to stack arrays in sequence horizontally (column wise). It is equivalent to concatenation along the second axis and is similar to the vstack() function in terms of stacking and parameters, but it's just along the column axis instead of row.</p><p class="ql-align-center"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/bd296209095637581259f10acbc64d5b.png"></p><p class="ql-align-center"><strong>Example of using the <em>hstack()</em> function for horizontal concatenation.</strong></p>
------------------------------
Question: What is the test area?
Answer: <p>IS the area for test users.</p>
------------------------------
Question: What is the difference between data mining and data analysis?
Answer: <p>Data mining is the process of discovering patterns in large data sets involving methods at the intersection of artificial intelligence, machine learning, statistics, and database systems. Data analysis is the process of inspecting, cleaning, transforming, and modeling data with the goal of discovering useful information, suggesting conclusions, and supporting decision making.</p>
------------------------------
Question: 2.	What is Data Wrangling or Data Cleansing/Cleaning?
Answer: <p>Data wrangling or data munging is the process of transforming and mapping data from one raw data form into another form with the intent of making it more appropriate and valuable for various tasks. In contrast, data cleaning or cleansing is the process of detecting and removing corrupted or inaccurate records from a record set, table or database.</p>
------------------------------
Question: what is collaborative filtering?
Answer: <p>The collaborative filtering method is commonly used in the development of recommender systems. It uses similarities between users and items simultaneously to provide recommendations. This allows for recommendations, using the interactions and data collected by the system from other users. Collaborative filtering models can recommend an item to user A based on the interests of a similar user B. It's based on the basic principle that people who agreed in their evaluation of certain items/content are likely to agree again in the future.</p>
------------------------------
Question: What is A/B testing?
Answer: <p>A/B testing, or split testing, is the creation of at least one variant to test against a current webpage to ascertain which one performs better in terms of agreed metrics such as revenue per visitor (for e-commerce websites). It allows us to make more out of your existing traffic so that we can test and optimize our web page to bring the best result in conversions. In a broader sense, it is used to test different elements of a user's experience to determine which variation helps the business achieve its goal more effectively.</p><p>Example: If we want to test a headline of our website, we can design two similar web pages (let’s call them A and B) with separate headlines and directs the same traffic to each web page at the same time to see which web page is getting more views, higher conversions, etc.</p>
------------------------------
Question: What is data modeling?
Answer: <p>Data modeling is a data formulation process in a standardized format in an information system that helps to quickly analyze data, to meet business needs. It is very important as it provides a coherent framework for designing a system as opposed to an approach with no planning and aids to improve overall functionality/integrability within a larger system. Two main modeling approaches are:</p><ul><li><strong>Bottom-up</strong> models often start with existing data structures forms, fields, or reports. These models are usually physical, application-specific, and incomplete from an enterprise perspective.</li><li><strong>Top-down</strong> logical models, on the other hand, are created in an abstract way by getting information from people who know the subject area.</li></ul><p>There also exists a hybrid model, created using a mixture of the two methods, by considering the data needs and structure of an application while consistently referencing a subject-area model.</p>
------------------------------
Question: Why is data cleansing important for data visualization?
Answer: <p>Data cleansing reduces noise. In the entire process of data mining and analytics, the hardest part is often getting the right data. Data cleansing is also important because it improves our data quality and in doing so, increases overall productivity.&nbsp;Most of the real-world data are dirty and almost always unstructured, transforming these data into feature vectors for any algorithms, would require this step.</p><p><strong>For example:</strong></p><p><strong>1.</strong> You have a csv file with the date fields written as: 2014%%:12FL:10 where what you wanted was 2014-12-10</p><p>=&gt; DataCleaner(2014%%:12FL:10) = 2014-12-10 | Where, DataCleaner is a function that fixes the date formatting.</p><p><strong>2.</strong> Removing all those unneeded '\n','\t','\r','&lt;li&gt;','&lt;p&gt;',etc html tags from data you've scraped from your the internet.</p>
------------------------------
Question: What is the difference between and K-nearest neighbors algorithms?K-means clustering and
Answer: <p><strong>K-nearest neighbor(Knn)</strong> is a type of supervised learning algorithm. It is supervised because we are trying to classify a point based on the known classification of other points and is mainly used for the classification and regression of given data when the attribute is already known.&nbsp;</p><p><br></p><p>In contrast, <strong>K-means</strong> is a type of unsupervised learning, clustering algorithms that tries to partition a set of points into K sets (clusters) such that the points in each cluster tend to be near each other. It is unsupervised because the points have no external classification i.e. are unlabelled.</p><p><br></p>
------------------------------
Question: 4.	What is the Difference Between Covariance and Correlation?
Answer: <p><strong>Correlation </strong>and<strong> covariance</strong> are very closely related to each other, and yet they differ a lot. Covariance is affected by the change in scale, i.e. if all the value of one variable is multiplied by a constant and all the value of another variable are multiplied, by a similar or different constant, then the covariance is changed. </p><p>In contrast, correlation is not influenced by the change in scale. It is dimensionless, i.e. it is a unit-free measure of the relationship between variables. </p><p>When it comes to making a choice of, which is a better measure of the relationship between two variables, correlation is preferred over covariance, in most cases because it can also be used to make a comparison between two pairs of variables while remaining unaffected by the change in location and scale.</p>
------------------------------
Question: What are treemaps as data visualization?
Answer: <p>Treemap is a visual representation of a data tree, where each node is displayed as a rectangle, sized and colored according to values assigned by a user. The size and color dimensions correspond to node value relative to all other nodes in the graph. They are used to capture two types of information in the data: </p><p>&nbsp;(1) The value of individual data points, and</p><p> (2) The structure of the hierarchy.</p>
------------------------------
Question: What is an indifference curve in economics?
Answer: <p>In economics, an indifference curve is a curve that shows the combination of two goods that give a consumer equivalent satisfaction and utility. This curve indicates that a consumer is indifferent about the two products since he derives equal satisfaction from both. The indifference curve analysis works on a simple graph on the co-ordinate plane. Each individual axis indicates a single type of economic good. An indifference curve is the loci of all combinations of the two goods that give the consumer the same level of utility.</p><p><br></p><p>Typically, an indifference curve is a convex, downward sloping curve. This is because, as the quantity of one good decreases, the quantity of the other good must be increased in order to compensate the consumer and provide him with the same level of utility as earlier, and vice versa.</p>
------------------------------
Question: Explain the cardinal utility framework.
Answer: <p>Cardinal Utility is the idea that economic welfare can be directly observable and be given a value. For example, people may be able to express the utility that consumption gives for certain goods. For example, if a Nissan car gives 5,000 units of utility, a BMW car would give 8,000 units. This is important for welfare economics which tries to put values on consumption.</p><p><br></p><p>In this manner, not only can we establish a consumer's preference of one good over the other, we can also determine the magnitude of his preference, i.e., by how much the consumer prefers one good over another.</p>
------------------------------
Question: What do we mean by the point of saturation in economics?
Answer: <p>Point of satiety, or point of saturation in consumption is the point where a consumer maximizes his level of satisfaction and needs no more units to consume. A common method of representing the point of saturation is by plotting a graph on the co-ordinate plane. Let the X-axis denote the quantity consumed of a particular good, while the Y-axis denotes the total utility obtained by the consumer. In most cases, the total utility rises with an increase in the quantity of the good consumed, until it reaches its maxima, after which the utility begins to decrease with an increase in the quantity - thereby forming an inverted U-shaped curve. That maximum point is the point of saturation. </p><p><br></p><p>Market saturation arises when the volume of a product or service in a marketplace has been maximized. This usually happens due to competition, decreased need of the product, or obsolescence among other factors. At the point of saturation, a company can only achieve further growth through new product improvements by taking existing market share from competitors or increasing overall consumer demand.</p>
------------------------------
Question: Explain the concept of marginal utility.
Answer: <p>Marginal utility is defined as the utility derived from the marginal or additional unit of a commodity consumed by an individual. The concept of marginal utility is used by economists to determine how much of an item consumers are willing to purchase. It can be positive, negative, or even zero.</p><p><br></p><p>For example, let's say the utility derived from the consumption of two pizzas in 8 utils, and the utility derived from the consumption of three pizzas in 11 utils. This means, that the marginal utility derived from the consumption of the third pizza is 3 utils. Let us further assume that the total utility derived from the consumption of four pizzas is 10 utils (overconsumption leads to decrease in satisfaction). In this scenario, the marginal utility derived from the consumption of the fourth pizza is -1 utils.</p>
------------------------------
Question: What is the law of diminishing marginal utility?
Answer: <p>In economics, the&nbsp;law of diminishing marginal utility&nbsp;states that the&nbsp;marginal utility&nbsp;of a good or service declines as more of it is consumed by an individual. Economic actors receive less and less satisfaction from consuming incremental amounts of a good. Demand curves&nbsp;are downward-sloping in microeconomic models since each additional unit of a good or service is put toward&nbsp;a less valuable use. Let us consider an example.</p><p><br></p><p>Suppose, that the total utility obtained from the consumption of one, two, and three slices of pizza is 5 utils, 8 utils, and 10 utils respectively. Here, the marginal utilities of slice 1, 2, and 3 are 5, 3, and 2 respectively. As we can notice, the marginal utility decreases as the consumption of pizza increases, although the total utility continues to increase simultaneously. This is because, the first slice of pizza provides more satisfaction to the consumer than the slices to follow can. Upon proceeding to the second slice, the consumer's appetite has already reduced from its initial stage, and the taste of the pizza has also been enjoyed. Naturally, the second slice fails to provide as much satisfaction as the first slice does.</p>
------------------------------
Question: List the properties of indifference curves.
Answer: <p>Indifference curves have four key properties, as listed below -</p><p><br></p><ol><li><strong>Indifference curves have a negative slope</strong> - Indifference curve being downward sloping means that when the amount of one good in the combination is increased, the amount of the other good is reduced. This must be so if the level of satisfaction is to remain the same on an indifference curve. If, for instance, the amount of good 1 is increased in the combination, while the amount of good 2 remains unchanged, the new combination will be preferable to the original one and the two combinations will therefore not lie on the same indifference curve (assuming safely that more of a commodity gives more satisfaction). Point B has a larger quantity of good 1 than point A does, so for both points to be on the same indifference curve, point B must have a lower quantity of good 2.<img src="https://www.zigya.com/application/zrc/images/qvar/ECEN12043821.png" alt="Briefly explain the following concepts. (a) Indifference curve and its  properties. (b) Indifference map. from Economics Theory Of Consumer  Behaviour Class 12 CBSE"></li><li><strong>Indifference curves are convex to the origin - </strong>In other words, the indifference curve is relatively flatter in its right-hand portion and relatively steeper in its left-hand portion. This is because of the law of diminishing marginal rate of substitution. As more and more of good Y is provided to compensate for less and less of good X, each decrement in X must be balanced by a greater increment in Y.</li><li><strong>Indifference curves cannot intersect each other - </strong>This is because if two indifference curves intersect, then their point of intersection would belong to both curves. Invariably, one curve must be above the other, indicating a level of higher satisfaction. However, the presence of a common point to both curves refutes that very claim. Hence, two indifference curves can never intersect. Here, point A is superior to point B, but point C is common to both indifference curves, which is a fallacy.<img src="https://www.researchgate.net/profile/Eleftherios-Giovanis/publication/312032862/figure/fig1/AS:446374513778689@1483435385780/Indifference-Curves-Cannot-Intersect-Each-Other.png" alt="Indifference Curves Cannot Intersect Each Other | Download Scientific  Diagram"></li><li><strong>A higher indifference curve represents a higher level of satisfaction than a lower indifference curve - </strong>The combinations which lie on a higher indifference curve will be preferred to the combinations which lie on a lower indifference curve. This is because, so long as the assumption that both goods are desirable holds true, more of both goods will always be preferred.<img src="https://farm6.staticflickr.com/5651/22494061327_a089d510b0_o.png" alt="Important Questions for Class 12 Economics Indifference Curve , Indifference  Map and Properties of Indifference Curve"></li></ol>
------------------------------
Question: What are some limitations of the cardinal approach to utility in economics?
Answer: <p>The Cardinalist school of thought hypothesizes that the utility that an individual derives from the consumption of a good, can be expressed in absolute terms which is unrealistic since utility or satisfaction is a purely abstract concept, hence providing cardinal numbers to utility is not a sustainable concept. This is opposed to the far more dignified hypothesis that the Ordinal School of thought says that utility is a concept that can only be expressed in relative terms ie with respect to the satisfaction derived from a related or unrelated commodity. This concept of ranking our utility lets us study concepts like the effect of price changes on the demand for a commodity and is the basis of the Classical Demand Theory.</p>
------------------------------
Question: What are perfect substitutes? Give an example.
Answer: <p>Perfect Substitute Goods are those goods that can satisfy the same necessity in exactly the same way. A substitute good can be used in place of another. In some cases of consumption, a two-good (X and Y) consumer may prefer to substitute one of the goods, say, X, for the other good Y at a constant rate, to keep his level of utility constant, i.e., marginal rate of substitution between X and Y is constant.</p><p>For example, compact disks (CDs) from different&nbsp;providers are nearly perfectly substitutable. The utility provided by substitute goods is an increasing function of the sum of the quantity of each good. For example, the following function is a utility function of two perfect substitute&nbsp;goods: U = X+Y.</p><p>A perfect substitute can be used in exactly the same way as the good or service it replaces and this is where the utility of the product or service is pretty much identical. For example, a two-dollar bill is a perfect substitute for two one-dollar bills.</p>
------------------------------
Question: What are perfect complements? Explain with the help of an example.
Answer: <p>A complementary good or service is an item used in conjunction with another good or service. Usually, the complementary good has little to no value when consumed alone, but when combined with another good or service, it adds to the overall value of the offering. Two goods are perfect complements when they must always be consumed in a precise ratio. Excess of either of the two goods without an increment in the other is rendered useless as the two goods must always be consumed in a given ratio for any utility to be derived.</p><p><br></p><p>For example, consider left foot shoes and right foot shoes. Almost always, the two goods consumed individually have little to no value. However, when paired together, some utility is obtained.</p>
------------------------------
Question: Explain the concept of marginal rate of substitution.
Answer: <p>In economics, the marginal rate of substitution (MRS)&nbsp;is the amount of a good that a consumer is willing to consume in relation to another good, as long as the new good is equally satisfying. Marginal rate of substitution is the slope of the indifference curve at any given point along the curve and displays a frontier of utility for each combination of goods X and Y. When the law of diminishing MRS is in effect, the MRS forms a downward, negative sloping, convex curve showing more consumption of one good in place of another. Mathematically,</p><p><br></p><p>MRS<sub>XY</sub> = ?Y/?X</p>
------------------------------
Question: What does an indifference curve look like if both goods are bad?
Answer: <p>When both goods are bad, more of either good makes the consumer worse off. Let us refer to the usual co-ordinate plane. Let the X-axis represent good X and the Y-axis represent good Y, both of which are bad goods. If good Y is reduced, the consumer is better-off. So, for the consumer to be on the same indifference curve, good X must be increased. As a result, the shape of the indifference curve is downward sloping like in the general case of two normal goods. However, since less is better in this scenario, the indifference curve map moves inward towards the origin. As a result, the curves are concave to the origin.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/fff2ebd6a1f9b92e12e717caef98e657.png"></p>
------------------------------
Question: What is ordinal utility?
Answer: <p>Ordinal Utility explains that the satisfaction after consuming a good or service cannot be scaled in numbers, however, these things can be arranged in the order of preference. In the 1930s, two English economists, John Hicks and R.J. Allen argued that the theory of consumer behavior should be developed on the basis of ordinal utility. It is highly subjective in nature and varies across individuals and therefore, it cannot be measured in quantifiable terms. The ordinal utility theory explains that it is meaningless to ask how much better one good is as compared to another and therefore, all these studies of customer decision making are expressed in terms of ordinal utility. The concept of ordinal utility was originally introduced by Vilfredo Pareto. It is argued this is more relevant in the real world. We often make choices between two goods, but hardly quantify by how much we prefer one good over the other.</p>
------------------------------
Question: What is a budget constraint in microeconomics?
Answer: <p>The budget constraint is <strong>the boundary of the opportunity set</strong>—all possible combinations of consumption that someone can afford given the prices of goods and the individual's income. Consider a co-ordinate plane with the x and y axes representing goods 1 and 2 respectively. The budget constraint is a curve in this infinite space that divides it into two parts - the affordable set and the unaffordable set.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/df4957839e0e357ac005d205ec34e847.png"></p>
------------------------------
Question: How does the budget line change with a change in the price of goods?
Answer: <p>Assume a co-ordinate plane where the two axes represent two goods. Given any arbitrary set of prices for the two goods and a constant income of the consumer, we have a budget line. Now, let us study the effect of change in price of good 1, while the price of good 2 remains unchanged. If good 1 becomes cheaper, the consumer is able to afford a larger quantity of it without having to change his maximum possible purchasing capability of good 2. Hence, the intersect on the axis representing good 1 shifts further away from the origin. The budget line becomes flatter if good 1 is on the X axis and steeper if it's on the Y axis. On the other hand, if the price of good 1 increases, then the consumer is able to afford less of it. Hence, the intersect on the axis representing good 1 shifts closer to the origin. The budget line becomes flatter if good 1 is on the Y axis and steeper if it's on the X axis.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/307ca3f391931d862073e151a34ece24.png"></p>
------------------------------
Question: How does a consumer find his utility maximizing choice given his budget?
Answer: <p>The theory of consumer behavior is built on the premise that, given the budget constraint, the consumer seeks to maximize utility, that is, to remain on the highest possible indifference curve. Consider the following diagram. The budget constraint of the consumer is given by the line BC. The three indifference curves used for study are I1 &lt; I2 &lt; I3.</p><p>Clearly, any optimal solution must be on the budget line, as any point below it would be feasible, but underutilization of resources, and any point above it beyond the capability of the given budget. Moreover, the higher indifference curve that has a point in common with the budget line is one that is tangent to it. Hence, (X*,Y*) is the optimal solution in this scenario, which lies on the indifference curve I2.</p><p><br></p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/0/04/Consumer_constraint_choice.svg/405px-Consumer_constraint_choice.svg.png" alt="Consumer choice - Wikipedia"></p>
------------------------------
Question: What is a consumer's demand function for a good?
Answer: <p>In economics, demand is a principle referring to a consumer's desire for a specific good or service. The utility function describes the amount of satisfaction a consumer gets from a particular bundle of goods. A consumer's budget—the amount of money available to spend on a product or service—is combined with the utility function to determine the demand function. For example, say there are two goods a consumer can choose from, x and y. To maximize utility, the consumer wants to use the entire budget to&nbsp;buy&nbsp;the most x and y possible. The demand function for either good is a schedule showing the quantities of the good demanded for each combination of income and prices of the two goods.</p>
------------------------------
Question: What is the difference between normal and inferior good?
Answer: <p><span style="background-color: transparent; color: rgb(0, 0, 0);">Inferior goods refer to the commodities, the demand for which falls as the income of the consumer increases. Examples of inferior goods include low quality food grains. The intuition behind this inverse relationship is that as the income of the consumer crosses a certain threshold value, the consumer can substitute these, with better quality goods. If we plot, income and quantity demanded of an inferior good, we will notice a downward sloping curve (Engel curve). The income effect for an inferior good is also negative. Normal goods on the other hand refer to the commodities, the demand for which increases as the income of the consumer increases. Say, luxury goods, these are a seminal example of what normal goods are like. The Engel curve for normal goods is upward sloping and the income effect for normal goods is positive.</span></p><p><br></p><p><br></p>
------------------------------
Question: Under what circumstances are corner solutions optimal?
Answer: <p><span style="background-color: transparent; color: rgb(0, 0, 0);">Corner solutions are obtained when it is utility maximizing for the consumer to spend all of his income on a single good. In this scenario, the indifference curve of the consumer isn't convex to the origin as is assumed generally. Assume that the indifference curves of the consumer are concave to the origin.</span></p><p><br></p><p><span style="background-color: transparent; color: rgb(0, 0, 0);"><img src="https://lh3.googleusercontent.com/stfv6nH0xy-t2mhaShZc9VE_Iy3HCnu_i695chcUu8JAvQ9qrlW6WFU-YMFE9GVzfWgi5geFQOCtHzM5BSxaAHO3zpb-24QahVyPkUsEial_6HO3spIzZjvvl37gcaAjBTZKVFW9" alt="Concave utility functions corner solution explanation - Economics Stack  Exchange" width="500" height="373"></span></p><p><span style="background-color: transparent; color: rgb(0, 0, 0);">The point of tangency in this scenario is the non-optimal choice, as the consumer is able to attain a higher indifference curve if he spends his entire budget on good x1 (i.e., point Z). Note that, there are other kinds of indifference curves which also exhibit this characteristic of utility maximization.</span></p><p><br></p><p><br></p>
------------------------------
Question: What is an income consumption curve?
Answer: <p>An income consumption curve is the loci of all optimal solutions corresponding to different incomes of the consumer. In other words, every point on the income consumption curve (ICC) represents an optimal solution to a particular income.</p><p><br></p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/9530be003bbd16f9ec028105e4c90585.jpeg" alt=""></p><p><br></p><p>With given prices and a given money income as indicated by the budget line P1L1 the consumer is initially in equilibrium at point Q1 on the indifference curve IC1 and is having OM1 of X and ON1 of Y. Now suppose that the income of the consumer increases. With his increased income, he would be able to purchase larger quantities of both the goods.</p><p>As a result, the budget line will shift upward and will be parallel to the original budget line P1L1. Let us assume that the consumer’s money income increases by such an amount that the new budget line is P2L2 (consumer’s income has increased by L1L2 in terms of X or P1P2 in terms of Y). With budget line P2L2, the consumer is in equilibrium at point Q2 on indifference curves IC2 and is buying OM2 of X and ON2 of Y.</p><p>Thus as a result of the increase in his income the consumer buys more quantity of both the goods Since he is on the higher indifference curve IC2 he will be better off than before i.e., his satisfaction will increase. If his income increases further so that the budget line shifts to P3L3, the consumer is in equilibrium at point Q3 on indifference curve IC3 and is having greater quantity of both the goods than at Q2.</p><p>Consequently, his satisfaction further increases. In the above figure, the consumer’s equilibrium is shown at a still further higher level of income and it will be seen that the consumer is in equilibrium at Q4 on indifference curves IC4 when the budget line shifts to P4L4. As the consumer’s income increases, he switches to higher indifference curves and as a consequence enjoys higher levels of satisfaction.</p><p>If now various points Q1, Q2, Q3 and Q4 showing consumer’s equilibrium at various levels of income are joined together, we will get what is called Income Consumption Curve (ICC). Income consumption curve is thus the locus of equilibrium points at various levels of consumer’s income. Income consumption curve traces out the income effect on the quantity consumed of the goods. Income effects can either be positive or negative.</p>
------------------------------
Question: What do we mean by homothetic preferences?
Answer: <p>In consumer theory, a consumer's preferences are called homothetic if they can be represented by a utility function which is homogeneous of degree 1. Suppose that a consumer's preferences depend on the ratio between quantities of good 1 and good 2. That is, if the consumer prefers (x1, x2) to (y1, y2), he will also prefer (tx1, tx2) to (ty1, ty2) for all t&gt;0. Preferences satisfying this property are called homothetic.</p>
------------------------------
Question: What are Giffen goods?
Answer: <p>Giffen good is a special case of an inferior good. A Giffen good is a low income, non-luxury product that defies standard economic and consumer demand theory. Demand for Giffen goods rises when the price rises and falls when the price falls. In econometrics, this results in an upward-sloping demand curve, contrary to the fundamental&nbsp;laws of demand which create a downward sloping demand curve. The term "Giffen goods" was coined in the late 1800s, named after noted Scottish&nbsp;economist, statistician, and journalist Sir Robert Giffen. These goods are commonly essentials with few near-dimensional substitutes at the same price levels. Giffen goods are those, whose demand curve doesn’t conform to “the first rule of demand”, i.e. price and quantity demanded of Giffen goods are inversely related to each other, unlike other goods, where price and quantity demanded are positively related.</p><p><br></p><p><img src="https://qph.fs.quoracdn.net/main-qimg-a449dd3f46424bc1ef748acbee4b9ac9.webp" alt="What are the examples of Giffen goods? - Quora"></p>
------------------------------
Question: Establish a relationship between marginal rate of substitution and price ratio.
Answer: <p>The relationship between the marginal rate of substitution and the price ratio is given as follows.</p><p>MRS_(1,2)= MU1/MU2= p1/p2.</p><p>This is the result of the consumer’s constrained optimization problem i.e choosing a bundle of goods such that the highest possible indifference curve is tangent to the budget constraint. The intuition behind this being the consumer’s optimal choice can be explained as follows. Consider two alternative cases, at point 1, the budget constraint and the indifference curves intersect as a result of which MRS &gt; price ratio. Here, the consumer is willing to give up more of good 2 to get a little more of Good 1.&nbsp;However the budget constraint states that this substitution is not necessary. However at point 3, MRS= price ratio and no such problem arises. Alternatively, MU1/P1= MU2/P2. This is the law of equimarginal utility. This states that the consumer should be getting the same marginal utility per dollar from the consumption of good 1 and good 2</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/eac9afc811122c7a508bb73742bbdce6.png"></p>
------------------------------
Question: What is an Engel curve?
Answer: <p>In microeconomics, an Engel curve describes how household expenditure on a particular good or service varies with household income. They are named after the German statistician Ernst Engel (1821–1896), who was the first to investigate this relationship between goods expenditure and income systematically in 1857. Graphically, the Engel curve is represented in the first quadrant of the Cartesian coordinate system. An Engel curve is a curve that shows the optimum quantity of a commodity purchased at a different level of income. In other words, Engel’s curve indicates how much the quantity of a commodity a consumer will consume at different levels of his income in order to be in equilibrium. It may be derived from the income consumption curve, by taking the optimal quantity of either of the two goods and plotting it against the different income levels of the consumer.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/49ce210934a2c7e0e080f1c629991ead.png"></p>
------------------------------
Question: Is the tangency condition sufficient to determine optimality?
Answer: <p><span style="background-color: transparent; color: rgb(0, 0, 0);">Although the tangency condition is sufficient to determine optimality in almost all general scenarios, there exist special cases where the same does not hold true. For example, when a consumer's indifference curves aren't convex to the origin as is the general case, the tangency condition may not be sufficient to establish optimality in a potential solution. Consider the following special case -</span></p><p><br></p><p><span style="background-color: transparent; color: rgb(0, 0, 0);"><img src="https://lh5.googleusercontent.com/3_Lmb6jZ0M39A_U2STfmf39rJIIE6ctG4A1znQo7jAqYXs7ZVfn56CidSP-8iWQAxhknwlPoS6LjGHzHcnBsr1F_768ZyzFmA-yyQ8oOqbTzB4Zwr1tENxDWT1jZLc2857z3DZHi" width="500" height="276"></span></p><p><span style="background-color: transparent; color: rgb(0, 0, 0);">Here, although both indifference curves (U1 and U2) are tangent to the budget line at points A and B respectively, the optimal solution is clearly point B and not point A. It is interesting to note that while the tangency condition is not sufficient in this case, it is still a necessary condition.</span></p>
------------------------------
Question: What does the theory of revealed preference say?
Answer: <p>Revealed preference theory tries to understand the preferences of a consumer among bundles of goods, given their&nbsp;budget constraint. It is assumed that the consumer’s preferences are stable over the observed time period, i.e. the consumer will not reverse their relative preferences regarding&nbsp;A&nbsp;and&nbsp;B. As a concrete example, if a person chooses (2 apples, 3 bananas) over an affordable alternative (3 apples, 2 bananas), then we say that the first bundle is&nbsp;revealed preferred&nbsp;to the second. It is assumed that the first bundle of goods is always preferred to the second, and that the consumer purchases the second bundle of goods only if the first bundle becomes unaffordable. For example, if a consumer purchases bundle of goods A over bundle B, where both bundles are affordable, it is revealed that the consumer directly prefers A to B.</p>
------------------------------
Question: What is the weak axiom of revealed preferences?
Answer: <p>Weak Axiom of Revealed Preferences (WARP) is a simple consistency condition developed independently by Abraham Wald and Paul Samuelson to show whether a consumer is following the utility maximizing model or not. It is stated as follows -</p><p>Suppose that A and B are two commodity bundles. If A is directly revealed preferred to B, then B cannot be directly revealed preferred to A.</p><p>Algebraically, suppose A = (x1, x2) and B = (y1, y2). Then, it must be so that p1x1 + p2x2 <u>&gt;</u> p1y1 + p2y2, where (p1, p2) are the set of prices of the two goods.</p><p>This essentially implies that the consumer will only select bundle B over bundle A, when bundle A is too expensive and out of his budget constraint.</p>
------------------------------
Question: What is the strong axiom of revealed preferences?
Answer: <p>To ensure consistency of choices when there exists an arbitrary number of commodity bundles, a somewhat more complex condition must be imposed. This is known as the Strong Axiom of Revealed Preferences (SARP). Formally, it is stated as -</p><p><br></p><p>Let A1, A2, ... , An be n commodity bundles, of which at least two are distinct. Also, let us assume that A1 is directly revealed preferred to A2, A2 is directly revealed preferred to A3, and so on. SARP states that as a consequence, An cannot be directly revealed preferred to A1. This holds for n=2, 3,...</p><p><br></p><p>Note that the Weak Axiom of Revealed Preferences (WARP) is a special case of SARP for which n=2.</p>
------------------------------
Question: What is meant by consumer surplus?
Answer: <p><span style="color: rgb(0, 0, 0); background-color: transparent;">Consumer Surplus is an economic measure that depicts consumer satisfaction in terms of the difference between the market price of a good and what consumers are willing and able to pay for it. In a regular demand-supply schedule, the consumer surplus is given by the area under the demand curve that is over the equilibrium price level.</span></p><p><br></p><p><span style="color: rgb(0, 0, 0); background-color: transparent;"><img src="https://lh3.googleusercontent.com/z9mtYfN8ALRcSIgKttL_9Ajvhoe4bofcLnogd1yA4zDBIRzt3A5KgIZtU-n5G-3Yv37Z9Bug4aD8AeHZH8DOlK0v15Eg0UmoblGsp3bOwg5nX9Ep6XkVNDXDRhM5L2jyDNS8ZR_A" height="395" width="396"></span></p>
------------------------------
Question: What is Paasche's Index?
Answer: <p>Paasche Price Index is one of the key ratios in order to determine the velocity of Inflation in the Basket of Goods and Services. It is calculated on a monthly basis to understand the Trend whether it is going in the upward or the downward direction along with the necessary steps or actions to be taken to maintain the same. Like Laspeyres’ index, this too, is a weighted index number however here, we use the current period quantity as a weight. The Paasche price index has a tendency to understate price changes, since it already reflects some of the changes in consumption pattern when consumers respond to price changes. Paasche’s index is easy to compute.</p><p><br></p><p>It is given by the formula -</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/345ed7f09525a0d75a6eb084f3a362b2.png"></p>
------------------------------
Question: What is Laspeyres' Index?
Answer: <p>Laspeyres index, index proposed by German economist Étienne Laspeyres (1834–1913) for measuring current prices or quantities in relation to those of a selected base period. A Laspeyres price index is computed by taking the ratio of the total cost of purchasing a specified group of commodities at current prices to the cost of that same group at base-period prices and multiplying by 100. Laspeyres’ method can be used to compute price indices easily. It presents a meaningful comparison, as changes in the index are attributable to the changes in price. The Laspeyres Price Index is one of the most commonly used price indices in measuring the change in the prices of a&nbsp;basket of goods and service&nbsp;over the years.</p><p>It is stated as -</p><p><img src="https://cdn.corporatefinanceinstitute.com/assets/laspeyres-price-index2-1024x102.png" alt="Laspeyres Price Index - Modified Formula" height="102" width="1024"></p>
------------------------------
Question: What is Fisher's Index?
Answer: <p>Fisher’s index is a computed using the geometric mean of Laspeyre’s index and Paasche’s Index. Fisher’s index is considered to be the most appropriate index to measure changes in prices as it corrects the positive price bias in the Laspeyres’ index and the negative price bias in the Paasche’s index.</p><p>It is given as -</p><p><img src="http://4.bp.blogspot.com/_DJEIRrK4tl4/SPxA4qBzTvI/AAAAAAAAAuU/xhflXxcjWoQ/w1200-h630-p-k-no-nu/f.bmp" alt="Mathematics Education: Why Fisher Index number is So Ideal Index Number"></p>
------------------------------
Question: What are the different tests for adequacy of index numbers?
Answer: <p>The three common tests to check for adequacy of index numbers are as follows -</p><ol><li><strong>Unit Test </strong>- This test states that the formula for constructing an index number should be independent of the units in which prices and quantities are expressed. All methods, except simple aggregative method, satisfy this test. Except for unweighted aggregative index number, all other indices satisfy this test.</li><li><strong>Time Reversal Test</strong> - This test guides whether the method works both ways in time forward and backward. Ideally, the formula for calculating an index number should be such that it gives the same ratio between one point of time and the other, no matter which of the two time is taken as the base. In other words, when the data for any two years are treated by the same method, but with the base reversed, the two index numbers should be reciprocals of each other. Fisher's method satisfies this test.</li><li><strong>Factor Reversal Test - </strong>It says that the product of a price index and the quantity index should be equal to value index. The test says that the change in price multiplied by change in quantity should be equal to total change in value. Only Fisher's method passes this test.</li></ol>
------------------------------
Question: What is the consumer price index (CPI)?
Answer: <p>The CPI is the Consumer Price Index and is a metric used to measure inflation. Consumer price index numbers are designed to show changes in the price level of specified basket of goods and services purchased by households. CPI is also known as retail price index number. The need for constructing consumer price index numbers arises because the general price index numbers do not give the exact idea about the repercussions of price change on the cost of living of different categories of consumers. In countries like India, CPI is constructed for classes like agricultural laborers, industrial workers and urban employees. CPI is a measure of the purchasing power of money and is used a proxy for macroeconomic variables like inflation since CPI can be computed easily. It is used to measure real wages and has important policy implications because governments use CPI to assess the impact of price changes on the general public. The efficacy of public policies that the government depend on the CPI to a considerable extent.</p>
------------------------------
Question: What is the wholesale price index (WPI)?
Answer: <p>Wholesale Price Index measures the relative changes in the prices of goods in the wholesale market. It shows the prices of the representative basket of goods traded in the wholesale prices of a few selected commodities. It takes into account of the price movement of each commodity individually. Based on the price changes of individual commodity, the WPI is calculated through the averaging principle using Laspeyres Index. The Laspeyres price index is an index formula used in price statistics for measuring the price development of the basket of goods and services consumed in the base period. WPI is the weighted arithmetic mean based on the fixed value based weights for the base period. The current WPI in the USA has 2011-12 as the base year.</p>
------------------------------
Question: Which index number is the most ideal index number?
Answer: <p>Fisher’s index number is commonly known as the ideal index number. It is the geometric mean of the Laspeyres' and Paasche's index numbers. Fisher's index is ideal for a number of reasons -</p><p><br></p><ol><li><strong>Use of Variable Weights:</strong> In this formula variable weights are used, i.e., different weights are used in base year and current year on the basis of quantities in these years. It is practical also because quantity of consumption may not remain the same every year.</li><li><strong>Based on Geometric Mean: </strong>Fisher’s index is based on geometric mean which is supposed to be the best average for the construction of index numbers.</li><li><strong>Satisfaction of Reversal Tests:</strong> It satisfies the time-reversal as well as the factor reversal tests.</li></ol>
------------------------------
Question: What is Marshall-Edgeworth Index?
Answer: <p>If the weights are taken as the arithmetic mean of base and current year quantities, then the weighted aggregative index is called Marshall-Edgeworth index. Like Fisher’s index, Marshall-Edgeworth index also requires too much labor in selection of commodities. In some cases the usage of this index is not suitable, for example the comparison of the price level of a large country to a small country. Marshall-Edgeworth index can be calculated by using the formula given below -</p><p><br></p><p><img src="//:0"><img src="//:0"><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/f5c5073f4e49372591419058e7227699.jpeg"></p><p>It is simple to understand, and easy to calculate.</p><p>It gives a result which is approximately close to the results obtained by the Fisher’s ideal formula, and lies between the results of Laspeyres and Paasche’s indices.</p><p>It satisfies the time reversal test of consistency as the weights remain constant under this method.</p><p>It also satisfies the unit test of consistency.</p>
------------------------------
Question: What is a unit test in economical statistics?
Answer: <p>Unit test in economics, and particularly in index number computing, is a method where a particular index is tested against the presence of a unit. Ideally, an index number should be devoid of any units (such as, the units of prices and quantities). This is because indices are particularly used to represent the level of inflation in an economy, and should not necessarily pertain to a singular variable (such as price or quantity). Most commonly used indices pass the unit test, barring the unweighted aggregative index number method.</p>
------------------------------
Question: What is the time reversal test?
Answer: <p>Time reversal test is a test to determine whether a given method will work both ways in time, forward and backward. Where P<sub>01</sub> is the index for time “1” on time “0” as base and P<sub>10</sub> is the index for time “0” on time “1” as base. For an index number to pass the time reversal test, the product of the two must be unity. If the product is not unity, there is said to be a time bias in the method. If the index involves a multiplying factor (of say, 100) then it must not be considered during the test. The time reversal test is satisfied by several indices, including the Fisher index, the Marshall-Edgeworth method, Kelly's method, etc.</p>
------------------------------
Question: What is the factor reversal test?
Answer: <p>Factor reversal test is one of the most common tests for indices. It holds that the product of a price index and the quantity index should be equal to the corresponding value index. In the words of Dr. Irving Fisher, “Just as each formula should permit the interchange the prices and quantities without giving inconsistent results, so it ought to permit the interchange of the two times without giving inconsistent results, i.e., the two results multiplied together should give the true value ratio.” In other words, the test is that the change in price multiplied by the change in quantity should be equal to the total change in value. Since the factor reversal test is satisfied by the Fisher’s Ideal index and this means, of course, that the formula serves equally well for constructing indices of quantities as for constructing indices of prices, the quantity index being derived by interchanging p and q in the ideal formula.</p>
------------------------------
Question: What is the circular test in economics?
Answer: <p>Circular test is one of the tests for adequacy of index numbers. It is desirable to change the base year in indices if measurement of price changes over a period of years is required. A test of this shift ability of base is called to the circular test.&nbsp;This test is just an extension of the time reversal test. According to this, if indices are constructed for year one based on year zero, for year two based on year one and for year zero based on year two, the product of all the indices should be equal to 1. Kelly's method passes this test.</p>
------------------------------
Question: How is the market demand curve made up?
Answer: <p>The market demand curve is the summation of all the individual demand curves in a given market. The market demand curve gives the quantity demanded by everyone in the market for every price point. It can also be provided as a schedule, which is in table format. The market demand curve is a visualization of demand based on product pricing. On the x-axis, you have the number of times the product has been purchased in a given time period at that price point. Consider a market which consists of two consumers - Jack and Jill. The market demand curve is simply the aggregate of their demands, or in other words, the summation of their demand at every price point.<img src="https://study.com/cimages/multimages/16/market_demand_graph.png" alt="The Market Demand Curve: Definition, Equation &amp;amp; Examples - Video &amp;amp; Lesson  Transcript | Study.com"></p>
------------------------------
Question: What is elasticity in economics?
Answer: <p>Elasticity&nbsp;is a measure of a variable's sensitivity to a change in another variable. Most commonly this sensitivity is the change in quantity demanded relative to changes in other factors, such as price. It is predominantly used to assess the change in consumer demand as a result of a change in a good or service's price. When a product is elastic, a change in price quickly results in a change in the quantity demanded. When a good is inelastic, there is little change in&nbsp;the quantity of demand even with the change of the good's price.</p>
------------------------------
Question: When is the demand curve a straight line parallel to the X-axis?
Answer: <p>Assuming the typical demand-supply schedule, where quantity is represented on the X-axis and price on the Y-axis, a straight line demand curve parallel to the X-axis implies that there exists only one price at which there is a demand for that particular commodity in the market. In other words, the price of the commodity is fixed, and the seller can sell any amount of the product at that given price. If there is any change in the price, the consumers modify their demand instantaneously - likely to another commodity (a substitute).</p><p><br></p><p>Demand is said to be perfectly elastic in this scenario.</p><p><img src="https://www.vdube.com/custom/article_images/2021/02/image_2.jpg" alt="What are different types of price elasticity of demand? OR Explain  price-quantity elasticity of product OR Explain elasticity of demand and  distin.. | Vdube - Upload Video, Watch Online, Go Live, Share,"></p>
------------------------------
Question: When is the demand curve a straight line parallel to the Y-axis?
Answer: <p>Assuming the typical demand-supply schedule, where quantity is represented on the X-axis and price on the Y-axis, a straight line demand curve parallel to the Y-axis implies that the same quantity of the commodity is demanded in the market, no matter what price is charged. Although this is a hypothetical situation, some absolute necessities exhibit similar characteristics. This is a case of perfectly inelastic demand.</p><p><br></p><p><img src="https://haygot.s3.amazonaws.com/questions/1087568_775635_ans_f6af002e02914b25883051d67772999d.jpg" alt="Perfectly inelastic demand curve is ."></p>
------------------------------
Question: What are the determinants of elasticity of demand?
Answer: <p>It is important for sellers to have an idea about the buyers' elasticity of demand in order to make sensible pricing decisions. They can usually get an idea of the elasticity of demand by gathering information regarding some of the common determinants of elasticity. They are -</p><p><br></p><ol><li>Availability of substitutes of the product in question</li><li>Importance of the product in consumers' purchase decisions</li><li>Number of times the product can be used on average</li></ol><p><br></p>
------------------------------
Question: What is cross-elasticity of demand?
Answer: <p>Cross-elasticity of demand is a measure of how much the quantity demanded of one good responds to a change in the price of another good, calculated as the percentage change in quantity demanded of the first good divided by the percentage change in the price of the second good. It can take positive or negative values, depending on whether both goods are substitutes (positive value) or complements (negative value). Symbolically, it is given as </p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/77acb098c38d7b35af05226071f5c258.png"></p><p>If the two goods are substitutes, then the rise in price of one means that the other will be relatively cheaper to the consumers, thereby increasing the demand of its quantity. If the two goods are complements, then the rise in price of one means that consumers will be able to afford less of that good, and consequently less of the other good as well.</p>
------------------------------
Question: What is income elasticity of demand?
Answer: <p>Income elasticity of demand refers to the sensitivity of the quantity demanded for a certain good to a change in real income of consumers who buy this good, keeping all other things constant. With income elasticity of demand, you can tell if a particular good represents a necessity or a luxury. The idea is to measure the impact that an increase or decrease in income will have on the buying habits of consumers and this type of assessment is very important to companies that produce goods and services that are not considered necessities, and often is considered when setting prices for those products. Symbolically, it is given as -</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/28339b438548f9a70091348120166d7f.png"></p>
------------------------------
Question: What is the income and price elasticity of demand of a Giffen good?
Answer: <p>A Giffen good is a special kind of an inferior good, the demand for which rises even when its prices rises. It happens usually due to a lack of an immediate substitute, or when the prices of the substitutes rise by even greater margins. Clearly, the demand for a Giffen good rises as its price rises. Hence, its price elasticity of demand is positive. However, as the income of a consumer increases, he is able to afford better alternatives, so his demand for inferior goods as a whole decreases. So, the income elasticity of demand of Giffen goods is negative.</p>
------------------------------
Question: What is a duplex connection and what are the different types of duplex connections?
Answer: <p>A duplex communication system is a point-to-point system composed of two or more connected parties or devices that can communicate with one another in both directions. There is a technical distinction between full-duplex communication, using a single physical communication channel for both directions simultaneously and dual-simplex communication that uses two distinct channels, one for each direction. From the user perspective, the technical difference doesn't matter and both variants are commonly referred to as full duplex. </p><p>Many Ethernet connections achieve full-duplex operation by making simultaneous use of two physical twisted pairs inside the same jacket, or two optical fibers which are directly connected to each networked device: one pair or fiber is for receiving packets, while the other is for sending packets. Other Ethernet variants, such as 1000BASE-T use the same channels in each direction simultaneously. In any case, with full-duplex operation, the cable itself becomes a collision-free environment and doubles the maximum total transmission capacity supported by each Ethernet connection.</p>
------------------------------
Question: What is Auto-MDIX
Answer: <p>Auto-MDI/MDIX feature allows a switch port to automatically detect what type of port is connected on other end (MDI port or MDIX port) and swap the transmission and receive pins. Thus, the transmission pins on one side may connect with the receive pins on other side.</p><p>:Auto-MDIX is enabled on the latest Cisco switches and allows the switch to detect and use whatever type of cable is attached to a specific port.?? <span style="color: rgb(88, 88, 91);">To examine the auto-MDIX setting for a specific interface, use the&nbsp;show controllers ethernet-controller&nbsp;command with the&nbsp;phykeyword. To limit the output to lines referencing auto-MDIX, use the&nbsp;include MDIX&nbsp;filter..</span></p>
------------------------------
Question: What is a production function in economics?
Answer: Production function is a mathematical method that describes the input-output relationship. Input consists of the four factors of production – land, capital, labor and entrepreneurship. It states the amount of product that can be obtained from every combination of factors, assuming that the most efficient available methods of production are used. It is a mathematical function that relates the maximum amount of output that can be obtained from a given number of inputs – generally capital and labor. The production function, therefore, describes a boundary or frontier representing the limit of output obtainable from each feasible combination of inputs.
------------------------------
Question: What is the difference between a short run and a long run production function?
Answer: <p>The two definitions of the short run and the long run are really just two ways of saying the same thing since a firm doesn't incur any fixed costs until it chooses a quantity of capital (i.e. scale of production) and a production process. Economists differentiate between the short run and the long run with regard to market dynamics as follows: </p><p><br></p><p>Short run: The number of firms in an industry is fixed (even though firms can "shut down" and produce a quantity of zero). </p><p><br></p><p>Long run: The number of firms in an industry is variable since firms can enter and exit the marketplace. </p><p><br></p><p>• Short run refers to a period of time in which the quantity of at least one input will be fixed, and quantities of other inputs used in the production of goods and services may be varied. </p><p><br></p><p>• The long run allows firms to increase/decrease the input of land, capital, labor, and entrepreneurship thereby changing levels of production in response to expected losses of profits in the future.</p>
------------------------------
Question: What is SSH?
Answer: <p>SSH, also known as Secure Shell or Secure Socket Shell, is a&nbsp;network protocol&nbsp;that gives users, particularly system administrators, a secure(encrypted) way to access a computer over an unsecured network. SSH uses TCP port 22. In addition to providing secure network services, SSH refers to the suite of utilities that implement the SSH protocol. In addition to providing strong encryption, SSH is widely used by network administrators for managing systems and applications remotely, enabling them to log in to another computer over a network, execute commands and move files from one computer to another. U<span style="color: rgb(88, 88, 91);">nlike Telnet, with SSH the username and password are encrypted. </span>The protocol was developed to replace old insecure protocols, increasing security and privacy across all operating systems.</p>
------------------------------
Question: What is average product of labour and capital?
Answer: <p>The average product of labor gives a general measure of output per worker, and it is calculated by dividing total output (q) by the number of workers used to produce that output (L). Similarly, the average product of capital gives a general measure of output per unit of capital and is calculated by dividing total output (q) by the amount of capital used to produce that output (K). Average product of labor and average product of capital can be thought of as measures of labor and capital productivity, respectively.</p>
------------------------------
Question: What is marginal product of labour and capital?
Answer: <p>Marginal Product of capital refers to the change in the output produced by the firm when an additional unit of the capital is employed while the other inputs are constant and it plays an important role for the management of the firm because the decision with respect to the different investments in the firm are taken after comparing the marginal product of the capital arrived with the respective cost of capital. Similarly, marginal product of labor refers to the change in output produced by the firm when an additional unit of labor is employed, keeping everything else constant. It is important because the firm's hiring decision depends on it.</p>
------------------------------
Question: What is a router?
Answer: <p>The router is the piece of network hardware that allows communication between your local home network—like your personal computers and other connected devices—and the internet. Enabling the highest level of security on the router turns on things like the firewall, and is the best way to keep your computer system and information safe from attack. Routers perform the traffic directing functions on the Internet. Data sent through the internet, such as a web page or email, is in the form of data packets. A packet is typically forwarded from one router to another router through the networks that constitute an internetwork (e.g. the Internet) until it reaches its destination node.</p>
------------------------------
Question: What is the relation between the marginal product curve and the average product curve?
Answer: <p>The marginal product of a factor, say, labor, and the average product of the same, when plotted on a typical co-ordinate plane with the X-axis measuring units of the factor and the Y-axis measuring output produced, exhibit some notable characteristics. They are as follows -</p><p><br></p><ol><li>As long as average product is rising, marginal product curve lies over average product curve.</li><li>As long as the average product is falling, the marginal product curve lies below the average product curve.</li><li>Marginal product equals average product at the maxima of the average product curve.</li></ol>
------------------------------
Question: What is the law of variable proportion?
Answer: <p>The Law of Variable Proportion states that as the quantity of a factor is increased while keeping other factors constant, the Total Product (TP) first rises at an incremental rate, then at a decremental rate and lastly the total production begins to fall. The law of variable proportion is one of the most important laws in economics. Economists like Alfred Marshall, Benham, and Samulson contributed most to this law and this law is based on the short run production function. It is referred to as the law which states that when the quantity of one factor of production is increased, while keeping all other factors constant, it will result in the decline of the marginal product of that factor. Law of variable proportion is also known as the Law of Proportionality. When the variable factor becomes more, it can lead to negative value of the marginal product.</p>
------------------------------
Question: What is a network switch?
Answer: <p>A switch is a piece of networking hardware that connects devices on a network by using packet switching to receive and forward data. Multiple ethernet cables are connect to ports on a switch to enable communication between different network devices including routers and PCs. Switches help to manage data flow across the network by transmitting packets to their intended devices. The switch is able to differentiate between all the devices due to its IP address. Ethernet switches operate at layer 2 of the OSI model. Switches are mostly used in local area networks or small offices. There are also multiple types of switches including unmanaged switches, managed switches, smart switches, and enterprise managed switches.</p>
------------------------------
Question: What are all the Frame Forwarding methods?
Answer: <p>One frame forwarding method is store and forward switching. In this switching technique, when the switch receives the frame, it stores the frame data in buffers until the full frame has been received. The switch takes the destination MAC address from switching table, determines the outgoing interface port, and forwards the frame onto its destination through the designated switch port and this switching technique does not involve any error check process by the switch. </p><p>Another frame forwarding method is cut-through switching. There are 2 types of cut-through switching: fast-forward switching and fragment free switching. In fast-forward switching, the lowest level of latency (it is measured from the first bit received to the first bit transmitted) because it immediately forwards a packet after reading the destination address. Fast-forward switching starts forwarding as soon as it received the first byte of the packet, there may be a chance when packets are relayed with errors and this occurs rarely, and during this situation destination network adapter discards the faulty packet upon receipt and this switching is the typical cut-through method of switching. In fragment-free switching there is a trade-off between the high latency – high integrity of store and forward switching and the low latency – reduced integrity of fast-forward switching.</p>
------------------------------
Question: What are switching domains?
Answer: The terms collisions and congestion are used here in the same way that you use them in street traffic. The network segments that share the same bandwidth between devices are known as collision domains. In the case of half-duplex, the switch port will be part of a collision domain. To ensure a smooth process when you switch domain names, it’s sensible to make a backup of your website, in case anything should happen. and this means that you can download your backups locally, which means that you’ll be ready when it comes to switching domain names.
------------------------------
Question: What is a VLAN?
Answer: A VLAN (virtual LAN) is a subnetwork which can group together collections of devices on separate physical local area networks (LANs). A LAN is a group of computers and devices that share a communications line or wireless link to a server within the same geographical area. VLANs make it easy for network administrators to partition a single switched network to match the functional and security requirements of their systems without having to run new cables or make major changes in their current network infrastructure. VLANs are often set up by larger businesses to re-partition devices for better traffic management.
------------------------------
Question: What are the types of VLANs?
Answer: <p>There are 5 main types of VLANs: the Default VLAN, the Data VLAN, the Native VLAN, the Management VLAN, and the Voice VLAN. </p><p>The default VLAN is the VLAN that is on the switch during its initial setup. When the switch initially starts up, all switch ports become a member of the default VLAN (generally all switches have default VLAN named as VLAN 1), which makes them all part of the same broadcast domain. Using default VLAN allows any network device connected to any of the switch port to connect with other devices on other switch ports. It is not used for carrying management traffic or voice. </p><p>Data VLANs <span style="color: rgb(88, 88, 91);">are VLANs configured to separate user-generated traffic. They are referred to as user VLANs because they separate the network into groups of users or devices. A modern network would have many data VLANs depending on organizational requirements. </span></p><p><span style="color: rgb(88, 88, 91);">Native VLANs on a switch are used to distinguish default VLANs from VLANs that have been used.</span></p><p><span style="color: rgb(88, 88, 91);"> Management VLANs are a data VLAN configured specifically for network management traffic including SSH, Telnet, HTTPS, HTTP, and SNMP. By default, VLAN 1 is configured as the management VLAN on a Layer 2 switch. </span>A best practice is to set up a separate VLAN for management traffic like monitoring, system logging, SNMP, and other potentially sensitive management tasks. In addition to the security benefits, this ensures that bandwidth for management will be available even when user traffic is high. </p><p>Finally, there is the Voice VLAN, which supports Voice over IP (VoIP). Voice VLANs are mostly given high transmission priority over other types of network traffic. To ensure voice over IP (VoIP) quality (delay of less than 150 milliseconds (ms) across the network), we must have separate voice VLAN as this will preserve bandwidth for other applications.</p>
------------------------------
Question: What is DTP?
Answer: <p>The Dynamic Trunking Protocol (DTP) is a proprietary networking protocol developed by Cisco Systems for the purpose of negotiating trunking on a link between two VLAN-aware switches, and for negotiating the type of trunking encapsulation to be used. It works on Layer 2 of the OSI model. VLAN trunks formed using DTP may utilize either IEEE 802.1Q or Cisco ISL trunking protocols. <span style="color: rgb(88, 88, 91);">DTP manages trunk negotiation only if the port on the neighbor switch is configured in a trunk mode that supports DTP. Switches from other vendors do not support DTP. </span>The default DTP configuration for Cisco Catalyst 2960 and 3650 switches is dynamic auto. <span style="color: rgb(88, 88, 91);">To enable trunking from a Cisco switch to a device that does not support DTP, use the&nbsp;switchport mode trunk&nbsp;and&nbsp;switchport nonegotiate&nbsp;interface configuration mode commands. This causes the interface to become a trunk, but it will not generate DTP frames.</span></p><p><br></p>
------------------------------
Question: What is Inter-VLAN Routing?
Answer: <p><span style="color: rgb(88, 88, 91);">Inter-VLAN routing is the process of forwarding network traffic from one VLAN to another VLAN. It is used because VLANs are used to segment switched Layer 2 networks for a variety of reasons. Regardless of the reason, hosts in one VLAN cannot communicate with hosts in another VLAN unless there is a router or a Layer 3 switch to provide routing services. There are three types of inter-VLAN routing options that are all used in different situations. Legacy Inter-VLAN routing is a legacy solution and does not scale well.  Router-on-a-Stick is used for small to medium sized networks. Layer 3 switch using switched virtual interfaces (SVIs) is the most scalable solution and is used for medium to large networks.</span></p>
------------------------------
Question: What is Spanning Tree Protocol?
Answer: <p>Spanning Tree Protocol (STP) is based on an algorithm, which was developed by Radia Perlman at DEC (Digital Equipment Corporation, now part of HP). <span style="color: rgb(88, 88, 91);">Her spanning tree algorithm (STA) creates a loop-free topology by selecting a single root bridge where all other switches determine a single least-cost path. </span>The Spanning Tree Protocol (STP) was then standardized by IEEE as IEEE 802.1D. Spanning Tree Protocol (STP) is a Layer 2 network protocol used to prevent looping within a network topology. <span style="color: rgb(88, 88, 91);">Without the loop prevention protocol, loops would occur rendering a redundant switch network inoperable. </span>STP was created to avoid the problems that arise when computers exchange data on a local area network (LAN) that contains redundant paths. If the flow of traffic is not carefully monitored and controlled, the data can be caught in a loop that circles around network segments, affecting performance and bringing traffic to a near halt.</p>
------------------------------
Question: What are the different version of Spanning Tree Protocol?
Answer: <p>A network can contain switches running different spanning tree versions. There are seven main versions of the spanning tree protocol that is used. The STP is <span style="color: rgb(88, 88, 91);">the original IEEE 802.1D version (802.1D-1998 and earlier) that provides a loop-free topology in a network with redundant links. Also called Common Spanning Tree (CST), it assumes one spanning tree instance for the entire bridged network, regardless of the number of VLANs. The er-VLAN Spanning Tree (PVST+) is a Cisco enhancement of STP that provides a separate 802.1D spanning tree instance for each VLAN configured in the network. PVST+ supports PortFast, UplinkFast, BackboneFast, BPDU guard, BPDU filter, root guard, and loop guard. The Rapid Spanning Tree Protocol (RSTP) or IEEE 802.1w is an evolution of STP that provides faster convergence than STP. The 802.1D-2004 is an updated version of the STP standard, incorporating IEEE 802.1w. The Rapid PVST+ is a Cisco enhancement of RSTP that uses PVST+ and provides a separate instance of 802.1w per VLAN. Each separate instance supports PortFast, BPDU guard, BPDU filter, root guard, and loop guard. The Multiple Spanning Tree Protocol (MSTP) is an IEEE standard inspired by the earlier Cisco proprietary Multiple Instance STP (MISTP) implementation. MSTP maps multiple VLANs into the same spanning tree instance. Finally, the Multiple Spanning Tree (MST) is the Cisco implementation of MSTP, which provides up to 16 instances of RSTP and combines many VLANs with the same physical and logical topology into a common RSTP instance. Each instance supports PortFast, BPDU guard, BPDU filter, root guard, and loop guard.</span></p>
------------------------------
Question: What is an EtherChannel?
Answer: <p>EtherChannel is a port link aggregation technology or port-channel architecture used primarily on Cisco switches. It allows grouping of several physical Ethernet links to create one logical Ethernet link for the purpose of providing fault-tolerance and high-speed links between switches, routers and servers. All member ports within the bundle must have the same physical settings such as port type, speed and duplex. Traffic is load-shared across the ports. It is important to note that traffic is not  load balanced, as load distribution is not always equals across all member ports. EtherChannel links formed when two or more links bundled together for the purposes of aggregating available bandwidth and providing a measure of physical redundancy.</p>
------------------------------
Question: What is DHCP?
Answer: Dynamic Host Configuration Protocol (DHCP) is a client/server protocol that automatically provides an Internet Protocol (IP) host with its IP address and other related configuration information such as the subnet mask and default gateway. RFCs 2131 and 2132 define DHCP as an Internet Engineering Task Force (IETF) standard based on Bootstrap Protocol (BOOTP), a protocol with which DHCP shares many implementation details. It's also used to configure the subnet mask, default gateway, and DNS server information on the device.
------------------------------
Question: What is SLAAC?
Answer: <p>SLAAC (Stateless Address Autoconfiguration) is a method in which the host or router interface is assigned&nbsp;a 64-bit prefix, and then the last 64 bits of its address are derived by the&nbsp;host or router with help of EUI-64 process which is described in next few lines. SLAAC uses NDP protocol to work. <span style="color: rgb(88, 88, 91);">The SLAAC method enables hosts to create their own unique IPv6 global unicast address without the services of a DHCPv6 server. SLAAC is a stateless service. This means there is no server that maintains network address information to know which IPv6 addresses are being used and which ones are available. </span>SLAAC uses ICMPv6 RA messages to provide addressing and other configuration information that would normally be provided by a DHCP server. A host configures its IPv6 address based on the information that is sent in the RA. RA messages are sent by an IPv6 router every 200 seconds. A host can also send a Router Solicitation (RS) message requesting that an IPv6-enabled router send the host an RA. SLAAC can be deployed as SLAAC only, or SLAAC with DHCPv6.</p>
------------------------------
Question: What is FHRP?
Answer: <p>A first hop redundancy protocol (FHRP) is a computer networking protocol which is designed to protect the default gateway used on a subnetwork by allowing two or more routers to provide backup for that address; in the event of failure of an active router, the backup router will take over the address, usually within a few seconds. In practice, such protocols can also be used to protect other services operating on a single IP address, not just routers. FHRP is used to prevent network failure at a default gateway and this is achieved by configuring multiple routers with the same IP address and Mac address, thus presenting an illusion of a single virtual router to the hosts in a Local Area Network (LAN). The IP address of the virtual router is configured on all hosts in that network or subnet as their default gateway.</p>
------------------------------
Question: What is HSRP?
Answer: <p>Hot Standby Router Protocol (HSRP) is a proprietary CISCO protocol, which has 2-versions :version 1 : The messages are multicast at 224.0.0.2 and uses the UDP port 1985 and this version allows group number range from 0 to 255.version 2 : The messages are multicast at 224.0.0.102 and uses the UDP port 1985.All the routers in a single HSRP group shares a single MAC address and IP address, which acts a default gateway to the local network. If it fails, the Standby router takes up all the responsibilities of the active router and forwards the traffic. Some important terms related to HSRP :Virtual IP : IP address from local subnet is assigned as default gateway to all local hosts in the network. Virtual MAC address : MAC address is generated automatically by HSRP. e.g.- if the group number is 10 then the last 8 bits will be 0a. As soon as the hold down time is finished, the standby router will become the active router and takes up all the responsibilities of active router and this is known as preempt. If in case the original active router comes back then we can decrease the priority of the standby router so that it will become the standby router again. </p>
------------------------------
Question: What is a DDoS Attack?
Answer: A distributed denial-of-service attack (DDoS attack) sees an attacker flooding the network or servers of the victim with a wave of internet traffic so big that their infrastructure is overwhelmed by the number of requests for access, slowing down services or taking them fully offline and preventing legitimate users from accessing the service at all. While a DDoS attack is one of the least sophisticated categories of cyberattack, it also has the potential to be one of the most disruptive and most powerful by taking websites and digital services offline for significant periods of time that can range from seconds to even weeks at a time. The onslaught of malicious connection requests places legitimate visitors at the back of an undiminishing traffic queue which prevents the website from loading. Targets don't just include web servers, a DDoS attack can disrupt any service connected to the internet such as networks, databases, mobile devices, and even specific application functions.
------------------------------
Question: What is a Data Breach?
Answer: Frequently conducted via the Internet or network connection, data breaches usually revolve around the pursuit of logical or digital data. A data breach occurs when there is an unauthorized entry point into a corporation’s databased that allows cyber hackers to access customer data such as passwords, credit card numbers, Social Security numbers, banking information, driver’s license numbers, medical records, and other sensitive information. The purpose of hacking these systems is to use this information for identity theft and fraud purposes and this can be done physically by accessing a computer or network to seal local files or by bypassing network security remotely.
------------------------------
Question: What are some network security devices?
Answer: <p>Network infrastructure devices are often easy targets for attackers. Manufacturers build and distribute these network devices with exploitable services, which are enabled for ease of installation, operation, and maintenance. Owners and operators of network devices often do not change vendor default settings, harden them for operations, or perform regular patching. Internet service providers may not replace equipment on a customer’s property once the equipment is no longer supported by the manufacturer or vendor. Owners and operators often overlook network devices when they investigate, look for intruders, and restore general-purpose hosts after cyber intrusions. Many of these devices are not maintained at the same security level as general-purpose desktops and servers, but there are steps users and network administrators can take to better secure their network infrastructure. Some of these more secure devices include VPN-Enabled Routers, NGFWs, and NACs. VPN-Enabled Routers <span style="color: rgb(88, 88, 91);">provide a secure connection to remote users across a public network and into the enterprise network. VPN services can be integrated into the firewall. NGFWs provide stateful packet inspection, application visibility and control, a next-generation intrusion prevention system (NGIPS), advanced malware protection (AMP), and URL filtering. NAC devices&nbsp;include authentication, authorization, and accounting (AAA) services. In larger enterprises, these services might be incorporated into an appliance that can manage access policies across a wide variety of users and device types. The Cisco Identity Services Engine (ISE) is an example of a NAC device.</span></p>
------------------------------
Question: What is a WLAN?
Answer: A wireless LAN (WLAN) is a wireless computer network that links two or more devices using wireless communication to form a local area network (LAN) within a limited area such as a home, school, computer laboratory, campus, or office building and this gives users the ability to move around within the area and remain connected to the network. Through a gateway, a WLAN can also provide a connection to the wider Internet. WLANs use high-frequency radio waves and often include an access point to the Internet.
------------------------------
Question: What does an IP routing table do?
Answer: <p>Routing Table provides the device with instructions for sending the packet to the next hop on its route across the network. Each entry in the routing table consists of the following entries Network ID (The network ID or destination corresponding to the route), Subnet Mask (The mask that is used to match a destination IP address to the network ID), Next Hop (The IP address to which the packet is forwarded), Outgoing Interface (Outgoing interface the packet should go out to reach the destination network), and Metric (A common use of the metric is to indicate the minimum number of hops (routers crossed) to the network ID). Routing table entries can be used to store the following types of routes: Directly Attached Network IDs, Remote Network IDs, Host Routes, Default Route, and Destination. When a router receives a packet, it examines the destination IP address, and looks up its routing table to figure out which interface packet will be sent out.</p>
------------------------------
Question: What are static routes?
Answer: <p>A default static route is a route that will match all packets. A common use is when connecting a company’s edge router to the ISP network. There may be many limitations to static routing, but there are various incidents where a static route is the most logical &amp; efficient methods for the path. It is the opposite of dynamic routing. Dynamic routing is a system in which routers will automatically adjust to the changes in network traffic. It is considered the purest form of routing &amp; requires extensive manual processes. There are 4 types of static routes: the standard static route, the default static route, the floating static route, and the summary static route.</p>
------------------------------
Question: What is an isoquant?
Answer: <p>An isoquant in economics is a curve that, when plotted on a graph, shows all the combinations of two factors that produce a given output. Often used in manufacturing, with capital and labor as the two factors, isoquants can show the optimal combination of inputs that will produce the maximum output at minimum cost. An isoquant is a firm’s counterpart of the consumer’s indifference curve. ‘Iso’ means equal and ‘quant’ means quantity and therefore, an isoquant represents a constant quantity of output. The isoquant curve is also known as an “Equal Product Curve” or “Production Indifference Curve” or Iso-Product Curve.”</p><p><br></p><p><img src="https://images.saymedia-content.com/.image/t_share/MTc0MjcwMjk3NjExMTE4NDYw/isoquant-meaning-and-properties.png" alt="Isoquant - Meaning and Properties - Owlcation"></p>
------------------------------
Question: What is an isoquant map?
Answer: <p>An isoquant in economics is a curve that, when plotted on a graph, shows all the combinations of two factors that produce a given output. Often used in manufacturing, with capital and labor as the two factors, isoquants can show the optimal combination of inputs that will produce the maximum output at minimum cost. An isoquant map is a set of isoquants that shows the maximum attainable output from any given combination inputs. Like indifference curves, the higher the isoquant, the greater the output it corresponds to.</p><p><br></p><p><img src="https://www.economicsdiscussion.net/wp-content/uploads/2015/12/clip_image006_thumb21.jpg" alt="Study Notes on Isoquants ( With Diagram)"></p>
------------------------------
Question: What is marginal rate of technical substitution?
Answer: The marginal rate of technical substitution is the amount of capital that the manufacturer should refuse to use with an increase in labor costs per unit when old and new resources provide the same output. The marginal rate of technical substitution (MRTS) shows the ratio of a change in one production factor to a change in another factor subject to low production. MRTS= ?K/ ?L is a formula, where ?K is a capital change, ?L is a labor change. The proportion shows the marginal rate of technical substitution of capital by labor, provided that the volume of production remains unchanged, that is, the production level is located on the same isoquant. Calculating this ratio is easiest to accomplish by plotting the input amounts on an X-Y graph, in order to visually represent the shifting rate across a number of potential input combinations. It is not one fixed value and requires recalculation for each shift up or down on the variable continuum.
------------------------------
Question: What happens when marginal rate of technical substitution is constant?
Answer: <p>When the marginal rate of technical substitution is a constant figure, the isoquants are downward sloping straight lines. It may be represented by the production function</p><p>X = f(K, L) = aK + bL</p><p>where K might be capital and L might be labor.</p><p>Clearly, the absolute MRTS in this case is b/a.</p><p>This type of a production function is rarely encountered in real life, and is usually meant to be considered hypothetically. The optimal solution for the firm in such scenarios is to invest solely in either of the two factors. It is interesting to note that the two factors are perfect substitutes in this case.</p><p><br></p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/b/ba/Isoquant_perfectsubs.png/250px-Isoquant_perfectsubs.png" alt="Isoquant - Wikipedia"></p>
------------------------------
Question: What does the isoquant look like when the two factors of production are used in a fixed proportion?
Answer: <p>When the two inputs of production are always used in a fixed proportion, the production function may be given as -</p><p>X = f(K, L) = min(aK, bL)</p><p>That is, increment in either of the two factors without a proportional increase in the other factor will lead to no change in the output produced by the firm. The two factors must always be used in a specified ratio. The isoquant in this case resembles the letter L, and the production function is known as the Leontief production function.</p><p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/34/Isoquant_perfect_compliments.png/250px-Isoquant_perfect_compliments.png" alt="Isoquant - Wikipedia"></p>
------------------------------
Question: What are returns to scale?
Answer: <p>Returns to scale, in economics, is the quantitative change in output of a firm or industry resulting from a proportionate increase in all inputs. In other words, when a firm wants to evaluate how much change in output is brought about courtesy of a change in the units of factors (say, doubling of the labor and capital), the firm is essentially evaluating its returns to scale.</p>
------------------------------
Question: What are increasing returns to scale?
Answer: <p>Increasing returns to scale is a phenomenon where if a firm increases its inputs by a given proportion, the output increases by more than that proportion. For example, assume that 2 units of labor and 2 units of capital produce 5 units of output, and 4 units of labor and 4 units of capital produce 15 units of output, then there exists increasing returns to scale. This is because, while the inputs were doubled, then output more than doubled (tripled) as a consequence.</p>
------------------------------
Question: What are constant returns to scale?
Answer: <p>Constant returns to scale is described as the relationship between the amount of resources or inputs, such as labor, capital, and supplies, utilized in comparison to the amount of production or output. If a company increases input, they will see the exact same change in the amount of output. For example, if a firm doubles its inputs and its output exactly doubles as a consequence, then the production process is said to follow constant returns to scale.</p>
------------------------------
Question: What are decreasing returns to scale?
Answer: <p>Decreasing returns to scale is when all production variables are increased by a certain percentage resulting in&nbsp;a less-than-proportional increase in output. For example, if a computer manufacturer doubles its total input but gets only a 30% increase in total output, then it has&nbsp;experienced decreasing returns to scale.</p>
------------------------------
Question: What is meant by economies of scale?
Answer: <p>The term “economies of scale” refers to the advantages that can sometimes occur as a result of increasing the size of a business. By buying a large number of products at once, it could negotiate a lower price per unit than its competitors and this would allow the bulk purchaser to either pass on this benefit to its customers in the form of lower prices, or retain that benefit for themselves in the form of a higher profit margin. Financial economies of scale mean the company has cheaper access to capital. A larger company can get funded from the stock market with an initial public offering. It costs almost nothing to support each additional online customer with existing digital infrastructure and so, any revenue from the new customer is all profit for the business. A company has external economies of scale if its size creates preferential treatment.</p>
------------------------------
Question: What is meant by diseconomies of scale?
Answer: <p>Diseconomies of scale represent the situation where the marginal cost of a product increases as the output increases. In other words, it’s a point in the production process where&nbsp;economies of scale&nbsp;reach their limit and marginal costs begin to increase instead of decrease with additional production. DoS are related to a range of factors that pertain to a company’s performance. Reasons for the marginal cost to increase as the output increases may include difficulty in controlling complex projects (managerial inefficiency,) bureaucracy, ineffective maintenance of equipment, lack of employee motivation (labor inefficiency), and ineffective mechanisms of cost control.</p>
------------------------------
Question: What is elasticity of substitution in production?
Answer: <p>Elasticity of substitution in production deals with how easily a firm can substitute one factor of production (say, labor) for another (say, capital). If the production function is linear, input substitution is seamless. On the other hand, if inputs are used in fixed proportions, input substitution isn't possible at all. Mathematically, it is given as -</p><p><br></p><p>Elasticity of substitution = Percentage change in K / Percentage change in L</p><p><br></p><p>                                    = [(Change in K)/K] / [(Change in L)/L] </p><p><br></p><p>                                    = [Change in (K/L) . MRTS] / [Change in MRTS . (K/L)]</p><p><br></p><p>                                    = Percent change in (K/L) / Percent change in MRTS</p><p><br></p><p><br></p>
------------------------------
Question: What is the CES production function?
Answer: <p>The CES production function is a neoclassical production function that displays constant elasticity of substitution. In other words, the production technology has a constant percentage change in factor (e.g. labor and capital) proportions due to a percentage change in marginal rate of technical substitution.</p><p><br></p>
------------------------------
Question: What is the Cobb-Douglas production function?
Answer: <p>The Cobb-Douglas Production Function, given by Charles W. Cobb and Paul H. Douglas is a linear homogeneous production function, which implies, that the factors of production can be substituted for one another up to a certain extent only. With the proportionate increase in the input factors, the output also increases in the same proportion. Thus, there are constant returns to a scale. In Cobb-Douglas production function, only two input factors, labor, and capital are taken into the consideration, and the elasticity of substitution is equal to one. It is also assumed that, if any, of the inputs, is zero, the output is also zero.</p>
------------------------------
Question: What are opportunity costs in economics?
Answer: <p>Economists use the term opportunity cost to indicate what must be given up to obtain something that’s desired. For example, if one sleeps through his economics class the opportunity cost is the learning he misses. In essence, it refers to the hidden cost associated with not taking an alternative course of action. If, for example, a company pursues a particular business strategy without first considering the merits of alternative strategies available to them, they might therefore fail to appreciate their opportunity costs and although the company’s chosen strategy might turn out to be the best one available, it is also possible that they could have done even better had they chosen another path.</p>
------------------------------
Question: What is an isocost line?
Answer: <p>An isocost line is a graph of combinations of labor and capital, or any other two factors of production, such that the total cost remains the same. An isocost line is to the producers what a budget line is to a consumer. It is an important tool for determining what combination of factor-inputs the firm will choose for production process. The total cost indicated by an isocost increases as we move further away from the origin.</p><p><img src="https://businesstopia.b-cdn.net/wp-content/uploads/2018/01/shift-in-isocost-line-change-in-outlay.jpg" alt="Concept of Isocost Line - Businesstopia"></p>
------------------------------
Question: What is a firm's cost minimization problem?
Answer: <p>If a firm has multiple variable inputs, it faces a cost minimization problem. The cost-minimization problem of the firm is to choose an input bundle (z1, z2) feasible for the output y that costs as little as possible. In terms of the figure, a cost-minimizing input bundle is a point on the y-isoquant that is on the lowest possible isocost line. Mathematically, the firm's cost minimization problem is given by -</p><p><br></p><p>min rK + wL; where, K = capital, L = labor, r = rental rate, w = wage rate</p>
------------------------------
Question: What is a firm's expansion path?
Answer: <p>In economics, an expansion path is a path connecting optimal input combinations as the scale of production expands. A producer seeking to produce a given number of units of a product in the cheapest possible way chooses the point on the expansion path that is also on the isoquant associated with that output level. Economists Alfred Stonier and Douglas Hague defined “expansion path” as "that line which reflects the least–cost method of producing different levels of output, when factor prices remain constant. The points on an expansion path occur where the firm's isocost curves, each showing fixed total input cost, and its isoquants, each showing a particular level of output, are tangent; each tangency point determines the firm's conditional factor demands. As a producer's level of output increases, the firm moves from one of these tangency points to the next; the curve joining the tangency points is called the expansion path. A Cobb–Douglas production function is an example of a production function that has an expansion path which is a straight line through the origin.</p><p><br></p><p><img src="https://newsandstory.com/tempImage/11051117381120198978.PNG" alt="What is Expansion Path in Economics?"></p>
------------------------------
Question: What is average cost of a firm?
Answer: <p>Average cost of a firm is the cost it incurs on average to produce one unit of its product. Mathematically, it is the total cost of the firm divided by the total quantity produced at any point in time.</p>
------------------------------
Question: What is marginal cost?
Answer: <p>Marginal cost is the cost incurred by a firm for the production of one more unit. For example, if the total cost of producing 5 light bulbs is $50, and the total cost of producing 6 light bulbs is $58, then the marginal cost of producing the 6th light bulb in $8.</p>
------------------------------
Question: State Shephard’s Lemma.
Answer: <p>Shephard's lemma is a major result in microeconomics having applications in the theory of the firm and in consumer choice. The lemma states that if indifference curves of the expenditure or cost function are convex, then the cost minimizing point of a given good i with price Pi is unique. The idea is that a consumer will buy a unique ideal amount of each item to minimize the price for obtaining a certain level of utility given the price of goods in the market.</p>
------------------------------
Question: Explain Hotelling’s lemma.
Answer: <p>Hotelling's lemma is a result in microeconomics that relates the supply of a good to the maximum profit of the producer. It was first shown by Harold Hotelling, and is widely used in the theory of the firm. Hotelling's Lemma says that we can find the profit-maximizing choices of output and input by taking partial derivatives of the profit function of the firm. Let output <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d" alt="y"> have price <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" alt="p"> and inputs <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a8788bf85d532fa88d1fb25eff6ae382a601c308" alt="x_{1}"> and <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d7af1b928f06e4c7e3e8ebfd60704656719bd766" alt="x_{2}"> have prices <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2f6728d2b30f42f88b52281be5ae0584fdc9df64" alt="w_{1}"> and <img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8998e0957bb573a19e7d9d934ced62ee68ab8fb8" alt="w_{2}">. Then, we have -</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/fe31c8f389544832a353734b14628eb8.png"></p>
------------------------------
Question: What is Roy's identity?
Answer: <p>Roy's identity (named for French economist René Roy) is a major result in microeconomics having applications in consumer choice and the theory of the firm. The lemma relates the ordinary (Marshallian) demand function to the derivatives of the indirect utility function. Specifically, denoting the indirect utility function as v (p, w) the Marshallian demand function for good i can be calculated as -</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/7d4a84c9c80892111fc3f601e3b13a35.png"></p><p><br></p><p>Here, pi stands for the price of the ith good, and w stands for the individual's income.</p>
------------------------------
Question: Explain duality in production.
Answer: <p>In consumer’s theory (where consumption duality is analyzed), the firm´s input decision has a dual nature. Finding the optimum levels of inputs, can not only be seen as a question of choosing the lowest isocost line tangent to the production isoquant (as seen when minimizing cost), but also as a question of choosing the highest production isoquant tangent to a given isocost line (maximizing production). In other words, having a cost function that sets a budget constraint, solving for the inputs allocation that gives the highest output.</p>
------------------------------
Question: ‘Non optimal input choices must be made in the short run’ - Explain
Answer: <p>Short run costs are non optimal in the sense that they aren't the minimal costs for producing the various output levels. Since capital is held fixed in the short run, the firm does not have the flexibility of input choice. Rather to vary its output level in the short run, the firm will be forced to use non optimal input combinations. The marginal rate of technical substitution will not necessary be equal to the ratio of the input prices.</p>
------------------------------
Question: Give a short note on the homogeneity of cost functions.
Answer: <p>All total cost functions are homogeneous of a certain degree in input prices. Say we consider a Cobb-Douglas production function, the corresponding cost function is homogeneous of degree 1. Hence, doubling the input prices will double the cost of producing a given level of output. Cost minimization requires that the ratio of input prices equals MRTS, hence if input prices are doubled, the ratio doesn't change. So, the firm will buy exactly the same set of inputs and pay precisely twice as much for them. Therefore, cost function will shift upward or downward in precise correspondence to the rate of input price inflation or deflation, as the case may be.</p>
------------------------------
Question: State intuitively, how a long run cost function can be derived from short run cost functions.
Answer: <p>A long run cost function can be derived as an envelope of the short run cost function. Graphically, a long run average cost function can be derived by tracing the minimum points of the short run cost functions. The different short run average cost curves correspond to different economies of scale.</p><p><br></p><p><img src="https://www.researchgate.net/profile/Fiona-Maclachlan-2/publication/290601599/figure/fig2/AS:654794607845376@1533126608008/The-long-run-average-cost-LRAC-curve-is-an-envelope-curve-of-the-short-run-average-cost.png" alt="The long-run average cost (LRAC) curve is an envelope curve of the... |  Download Scientific Diagram"></p>
------------------------------
Question: What do we mean by perfect competition?
Answer: Perfect competition, also known as pure competition or a perfect market, is the market economy at its finest, the most competitive market possible, a market where there are no monopolies, duopolies, oligopolies, oligopsonies or monopsonies. In a market with perfect competition, conditions are so ideal that any individual seller or buyer has no significant impact on prices. Perfect competition is the opposite of imperfect competition, where the ones who call the shots are frequently in a position to abuse their power. Perfect competition is a benchmark, or "ideal type," to which real-life market structures can be compared. Perfect competition is theoretically the opposite of a monopoly, in which only a single firm supplies a good or service and that firm can charge whatever price it wants since consumers have no alternatives and it is difficult for would-be competitors to enter the marketplace.
------------------------------
Question: What are the assumptions of perfect competition?
Answer: <p>Perfect competition has four key assumptions. They are as follows -</p><p><br></p><ol><li>Firms produce a homogeneous product.</li><li>There are many buyers and sellers of the product, such that an individual market agent's part of the produce is relatively insignificant.</li><li>All buyers and sellers possess perfect information about prices and quality of the product.</li><li>Sellers and buyers are free to move in and out of the market as they please. </li></ol>
------------------------------
Question: What is meant by anonymity of firms?
Answer: <p>Anonymity of firms is one of the assumptions of a market following perfect competition. It says that the product of one firm cannot be distinguished from that of another firm. In other words, trademarks, brand labels, etc. do not exist. So, essentially, to the consumers, each seller is selling the exact same product.</p>
------------------------------
Question: What is average revenue?
Answer: <p>Average revenue of a business is obtained by dividing the total revenue with the total output. Average revenue is usually calculated over a fixed period of time. For example, if total revenue collected from the sale of 10 units is $100, then the average revenue is $10.</p>
------------------------------
Question: What is marginal revenue?
Answer: <p>In economics, marginal revenue refers to the additional revenue that will be obtained by the production of one additional unit of a product. Mainstream economic theory teaches that a firm will produce a given product up until the point where marginal cost is equal to marginal revenue and this is the point at which profits are maximized, and beyond which, if one more unit were produced, it would result in a loss. </p>
------------------------------
Question: How is the profit function of a firm defined?
Answer: A profit function is a function that focuses on business applications. The primary purpose for a business is to sell a product or service in order to make a profit, which is the revenue a company receives for selling a product or service less the cost for creating a product or service. The profit function equation is made up of two primary functions: the revenue function and the cost function. If x represents the number of units sold, we will name these two functions as follows: R(x) = the revenue function; C(x) = the cost function and therefore, our profit function equation will be as follows: P(x) = R(x) - C(x). It equals total revenue minus total costs, and it is maximum when the firm’s marginal revenue equals its marginal cost.
------------------------------
Question: What is breakeven point?
Answer: <p>The breakeven point (also known as neutral) is a business term that refers to the point of activity where revenues are equal to costs; that is, at the point of activity where there is no gain or loss. In investing, the breakeven point is said to be achieved when the market price of an asset is the same as its original cost.</p>
------------------------------
Question: What is the theory of contestability?
Answer: The contestable market theory is an economic concept stating that companies with few rivals behave in a competitive manner when the market they operate in has weak barriers to entry. The theory assumes that even in a monopoly or oligopoly, incumbents will act competitively when there is a lack of barriers, such as government regulation and high entry costs, doing everything they can to prevent new entrants from one day putting them out of business. The continuous risk of new entrants emerging and stealing market share leads incumbents to focus more on maximizing sales rather than profits. That's because opening up a market to potential new entrants may be sufficient to encourage efficiency and discourage anti-competitive behavior. A perfectly contestable market is one with no entry or exit costs.
------------------------------
Question: What are normal profits?
Answer: Normal profit is the base profit which a business needs to earn in order for someone to keep the business going. When businesses earn less than this, it is an indicator that they should switch industries or make other changes to their practices. Businesses which exceed this are doing well, and create an incentive for additional businesses to enter the market. The compensation is higher than the opportunity cost that the firm loses for using its resources effectively and producing a given product. If a firm’s profits are lower than its revenues, the firm incurs losses. It must meet a minimum threshold to stay in business.
------------------------------
Question: What are supernormal profits?
Answer: <p>While normal profit is the amount of money made by an organization that is just enough to cover its expenses and not go out of business, supernormal profits are profits made in excess of normal profits. Supernormal profits can be in the long run or in the short run. Not all firms make supernormal profits in the short run. Firms that make supernormal profits in any market are giving indications that the market is viable and that there is an opportunity for money to be made.</p>
------------------------------
Question: What is a monopoly?
Answer: <p>Monopoly is the market condition where a single supplier dominates the market for a given product and therefore consumers can only buy the product from that company only. No other company competes with them in that space and, thus, the product has no substitutes. An unregulated monopoly has market power and can influence prices. Examples: Microsoft and Windows, DeBeers and diamonds, etc.</p>
------------------------------
Question: What is 1st degree price discrimination?
Answer: <p>First-degree price discrimination (also called perfect price discrimination) occurs when a producer charges each consumer his reservation price, the maximum amount that he is willing to pay, for each unit. When a firm has a single price and it faces a downward-sloping demand curve, it must reduce its price for all units in order to sell one more unit. The marginal revenue of such a firm equals price adjusted for the reduction in revenue for all units due to reduction in price. First-degree price discrimination is efficient. It does not result in dead-weight losses, so no economic welfare is lost. In other words, the firm charges its prices at points along the demand curve.</p>
------------------------------
Question: What is 2nd degree price discrimination?
Answer: Second-degree price discrimination (also called nonlinear price discrimination) occurs when a firm charges different prices for different quantities of the product. In most cases, firms do not have such detailed information about their customers, and they must infer reservation prices using some other measure. The law of diminishing marginal utility provides a useful insight: it tells us that the reservation price for the first unit must be higher than the second unit and so on because marginal utility decreases with increase in consumption. At the same time, consumers utility diminishes for each additional unit they buy. For example, a consumer may value their first doughnut at $1.50, but they don’t really want more than just the one and yet they would be willing to pay $0.75 for another as that value would represent the diminished utility they would receive from consuming another and so by offering additional units at a lower price, the seller is able to get around diminishing marginal utility.
------------------------------
Question: What is 3rd degree price discrimination?
Answer: <p>Third degree price discrimination is the most common of all the types of price discrimination. It is commonly used by restaurants, cinemas, taxis, train tickets, and retailers – among others and this is because it is relatively easy to implement and is largely more effective at increasing custom than first and second degree price discrimination. Prices are more closely linked to the consumer’s elasticity of demand – meaning prices are adjusted to the consumers’ willingness to pay. Third degree price discrimination is where a business charges consumers different prices based on which consumer segment they are in. The two main conditions of price discrimination are: – Ability to segment consumers into different groups.– Ability to prevent resale. Examples of third degree price discrimination include: cinema tickets, student discounts, restaurants, and taxis.</p>
------------------------------
Question: Why are competitive markets more economically efficient than monopolistic markets?
Answer: <p>Competitive markets have higher economic efficiency than the monopolies do. This is because competition among the firms may make the company invent new or better commodities. They may also invent more efficient processes. Besides, competition among the firms leads them to invent significantly low-cost manufacturing processes; thus, their profits may increase as a consequence and help them compete. A monopoly is less efficient since it does not have competitors; hence it may not be encouraged to invent lower-cost manufacturing or efficient processes.</p>
------------------------------
Question: Why do monopolies exist?
Answer: <p>A monopoly is a market with only one seller and no close substitutes for the product or service that the seller is providing. Monopolies typically originate due to barriers--which may be technological or physical or regulatory--that prevent other companies from entering the market and giving the monopolist some competition. Consumers who will not or cannot pay the price don’t get the product. For reasons both good and bad, the desire and conditions that create monopolies will continue to exist.</p>
------------------------------
Question: What is a natural monopoly?
Answer: <p>A market structure in which a single firm provides a product or service to consumers is called a Natural Monopoly. A natural monopoly may lack competition because the barriers to enter the market are so high that it is not profitable for new companies to enter. Firms that control the monopoly have usually achieved an economy of scale, meaning they have lowered their production costs while raising production output and lowering marginal costs. Natural monopolies share several characteristics such as high barriers to entry, occurring naturally, firm efficiency, and lower production prices because of efficiency. Barriers could be physical resources, infrastructure, geographical or monetary.</p>
------------------------------
Question: Explain the two-tier pricing system in monopolies.
Answer: <p>The two-tier pricing system is one under which a monopoly is allowed to charge some users a higher price while maintaining a lower price for marginal users. Essentially, the consumers paying the relatively higher price subsidize the losses of low-price consumers.</p>
------------------------------
Question: What are some benefits of monopolies?
Answer: <p>Monopolies are firms who dominate the market. A monopoly tends to set higher prices than a competitive market leading to lower consumer surplus. However, on the other hand, monopolies can benefit from economies of scale leading to lower average costs, which can, in theory, be passed on to consumers. Monopolies may have benefits, but the government should regulate price gouging and unfairness to consumers. Monopolies to some extend are beneficial in the sense that they engage in research and development thereby improving quality. Also, there is usually extensive customer care since they are the sole producer.</p>
------------------------------
Question: What is monopolistic competition?
Answer: <p>Monopolistic competition is a middle ground between monopoly and perfect competition (a purely theoretical state), and combines elements of each. All firms in monopolistic competition have the same, relatively low degree of market power; they are all price makers. In the long run, demand is highly elastic, meaning that it is sensitive to price changes. In the short run, economic profit is positive, but it approaches zero in the long run. Firms in monopolistic competition tend to advertise heavily. There is freedom of entry and exit in a monopolistic competitive market, and one firm's decision does not affect that of its competitors.</p>
------------------------------
Question: What is an oligopoly?
Answer: <p>Oligopoly is a market structure with a small number of large firms. A monopoly is one firm, a duopoly is two firms and an oligopoly is two or more firms. There is no precise upper limit to the number of firms in an oligopoly. The key characteristic of an oligopoly is that no single firm should have significant influence on the market. For example, in the US, the electrical goods market is oligopolistic.</p>
------------------------------
Question: What are the assumptions of the price leadership model?
Answer: <p>The assumptions of the price leadership model are as follows -</p><ol><li>There is one firm much larger than any other.</li><li>All firms except the dominant one are price takers.</li><li>The industry demand curve is known to the dominant firm.</li></ol>
------------------------------
Question: What is the Stackelberg model of oligopoly?
Answer: <p>A Stackelberg oligopoly is one in which one firm is a leader and other firms are followers and this model applies where: (a) the firms sell homogeneous products, (b) competition is based on output, and (c) firms choose their output sequentially and not simultaneously. In many cases the leader gets this position because it is a first-mover who chooses its output before other firms can do it. Since other firms must set their output decision given the leader’s output decision, the leader in a Stackelberg oligopoly typically has a bigger market share and higher profit than other firms in the oligopoly.</p>
------------------------------
Question: What is the cournot model of oligopoly?
Answer: <p>The Cournot model of oligopoly assumes that rival firms produce a homogeneous product, and each attempts to maximize profits by choosing how much to produce. All firms choose output (quantity) simultaneously. The basic Cournot assumption is that each firm chooses its quantity, taking as given the quantity of its rivals. The Cournot model remains the standard for oligopolistic competition, although it can also be extended to include multiple firms. Cournot’s ideas were adopted and popularized by the Swiss economist Leon Walras, considered by many to be the founder of modern mathematical economics. The Cournot model has some significant advantages. It also yields a stable&nbsp;Nash equilibrium, an outcome from which neither player would like to deviate unilaterally.</p>
------------------------------
Question: What is the Betrand model of oligopoly?
Answer: <p>The Bertrand model of oligopoly applies when firms in the oligopoly produce standardized products at same marginal cost. When the marginal cost is same, it is in the best interest of each firm in oligopoly to undercut its rival (i.e. beat its price), because the other firms are also trying to beat it and this price war leads to a situation at which market price is equal to the marginal cost. The differentiated-products Bertrand model contends that when an oligopoly produces differentiated products, price competition doesn’t necessarily lead to a competitive outcome.</p>
------------------------------
Question: How are cartels formed in oligoply?
Answer: <p>A cartel is an organization of sellers and buyers of a market who come together to do an agreement to fix the selling prices and material purchase prices.</p><p>Oligopolistic firms join a cartel to increase their market power, and members work together to determine jointly the level of output that each member will produce and/or the price that each member will charge. The cartel price is determined by market demand curve at the level of output chosen by the cartel. Cartels are more prevalent in such oligopoly markets where there are few numbers of firms or where the number of sellers or producers is few in number. In this way they control or manipulate the prices of commodity in the market.</p>
------------------------------
Question: What are the three basic elements of a game in game theory?
Answer: <p>The three basic elements in a game are -</p><ol><li>Players - the participants/agents involved in the game</li><li>Strategies - the actions or moves each player can choose at certain stages of the game</li><li>Payoffs - the outcome of a game that depends of the selected strategies of the players</li></ol>
------------------------------
Question: What is the difference between a dominant and a Nash equilibrium in game theory?
Answer: <p>According to game theory, the dominant strategy is the optimal move for an individual regardless of how other players act. A Nash equilibrium describes the optimal state of the game where both players make optimal moves but now consider the moves of their opponent. Essentially, in the dominant strategy, it is as if the other player's actions were considered, but each player's best strategy stayed the same nonetheless. Clearly, a Nash equilibrium need not be a dominant strategy equilibrium, but the vice versa is true.</p>
------------------------------
Question: What are mixed strategies in game theory?
Answer: <p>In game theory, a player is said to use a mixed strategy whenever he or she chooses to randomize over the set of available actions. Formally, a mixed strategy is a probability distribution that assigns to each available action a likelihood of being selected. If only one action has a positive probability of being selected, the player is said to use a pure strategy. A mixed strategy profile induces a probability distribution or lottery over the possible outcomes of the game. A (mixed strategy) Nash equilibrium is a strategy profile with the property that no single player can, by deviating unilaterally to another strategy, induce a lottery that he or she finds strictly preferable. </p>
------------------------------
Question: What are sequential games in game theory?
Answer: <p>In sequential games, the players play their respective moves or strategies one after the other. In such a game, players are able to observe what the other participants have done in their previous moves, and play accordingly. That is, there is a specific order of play. On the other hand, simultaneous games require players to play their moves/strategies at the same time, without observing the moves of one another.</p>
------------------------------
Question: What is an Edgeworth box?
Answer: <p>The Edgeworth box is named after Francis Ysidro Edgeworth, who presented it in his book Mathematical Psychics: An Essay on the Application of Mathematics to the Moral Sciences, 1881. Edgeworth's original two-axis depiction was developed into the now familiar box diagram by Pareto in his 1906 Manual of Political Economy and was popularized in a later exposition by Bowley. The modern version of the diagram is commonly referred to as the Edgeworth–Bowley box. The main use of the Edgeworth box is to introduce topics in general equilibrium theory in a form in which properties can be visualized graphically. It shows the possible consumption solutions of two economic agents choosing between two commodities. It can also show the difficulty of moving to an efficient outcome in the presence of bilateral monopoly. In the latter case, it serves as a precursor to the bargaining problem of game theory that allows a unique numerical solution.</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/a8117a1654ea60b1ceff40e77f728fbf.png"></p>
------------------------------
Question: Explain Walras' Law.
Answer: <p>In economics, Walras law is a theory that maintains that surplus in one market must be adequately complemented by insufficiency in another market so that the market will be in equilibrium. Walras law was developed in 1874 by Lon Walras, a French economist and this economic theory aims at achieving an equivalent position in the market and for markets to be in equilibrium, excess demand in one market must mean an excess supply in another market. Mathematically,</p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/ea2d71a5bf7a3b186f09f8f7ebd4219e.png"></p><p>where, Zi is the excess demand in the ith market, and Pn is the price of the nth good.</p>
------------------------------
Question: What is the first fundamental theorem of welfare economics?
Answer: <p>First fundamental theorem of welfare economics (also known as the “Invisible Hand Theorem”) states that any competitive equilibrium leads to a Pareto efficient allocation of resources, i.e., an allocation from which no party can be made better without making at least one of the other parties worse off. Thus, no intervention of the government is required, and it should adopt only “laissez faire” policies and however, those who support government intervention say that the assumptions needed in order for this theorem to work, are rarely seen in real life.</p>
------------------------------
Question: What is an offer curve?
Answer: <p>In economics and particularly in international trade, an offer curve shows the quantity of one type of product that an agent will export ("offer") for each quantity of another type of product that it imports. The offer curve was first derived by English economists Edgeworth and Marshall to help explain international trade. For example, say a country imports steel and exports cloth. The offer curve is essentially the loci of all combinations of exports and imports that the country agrees upon.</p><p><br></p><p><img src="https://www.economicsdiscussion.net/wp-content/uploads/2018/03/clip_image002_thumb_thumb3.jpg" alt="The Offer Curve | Trade Equilibrium | Economics"></p>
------------------------------
Question: What is the second fundamental theorem of welfare economics?
Answer: <p>The Second Welfare Theorem implies that the problems of distribution and efficiency can be separated. Whatever Pareto efficient allocation you want can be supported by the market mechanism. The market mechanism is distributionally neutral; whatever your criteria for a good or a just distribution of welfare, you can use competitive markets to achieve it. In other words, if all individuals have indifference curves that are strictly convex to the origin, then any Pareto optimal state of the economy that assigns positive quantities of every good  to every person can be achieved as a competitive equilibrium for an appropriate reallocation of initial endowment. </p>
------------------------------
Question: What is meant by Pareto efficiency?
Answer: Pareto efficiency or Pareto optimality is a situation where no individual or preference criterion can be better off without making at least one individual or preference criterion worse off or without any loss thereof. The concept is named after Vilfredo Pareto (1848–1923), Italian civil engineer and economist, who used the concept in his studies of economic efficiency and income distribution. An economy is said to be in a Pareto optimum state when no economic changes can make one individual better off without making at least one other individual worse off.
------------------------------
Question: State Kaldor's compensation principle.
Answer: <p>Compensation principle was subsequently developed into an important feature of welfare economics through the work of Hungarian-born economist Nicholas Kaldor (1908-1986) and English economist John Hicks (1904-1989). It refers to a transfer mechanism by which total economic welfare is maximized when individuals who gain from a change in the economy compensate those who have suffered because of the change. An example of a compensation principle is the Pareto criterion in which a change in states entails that such compensation is not merely feasible but required. Since it does not rely on the physical transfer of money, critics maintain that the compensation principle lacks a quantifiable verification of the relative gains and losses.</p>
------------------------------
Question: State Arrow's impossibility theorem.
Answer: <p>The impossibility theorem, named after economist Kenneth J. Arrow, is a social-choice paradox illustrating the flaws of ranked voting systems. It states that a clear order of preferences cannot be determined while adhering to mandatory principles of fair voting procedures. In other words, any social decision rule that satisfies the requirements of completeness, reflexivity and transitivity, universality, Pareto consistency, and independence of irrelevant alternatives, makes the leader a dictator.</p>
------------------------------
Question: State the principle of comparative advantage.
Answer: <p>The law of comparative advantage is popularly attributed to English political economist David Ricardo and his book “On the Principles of Political Economy and Taxation” written in 1817. The theory of comparative advantage introduces opportunity cost as a factor for analysis in choosing between different options for production. Comparative advantage is an economy's ability to produce a particular good or service at a lower opportunity cost than its trading partners. This is why trade can create value for both parties—because each person can concentrate on the activity for which they have the lower opportunity cost.</p>
------------------------------
Question: Explain the Heckscher-Ohlin model.
Answer: <p>The Heckscher-Ohlin model is an economic theory that proposes that countries export what they can most efficiently and plentifully produce. Also referred to as the H-O model or 2x2x2 model, it's used to evaluate trade and, more specifically, the equilibrium of trade between two countries that have varying specialties and natural resources. The model emphasizes the export of goods requiring factors of production that a country has in abundance. It also emphasizes the import of goods that a nation&nbsp;cannot produce as efficiently. It takes the position that countries should ideally export materials and resources of which they have an excess, while proportionately importing those resources they need.</p>
------------------------------
Question: What is the Stolper-Samuelson model?
Answer: <p>The Stolper–Samuelson theorem is a basic theorem in Heckscher–Ohlin trade theory. It describes the relationship between relative prices of output and relative factor rewards—specifically, real wages and real returns to capital. The Stolper-Samuelson theorem shows there is a positive relationship between changes in the price of an output and changes in the price of the factor used intensively in producing that product. It also shows that there is a negative relationship between changes in the price of an output and changes in the price of the factor not used intensively in producing that product.</p>
------------------------------
Question: What is the factor price equalization theorem?
Answer: <p>Factor price equalization is an economic theory, by Paul A. Samuelson (1948), which states that the prices of identical factors of production, such as the wage rate or the rent of capital, will be equalized across countries as a result of international trade in commodities. The theorem assumes that there are two goods and two factors of production, for example capital and labor. Whichever factor receives the lowest price before two countries integrate economically and effectively become one market will therefore tend to become more expensive relative to other factors in the economy, while those with the highest price will tend to become cheaper. Other key assumptions of the theorem are that each country faces the same commodity prices, because of free trade in commodities, uses the same technology for production, and produces both goods.</p>
------------------------------
Question: What is factor intensity reversal?
Answer: <p>Factor intensity reversal occurs when the same commodity is intensive to different factors of production in different countries. For example, if cloth is labor intensive in India and capital intensive in USA, factor intensity reversal is said to exist.</p>
------------------------------
Question: What is Pigouvian taxation?
Answer: <p>A Pigouvian tax is a tax on any market activity that generates negative externalities (costs not included in the market price). The tax is intended to correct an undesirable or inefficient market outcome (a market failure), and does so by being set equal to the external marginal cost of the negative externalities. In such a case, the market outcome is not efficient and may lead to over-consumption of the product. Often-cited examples of such externalities are environmental pollution, and increased public healthcare costs associated with tobacco and sugary drink consumption. Adverse side effects are those costs that are not included as a part of the product's market price.</p>
------------------------------
Question: What is Coase theorem?
Answer: The Coase Theorem is a legal and economic theory developed by economist Ronald Coase regarding property rights, which states that where there are complete competitive markets with no transaction costs and an efficient set of inputs and outputs, an optimal decision will be selected. It basically asserts that bargaining between individuals or groups related to property rights will lead to an optimal and efficient outcome, no matter what that outcome is. In essence, it states that private parties can solve the problem of externalities on their own, if they can bargain over the allocation of resources without cost.
------------------------------
Question: What is a public good?
Answer: <p>Public goods are commodities or services that benefit all members of society, and which are often provided for free through public taxation. Public goods are the opposite of private goods, which are inherently scarce and are paid for separately by individuals. Public goods are non-rival and non-excludable. Non-rivalrous means that the goods do not dwindle in supply as more people consume them; non-excludability means that the good is available to all citizens.</p>
------------------------------
Question: What is meant by the free rider problem?
Answer: <p>A free rider problem is a situation in which individuals or businesses are receiving benefits without actually contributing anything, thus creating an unfair balance in the distribution of revenues or other resources. A problem of this type can exist in a number of scenarios, since free riding can create a financial hardship over time that renders the process unprofitable for everyone concerned. The term has its origins in transportation. In days gone by, anyone who was found to be riding a stagecoach, or later a public street car or subway system without paying for a ticket was referred to as a free rider. Over time, the term has come to mean anyone who benefits from a business or investing situation, but does not actually pay for the benefits the receive. Because of the free-rider problem –&nbsp;public goods are under-provided or not provided at all. For example, it is good to reduce our production of landfill rubbish. However, if one person in a city of five million produces less rubbish, it makes little difference. There is an incentive to free-ride on efforts of other people to recycle and make less effort yourself.</p>
------------------------------
Question: What is adverse selection?
Answer: <p>Adverse selection is a term used in economics and insurance to describe a market process in which buyers or sellers of a product or service are able to use their private knowledge of the risk factors involved in the transaction to maximize their outcomes, at the expense of the other parties to the transaction. Adverse selection is most likely to occur in transactions in which there is an asymmetry of information—where one party has more or better information than the other party and although information asymmetry tends to favor the buyer in markets such as the insurance industry, the seller usually has better information than the buyer in markets such as used cars, stocks, and real estate. Adverse selection is a problem of knowledge, probabilities and risk.</p>
------------------------------
Question: What is moral hazard?
Answer: Moral hazard is the risk that a party has not entered into a contract in good faith or has provided misleading information about its assets, liabilities, or credit capacity. In addition, moral hazard also may mean a party has an incentive to take unusual risks in a desperate attempt to earn a profit before the contract settles. Moral hazards can be present at any time two parties come into agreement with one another. Each party in a contract may have the opportunity to gain from acting contrary to the principles laid out by the agreement. If, for example, your car is fully insured against any and all damage and there is no deductible, then you would have no incentive to avoid minor accidents, like scratches or backing into poles, beyond the inconvenience of getting the car fixed. You would be much more likely to take risks that could lead to minor car damage knowing that any damage is fully covered.
------------------------------
Question: What is Rawlsian utility?
Answer: <p>The Rawlsian approach to social welfare measures the welfare of a society by the well-being of the worst-off individual (the maximin criterion). A utilitarian measures the welfare of a society by the sum of the individuals' utilities. Rawls, whose work will most often be seen in Public Finance, considers that agents' behavior will axiomatically work in the fashion he envisions - that is, there should be a unanimity about the optimal social welfare. The result of this axiomatic approach is - and always will be - the attentive care of whomever is the least well-endowed, or most vulnerable. Thus there is no randomness in the optimal Rawlsian choice, as it is only optimal if it is the last point on the distribution, or the lowest point on the coordinate system.</p>
------------------------------
Question: What are positive and negative externalities?
Answer: <p>In economics, there are two different types of externalities: positive and negative externalities. As implied by their names, positive externalities generally have a positive effect, while negative ones have the opposite impact. Positive externalities refer to the benefits enjoyed by people outside the marketplace due to a firm’s actions but for which they do not pay any amount. A negative externality exists when the production or consumption of a product results in a cost to a third party.</p>
------------------------------
Question: What do we mean by national income accounting?
Answer: <p>National income accounting is a set of principles and methods used to measure the income and production of a country. There are basically two ways of measuring national economic activity: as the money value of the total production of goods and services during a given period (usually a year) or as the total of incomes derived from economic activity after allowance has been made for capital consumption. National income accounting helps in estimating the income added by the economy.</p>
------------------------------
Question: Why is national income accounting important?
Answer: <p>National income accounting is a bookkeeping system that a government uses to measure the level of the country's economic activity in a given time period.</p><p><strong> </strong>National income accounting is important for the following reasons:</p><p>1) It helps in policy making and planning</p><p>2) It helps in measuring inflation and deflation changes</p><p>3) It helps in comparing the standard of living</p>
------------------------------
Question: What is aggregate supply?
Answer: Aggregate supply, also known as total output, is the total supply of goods and services produced within an economy at a given overall price in a given period. It is represented by the aggregate supply curve, which describes the relationship between price levels and the quantity of output that firms are willing to provide. Typically, there is a positive relationship between aggregate supply and the price level.
------------------------------
Question: What is aggregate demand?
Answer: <p>The term “aggregate demand” refers to the overall demand for all goods and services produced in an economy during a given period of time, preferably a year. Since everything that is consumed domestically in an economy can be considered to be the same as what is produced in the economy during that period, such aggregate demand of an economy can be considered equivalent to its gross domestic product. </p>
------------------------------
Question: Why is the aggregate supply curve vertical in the long run?
Answer: <p>The long-run aggregate supply curve is vertical because in the long run wages are flexible. The long-run aggregate supply curve is perfectly vertical, which reflects economists’ belief that the changes in aggregate demand only cause a temporary change in an economy’s total output. Because the price level does not affect the long-run determinants of real GDP, the long-run aggregate&nbsp;. In other words, in the long run, the economy’s labor, capital,&nbsp;natural resources, and technology determine the total quantity of goods and services supplied, and this&nbsp;quantity supplied is the same regardless of what the price level happens to be. The long-run aggregate-supply curve is&nbsp;consistent with this idea because it implies that the quantity of output (a real variable) does not depend on&nbsp;the level of prices (a nominal variable). As noted earlier, most economists believe that this principle works&nbsp;well when studying the economy over a period of many years but not when studying year-to-year changes. Thus, the aggregate-supply curve is vertical only in the long run.</p>
------------------------------
Question: What is gross domestic product?
Answer: One way to measure the performance of any economy is by calculating the total output of goods and services (referred as aggregate output) of that economy. Gross Domestic Product (GDP) is defined as the total value of all goods and services produced within a country. It is the total value of all finished goods and services a country produces within a specific time, usually a year. As a broad measure of overall domestic production, it functions as a comprehensive scorecard of a given country’s economic health.
------------------------------
Question: What is gross national product?
Answer: <p>Gross national product (GNP) is the total monetary value of the products and services produced by a country’s citizen, regardless of where the location of production is. The production location may be in their country or outside the country and so, for example, the gross national product considers your output and the output of your friends or relatives working abroad. GNP counts the investments made by U.S. residents and businesses—both inside and outside the country—and computes the value of all products manufactured by domestic companies, regardless of where they are made.</p>
------------------------------
Question: What is net national product?
Answer: <p>Net national product considers all the goods, products and services that are manufactured by the country’s citizens, irrespective of their location, or in other words, net national product considers products that are produced domestically and also from overseas. Net national product (NNP) refers to gross national product (GNP), i.e. the total market value of all final goods and services produced by the factors of production of a country or other polity during a given time period, minus depreciation.</p>
------------------------------
Question: What is net domestic product?
Answer: <p>The net domestic product is defined as the net value of all the goods and services produced within a country’s geographic borders. The net domestic product (NDP) is calculated by subtracting the value of depreciation of capital assets of the nation such as machinery, housing, and vehicles from the gross domestic product (GDP). As a broad measure of overall domestic production, it functions as a comprehensive scorecard of a given country’s economic health.</p>
------------------------------
Question: What is real GDP?
Answer: <p>Real GDP tracks the total value of goods and services calculating the quantities but using constant prices that are adjusted for inflation. This is opposed to nominal GDP that does not account for inflation. Adjusting for constant prices makes it a measure of "real" economic output for apples-to-apples comparison over time and between countries. It provides a more realistic assessment of growth than nominal GDP.</p>
------------------------------
Question: Why is the calculation of real GDP important?
Answer: <p>Real GDP is important because it gives us a better idea of the progress made by an economy over the years. It is possible for an economy to report growth in absolute GDP without actually increasing its production capabilities. Without real GDP, it could seem like a country is producing more when it's only that prices have gone up. Knowing real GDP trends can help you prepare for recessions or make good financial decisions.</p>
------------------------------
Question: What is meant by the growth rate of an economy?
Answer: An economic growth rate is the percentage change in the value of all of the goods and services produced in a nation during a specific period of time, as compared to an earlier period. The economic growth rate is used to measure the comparative health of an economy over time. The numbers are usually compiled and reported quarterly and annually. The real economic growth rate, or real GDP growth rate, measures economic growth, as expressed by gross domestic product (GDP), from one period to another, adjusted for inflation or deflation. In other words, it reveals changes in the value of all goods and services produced by an economy—the economic output of a country—while accounting for price fluctuations.
------------------------------
Question: What is per capita GDP?
Answer: Per capita gross domestic product (GDP) is a metric that breaks down a country's economic output per person and is calculated by dividing the GDP of a country by its population. Small, rich countries and more developed industrial countries tend to have the highest per capita GDP. It divides the country's gross domestic product by its total population.
------------------------------
Question: What causes GDP to grow in the short run?
Answer: In the short term, economic growth is caused by an increase in aggregate demand (AD). If there is spare capacity in the economy, then an increase in AD will cause a higher level of real GDP. Economic growth is the increase in the market value of the goods and services that an economy produces over time. It is measured as the percentage rate change in the real gross domestic product (GDP).
------------------------------
Question: What is a business cycle?
Answer: <p>A business cycle, sometimes called a "trade cycle" or "economic cycle", refers to a series of stages in the economy as it expands and contracts. Cycles are just a part of theoretical knowledge companies and countries try to use in decision making. Where it shows expansions and contractions in economic activity that a nation experiences over a long period. Constantly repeating,  it is primarily measured by the rise and fall of the gross domestic product (GDP) of a country.</p>
------------------------------
Question: What is the trend path of GDP?
Answer: <p>The trend rate of growth is the rate of economic growth that can be maintained without inflationary pressures. It is also known as the ‘underlying trend rate of growth’. The real GDP growth rate can diverge from this average trend. For example, in a boom, growth may be higher than the trend rate and however, sustained economic growth above the trend rate&nbsp;is likely to cause inflation and be unsustainable. The trend path of GDP is the path it'd follow if factors of production were fully employed.</p>
------------------------------
Question: What is the fundamental national income accounting identity?
Answer: <p>The basic accounting identity for GDP, sometimes known as the national income identity, is computed using the following formula:</p><p>GDP = consumption + investment + government spending + (exports ? imports).</p><p>The quantitative information associated with national income accounting can be used to determine the effect of various economic policies.</p>
------------------------------
Question: What is human capital?
Answer: <p>Human capital is an intangible asset or quality not listed on a company's balance sheet. It can be classified as the economic value of a worker's experience and skills and this includes assets like education, training, intelligence, skills, health, and other things employers value such as loyalty and punctuality. Investing in these qualities produces greater economic output. The investments are called human capital because workers aren't separate from these assets. In a corporation, it is called talent management and is under the human resources department.</p>
------------------------------
Question: What is disposable income?
Answer: <p>At the macro level, disposable personal income is closely monitored as one of the key economic indicators used to gauge the overall state of the economy. Disposable earnings refers to the amount of earnings left over after mandatory federal, state and local deductions. Disposable earnings is not necessarily the same as your take-home pay. Deductions from your paycheck may include additional items such as health insurance, retirement plan contributions and health savings accounts. These deductions are voluntary, not mandatory.</p>
------------------------------
Question: What is government budget deficit?
Answer: <p>The federal budget deficit is an estimate of how much money the federal government expects to make (revenue) and how much it expects to spend (expenditures or outlay) each fiscal year. In the United States, like in many countries, revenue comes from taxes that are placed on individuals and corporations, as well as customs duties and tariffs. Public expenditures include things like defense, transportation, unemployment, and social welfare payments, and healthcare. A budget deficit occurs when government spending exceeds revenue.</p>
------------------------------
Question: What is trade deficit?
Answer: A trade deficit is an amount by which the cost of a country’s imports exceeds its exports. Balance of Trade (BOT), also known as trade balance, is the difference between the country’s imports and exports. A negative trade balance indicates a trade deficit. It's one way of measuring international trade, and it's also called a negative balance of trade. You can calculate a trade deficit by subtracting the total value of a country's exports from the total value of its imports.??
------------------------------
Question: What are intermediate goods?
Answer: Intermediate goods are inputs in a product. They are not the final product. Instead, they are sold by suppliers to manufacturers for to include in the final product. For example, coal used in the factory for further production. It can also be said that they act as inputs in other goods and constitute the final goods as ingredients.
------------------------------
Question: What are some of the problems associated with national income accounting?
Answer: <p>Some of the common problems associated with national income accounting are as follows -</p><ol><li>Incomplete/inadequate information</li><li>Danger of double counting</li><li>Changes in prices (price instability)</li><li>Problem of inclusion (not every produced output can be accounted for on a national scale)</li></ol>
------------------------------
Question: What is inflation?
Answer: <p>Inflation is the decline of purchasing power of a given currency over time. A quantitative estimate of the rate at which the decline in purchasing power occurs can be reflected in the increase of an average price level of a basket of selected goods and services in an economy over some period of time. The rise in the general level of prices, often expressed as a percentage, means that a unit of currency effectively buys less than it did in prior periods. Inflation can be contrasted with deflation, which occurs when the purchasing power of money increases and prices decline. In economics, inflation is a general increase in prices and a decrease in the purchasing power of money.</p>
------------------------------
Question: What is a GDP deflator?
Answer: <p>The GDP deflator is a measure of the general price level of all the goods and services being produced in an economy. It is given by -</p><p>GDP deflator = (Nominal GDP / Real GDP) x 100</p><p>Nominal GDP captures the valuation of all goods and services at current prices, while real GDP is the valuation of the same at constant prices without the effect of inflation. Its change measures aggregate price movement in the economy, hence is inflation indicator. It does not just incorporate raw material prices, intermediate products, end products, and services; but also public goods, which are excluded in the two indexes.</p>
------------------------------
Question: What is the producer price index?
Answer: <p>The Producer Price Index is a family of indexes that measures the average change over time in the selling prices received by domestic producers of goods and services. PPIs measure price change from the perspective of the seller and this contrasts with other measures, such as the Consumer Price Index (CPI), that measure price change from the purchaser's perspective.</p>
------------------------------
Question: What is consumption spending?
Answer: <p>Personal consumption expenditures is a measure of national consumer spending. Some examples are dry cleaners, yard maintenance, and financial services. Consumption Expenditure is the spending by households on goods and services, excluding new housing. In developed countries it has become the largest component of Gross Domestic Product (GDP).</p>
------------------------------
Question: What is meant by government purchases?
Answer: <p>Government purchases are expenditures on goods and services by federal, state, and local governments. The combined total of this spending, excluding transfer payments and interest on the debt, is a key factor in determining a nation's gross domestic product (GDP). Transfer payments are expenditures that do not involve purchases, such as Social Security payments and farm subsidies. Government purchases range from spending on infrastructure projects and paying civil service and public service employees, to buying office software and equipment and maintaining public buildings.</p>
------------------------------
Question: What is investment spending?
Answer: <p>Investment spending is an attempt to stimulate production by means of capital goods that are either created or acquired. Companies, individuals, and even the government partake in investment spending. In order to calculate investment spending, you simply subtract depreciation from gross investment. Examples of investment spending include a company buying land that will be used for farming to generate further profits, or a private individual investing in equity bonds.</p>
------------------------------
Question: What are consumer durables?
Answer: <p>Consumer durables involve any type of products purchased by consumers that are manufactured for long-term use. One of the most common of all consumer durables would be the furniture found in the home and this would include items such as sofas, chairs, tables, bed frames, and storage pieces such as chests of drawers and bookshelf units. While once thought to be limited to only items made of sturdy metal or wood, any type of furniture today that is intended for use over the period of at least a few years can rightly be classified as durable.</p>
------------------------------
Question: What are twin deficits?
Answer: A twin deficit occurs when a nation's government has both a trade deficit and a budget deficit. A trade deficit, also known as a current account deficit, happens when a nation imports more than it exports, buying more from other countries and foreign companies than it sells to them. The CA is part of the nation’s balance of payments (BOP) and is representative of its trade balance (exports minus imports). A CA deficit occurs when imports exceed exports, thus the country is a net borrower from abroad. The name “Twin Deficits” arose as CA balance and federal debt were seen to move in tandem.
------------------------------
Question: What is depreciation?
Answer: Depreciation is an accounting convention that allows a company to write off an asset's value over a period of time, commonly the asset's useful life. Assets such as machinery and equipment are expensive. Instead of realizing the entire cost of the asset in year one, depreciating the asset allows companies to spread out that cost and generate revenue from it. Depreciation is used to account for declines in the carrying value over time. If not taken into account, it can greatly affect profits.
------------------------------
Question: Explain the consumption function in brief.
Answer: <p>Mathematically, the consumption function is given as -</p><p><br></p><p>C = cY + c?</p><p><br></p><p>where, c = marginal propensity to consume, Y = disposable income of the individual, c? = autonomous spending of the individual</p><p><br></p><p>The consumption function suggests that an individual's consumption depends on his disposal income, only a part of which is used for consumption purposes. Also, there is a minimum level of consumption c? that the individual consumes even when his income is zero.</p>
------------------------------
Question: What happens to aggregate demand if government spending is increased?
Answer: <p>Increased government spending is likely to cause a rise in aggregate demand (AD) and this can lead to higher growth in the short-term. It can also potentially lead to inflation. If spending is focused on improving infrastructure, this could lead to increased productivity and a growth in the long-run aggregate supply. As aggregate demand increases, unemployment decreases as more workers are hired, real GDP output increases, and the price level increases.</p>
------------------------------
Question: What happens to aggregate demand if income tax is decreased?
Answer: <p>As a general rule, tax cuts increase aggregate demand, since less money paid to the tax authority means more money in the pockets of consumers. In more technical terms, tax cuts result in higher disposable income. In most instances consumers spend rather than save this additional disposable income and this spending results in greater supply, which means suppliers need to hire more employees or pay overtime and higher wages to existing ones to motivate them to produce more and this in turn creates new jobs and higher wages and yet higher total disposable income in the economy, further increasing aggregate demand and this secondary impact is referred to as the multiplier effect.</p>
------------------------------
Question: What is the IS curve?
Answer: <p>The IS curve shows the causation from interest rates to planned investment to national income and output. The IS curve is drawn as downward-sloping with the interest rate r on the vertical axis and GDP (gross domestic product: Y) on the horizontal axis. Every level of the real interest rate will generate a certain level of investment and spending: lower interest rates encourage higher investment and more spending.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/c961eeaa1cbcb59bbf0a9c5f7798691c.png"></p>
------------------------------
Question: Briefly explain the LM curve.
Answer: <p>The LM curve is the money market counterpart of the IS curve. The LM curve depicts the set of all levels of income (GDP) and interest rates at which money supply equals money (liquidity) demand. The LM curve slopes upward because higher levels of income (GDP) induce increased demand to hold money balances for transactions, which requires a higher interest rate to keep money supply and liquidity demand in equilibrium.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/b84c27f3bc695979e46c714bf0322f4f.png"></p>
------------------------------
Question: What are equities?
Answer: <p>Equities are investments that is made in a company by purchasing shares of that company. Investors purchase equities with the expectations that the company's value will rise, and they can lodge a profit by selling it. Technically, it is like part owning the company.</p>
------------------------------
Question: What is real money demand?
Answer: <p>The real demand for money is defined as the nominal amount of money demanded divided by the price level. Generally, the nominal demand for money increases with the level of nominal output (price level times real output) and decreases with the nominal interest rate.</p>
------------------------------
Question: What is a fiscal policy?
Answer: Fiscal policy is what the government employs to influence and balance the economy, using taxes and spending to accomplish this. Fiscal policy tries to nudge the economy in different ways through either expansionary or contractionary policy, which try to either increase economic growth through taxes and spending or slow economic growth to cutback inflation, respectively. Basically, fiscal policy intercedes in the business cycle by counteracting issues in an attempt to establish a healthier economy, and uses two tools - taxes and spending - to accomplish this. It is the sister strategy to monetary policy through which a central bank influences a nation's money supply. These two policies are used in various combinations to direct a country's economic goals.
------------------------------
Question: What is a monetary policy?
Answer: Monetary policy is a set of actions that can be undertaken by a nation's central bank to control the overall money supply and achieve sustainable economic growth. Monetary policy can be broadly classified as either expansionary or contractionary. Some of the available tools include revising interest rates up or down, directly lending cash to banks, and changing bank reserve requirements. Monetary policy is the domain of a nation’s central bank. The Federal Reserve System (commonly called the Fed) in the United States and the Bank of England of Great Britain are two of the largest such “banks” in the world and although there are some differences between them, the fundamentals of their operations are almost identical and are useful for highlighting the various measures that can constitute monetary policy.
------------------------------
Question: What is a liquidity trap?
Answer: First described by economist John Maynard Keynes, during a liquidity trap, consumers choose to avoid bonds and keep their funds in cash savings because of the prevailing belief that interest rates could soon rise (which would push bond prices down). Because bonds have an inverse relationship to interest rates, many consumers do not want to hold an asset with a price that is expected to decline. At the same time, central bank efforts to spur economic activity are hampered as they are unable to lower interest rates further to incentivize investors and consumers. A Liquidity trap emerges when interest charges are nil or during a downturn. People are afraid to spend money. Due to such circumstances, central banks consuming expansionary monetary policy doesn’t improve the economy. In simple words, a liquidity trap meaning is when monetary policies don’t effectively work.
------------------------------
Question: What is the crowding out effect?
Answer: The crowding out effect is an economic theory arguing that rising public sector spending drives down or even eliminates private sector spending. Government spending reduces savings in the economy, thus increasing interest rates and this can lead to less investment in areas such as home building and productive capacity, which includes the facilities and infrastructure used to contribute to the economy’s output.
------------------------------
Question: What are balance of payments?
Answer: The balance of payments (BOP) is the method by which countries measure all of the international monetary transactions within a certain period. The BOP consists of three main accounts: the current account, the capital account, and the financial account. The current account is meant to balance against the sum of the financial and capital account but rarely does.
------------------------------
Question: What is the current account in balance of payments?
Answer: <p>The current account of the balance of payments includes a country's key activity, such as capital markets and services. The current account balance should theoretically be zero, which is impossible, so in reality, it will tell whether a country is in a surplus or deficit. The four major components of a current account are goods, services, income, and current transfers.</p>
------------------------------
Question: What is the fixed exchange rate system?
Answer: <p>A fixed exchange rate&nbsp;is a regime applied by a government or central bank that ties the country's official currency exchange rate to another country's currency or&nbsp;the price of gold. The purpose of a fixed exchange rate system is to keep a&nbsp;currency's value within a narrow band. Fixed exchange rates provide greater certainty for exporters and importers and help the government maintain low inflation. Many industrialized nations began using the floating exchange rate system in the early 1970s.</p>
------------------------------
Question: What is the floating exchange rate system?
Answer: <p>A floating exchange rate is a regime where the currency price of a nation is set by the foreign exchange market based on supply and demand relative to other currencies and this is in contrast to a fixed exchange rate, in which the government entirely or predominantly determines the rate. A floating exchange rate doesn't mean countries don't try to intervene and manipulate their currency's price, since governments and central banks regularly attempt to keep their currency price favorable for international trade. Floating exchange rates became more popular after the failure of the gold standard and the Bretton Woods agreement. Floating exchange rate systems mean long-term currency price changes reflect relative economic strength and interest rate differentials between countries.</p>
------------------------------
Question: What are the two kinds of floating exchange rate systems?
Answer: <p>Floating exchange rates can be broadly divided into two forms. The first is a free-floating exchange rate system – in the foreign exchange market, the exchange rate is completely determined by the supply and demand of foreign exchange. The second is a managed floating exchange rate system – this is a system that assumes that the currency authorities will intervene in the foreign exchange market and manipulate the exchange rate in order to prevent the exchange rate from deviating significantly from the existing standards.</p>
------------------------------
Question: What is appreciation of currency?
Answer: Currency appreciation is an increase in the value of one currency in relation to another currency. Currencies appreciate against each other for a variety of reasons, including government policy, interest rates, trade balances, and business cycles. The value of a currency is not measured in absolute terms. It is always measured relative to the currency being measured against it. Countries use currency appreciation as a strategic tool to boost their economic prospects.
------------------------------
Question: What is depreciation of currency?
Answer: <p>Currency depreciation can generally be described in two different ways. In the case of a single currency, it refers to a loss of value of a country's money over time such that it buys fewer goods on the open market than it did at a prior point in time. When comparing two currencies, the term currency depreciation refers to the loss of value in one currency such that its relative value compared to the other currency has been reduced. Currency depreciation can occur due to factors such as economic fundamentals, interest rate differentials, political instability, or risk aversion among investors.</p>
------------------------------
Question: What is devaluation of currency?
Answer: <p>Devaluation is the deliberate downward adjustment of the value of a country's money relative to another currency, group of currencies, or currency standard. Countries that have a fixed exchange rate or semi-fixed exchange rate use this monetary policy tool. "Currency devaluation" is a phrase that no one wants to see in the headlines and however, the devaluation of currency is often confused with currency depreciation.</p>
------------------------------
Question: What is revaluation of currency?
Answer: A revaluation is a calculated upward adjustment to a country's official exchange rate relative to a chosen baseline. The baseline can include wage rates, the price of gold, or a foreign currency. In floating exchange rate systems, currency revaluation can be triggered by a variety of events, including changes in the interest rates between various countries or large-scale events that impact an economy. After a revaluation, a currency becomes expensive relative to the base currency by the factor of adjustment, and hence the process changes the purchasing power of that currency.
------------------------------
Question: How is depreciation different from devaluation of currency?
Answer: <p>Both devaluation and depreciation are similar in that they refer to the value of a currency decreasing in terms of another currency. While devaluation is done purposely for a number of reasons, depreciation occurs as a result of the forces of demand and supply. Most economists believe that allowing a currency to float may cause economic issues in the short term but will result in an economy that is more stable and solid and able to better cushion itself against market crashes because these factors are already mirrored in the movement of the currency. So, currency depreciation means reduce in the level of money due to market forces. It will happen due to financially viable fundamentals, interest rate differentials, political unsteadiness, risk aversion among investors, etc. Currency devaluation is a financial policy instrument (used internally by the country itself): a purposeful descending modification to the value of the currency comparative to another currency or group of currencies. Now, the difference, the key difference is that depreciation occurs with countries with floating exchange rate while devaluation occurs with countries with a fixed exchange rate.</p>
------------------------------
Question: What is the real exchange rate?
Answer: <p>The real exchange rate measures the price of foreign goods relative to the price of domestic goods. Mathematically, the real exchange rate is the ratio of a foreign price level and the domestic price level, multiplied by the nominal exchange rate. In reality, we use the consumer price index and not just a product to calculate the real exchange rate. The real exchange rate is very important and crucial to arrive at the value of a country’s net exports.</p>
------------------------------
Question: What is the beggar-thy-neighbor policy?
Answer: <p>Beggar-thy-neighbor is a type of strategy that is designed to enhance the financial stability and prosperity of one nation at the expense of other nations that currently do business with that country. Essentially, this trading strategy will make use of the devaluation of currency as well as changes in import and export policies and other economic measures to move the internal economy of the nation in a desired direction. Depending on the severity of the changes, a beggar-thy-neighbor situation may temporarily alleviate some of the economic issues faced by the nation, but can in the long run create new difficulties as the measures negatively impact the nation’s trading partners. It usually takes the form of some kind of trade barrier imposed on the neighbors or trading partners or a devaluation of the domestic currency to gain competitive advantage over them.</p>
------------------------------
Question: What is dirty floating?
Answer: Dirty float is a term used in international economics to describe a specific policy of a country’s monetary authority with respect to control over movements in the value of the nation’s currency within the foreign-exchange market. Under dirty float, a country maintains an independent monetary policy that allows its central bank to achieve a balance between inflation and growth. At the same time, occasional interventions in the foreign-exchange market allow the government or the central bank to avoid large swings in the exchange rates that might destabilize the economy. Both clean and dirty floats represent the floating exchange-rate regime, under which the government does not commit to maintain a specific level of the exchange rate, as is the case with a fixed exchange-rate regime.
------------------------------
Question: What is meant by money neutrality?
Answer: <p>The phrase neutrality of money refers to an economic theory that changes in the supply of money do not primarily impact the actual variables of an economy, such as the rate of employment or the gross domestic production (GDP). As a concept, neutrality of money has been a tenet of classical economics since the 1920s. The phrase “neutrality of money” was eventually coined by Austrian economist Friedrich A. Hayek in 1931. Originally, Hayek defined it as a market rate of interest at which malinvestments—poorly allocated business investments according to Austrian business cycle theory—did not occur and did not produce business cycles.</p>
------------------------------
Question: What is NAIRU?
Answer: <p>NAIRU stands for Non-Accelerating Inflation Rate of Unemployment. In simple words, it is the rate of unemployment that invariably needs to exist in the economy for inflation to remain stagnant. Inflation and unemployment are two phenomenons that tend to exhibit an inversely proportional relationship. Hence, for either of them to be decreased, one must risk increasing the other. </p>
------------------------------
Question: What does the Phillips curve show?
Answer: <p>The Phillips curve states that inflation and unemployment have an inverse relationship. Higher inflation is associated with lower unemployment and vice versa. The Phillips curve was a concept used to guide macroeconomic policy in the 20th century, but was called into question by the stagflation of the 1970's. Understanding the Phillips curve in light of consumer and worker expectations, shows that the relationship between inflation and unemployment may not hold in the long run, or even potentially in the short run. The concept behind the Phillips curve states the change in unemployment within an economy has a predictable effect on price inflation. Alternatively, a focus on decreasing unemployment also increases inflation, and vice versa.</p><p><br></p><p><img src="https://d3fy0m2nzntflb.cloudfront.net/images/answers/4ef7619ba4fe35f7ae75e497173738a7.png"></p>
------------------------------
Question: What is meant by sticky wage?
Answer: <p>Sticky wages is a concept to describe how in the real world, wages may be slow to change and get stuck above the equilibrium because workers resist nominal wage cuts. The sticky wage theory hypothesizes that employee pay tends to respond slowly to changes in company performance or to the economy. According to the theory, when unemployment rises, the wages of those workers that remain employed tend to stay the same or grow at a slower rate rather than falling with the decrease in demand for labor. Specifically, wages are often said to be sticky-down, meaning that they can move up easily but move down only with difficulty.</p>
------------------------------
Question: What are insider-outsider models?
Answer: <p>Insider Outsider Model is a new approach about wage employment relationship. According to this approach, those who become unemployed as a result of fall in the aggregate demand resulting in recession in the economy lose their influence on the wage setting process and do not participate in wage negotiations with the employers because unemployed people lose their status as union members and therefore neither the employers nor the labor union care about them and as a result, while the unemployed workers would want firms to cut wages to create more jobs which would enable them to get beck employment but the firms ignore them and negotiate with the workers who have jobs and are members of unions. The insider outsider model predicts that wages will not respond substantially to unemployment and that is another reason why we don’t quickly return to full employment once the economy experiences recession.</p>
------------------------------
Question: What are some of the properties of the aggregate supply curve?
Answer: <p>The aggregate supply curve exhibits the following properties -</p><ol><li>The aggregate supply curve is flatter the smaller the impact of output and employment changes on current wage.</li><li>The position of the aggregate supply curve depends on the past level of prices.</li><li>Over time, the aggregate supply curve shifts from its original position.</li></ol>
------------------------------
Question: What are the short run effects of a monetary expansion?
Answer: In the short run, economists assume the velocity of money is constant. Thus, when the money supply increases, it results in two possibilities: an increase in the price level and an aggregate output. If aggregate output increases at the same relative percentage, then the effect of the increase in the money supply on inflation is relatively controllable. The expansionary monetary policy encourages an increase in aggregate demand. When aggregate demand increases, it stimulates businesses to increase production and recruit more workers and as a result, the economy grows, inflation rises, and the unemployment rate falls. Increased money supply lowers interest rates and borrowing costs. It stimulates households to spend more, especially on durable goods like property and cars.
------------------------------
Question: What are rational expectations in economics?
Answer: <p>The rational expectations theory postulates that individuals base their decisions on human rationality, information available to them, and their past experiences. The rational expectations theory is a concept and theory used in macroeconomics. The theory also believes that because people make decisions based on the available information at hand combined with their past experiences, most of the time their decisions will be correct. The theory suggests that people’s current expectations of the economy are, themselves, able to influence what the future state of the economy will become.</p>
------------------------------
Question: What is the Lucas supply curve?
Answer: <p>The Lucas aggregate supply function or Lucas "surprise" supply function, based on the Lucas imperfect information model, is a representation of aggregate supply based on the work of new classical economist Robert Lucas. The model states that economic output is a function of money or price "surprise". Lucas supported his original, theoretical paper that outlined the surprise based supply curve with an empirical paper that demonstrated that countries with a history of stable price levels exhibit larger effects in response to monetary policy than countries where prices have been volatile. The model accounts for empirically observed short-run correlations between output and prices, but maintains the neutrality of money (the absence of a price or money supply relationship with output and employment) in the long-run.</p>
------------------------------
Question: What is meant by unanticipated money?
Answer: <p>When inflation occurs unexpectedly, those on a fixed income, such as retired individuals, often encounter losses. Because those on a fixed income don't, or can't, get an increase in their pay, the money they do receive is often not enough to live on or cover expenses since a dollar now has less value. Anticipated inflation occurs when people know inflation is going to occur and prepare for it. When this happens, many individuals are left unprotected, such as lenders who get paid back with a money that has a reduced purchasing power.</p>
------------------------------
Question: What is hyperinflation?
Answer: <p>Hyperinflation is a phenomenon where the prices of goods and services rise more than 50% per month. At that rate, a loaf of bread could cost one amount in the morning and a higher one in the afternoon. The severity of cost increases distinguishes it from the other types of inflation.</p>
------------------------------
Question: What is stagflation?
Answer: <p>Stagflation is a combination of several factors that all point toward a difficult economy. It occurs when prices are affected by inflation alongside unemployment and other economic output factors and this means people are earning less money while spending more on everything from housing and utilities to food, medicine, and consumer products. The term “stagflation” was initially coined in the 1960s, when a politician in the U.K. House of Commons described the combination of employment stagnation and inflated prices. As the U.S. entered recession in the 1970s, the country adopted the stagnation concept to describe the situation. Until that time, stagnation was viewed as an impossible economic theory. Simply put, stagflation is a period of rising inflation but falling output and rising unemployment.</p>
------------------------------
Question: What is the effect of a fiscal expansion in the short run?
Answer: <p>In Keynesian economic theory, fiscal expansionary policy is generally associated with an increase in aggregate demand — the total quantity of goods demanded by all consumers in the market — and triggers growth in output and this has the effect of increasing economic production, especially in the short run. Producers respond to this new demand by increasing production. Likewise, if taxes are lowered, consumers have more discretionary money to spend, and demand for goods increases, taking production along with it. When the government engages in fiscal expansion, it usually instigates a decrease in unemployment. While fiscal expansion tends to increase economic output and employment in the short-run, it is not likely to continue forever.</p>
------------------------------
Question: What does the Solow model of growth deal with?
Answer: <p>In the Solow growth model, economic growth is driven by the accumulation of physical capital until this optimum level of capital per worker, the so-called “steady state”, is reached. The steady state itself is determined by labor force growth, the savings rate, and the rate of depreciation. The model predicts more rapid growth when the level of physical capital per capita is low, something often referred to as “catch up” growth. Clearly, at some point the addition of more machines in a factory would not raise the output per worker, because each worker would already be fully occupied utilizing the existing machinery. This is the framework of the Solow model in a nutshell.</p>
------------------------------
Question: What is the golden rule level of capital and savings rate?
Answer: <p>In economics, the Golden Rule savings rate is the rate of savings which maximizes steady state level of the growth of consumption, particularly in the Solow model. In the Solow growth model, a steady state savings rate of 100% implies that all income is going to investment capital for future production, implying a steady state consumption level of zero. A savings rate of 0% implies that no new investment capital is being created, so that the capital stock depreciates without replacement and this makes a steady state unsustainable except at zero output, which again implies a consumption level of zero. Somewhere in between is the "Golden Rule" level of savings, where the savings propensity is such that per-capita consumption is at its maximum possible constant value.</p>
------------------------------
Question: What does the Modigliani life-cycle hypothesis say?
Answer: <p>The life-cycle hypothesis remains the most influential model of savings. The life-cycle hypothesis (LCH) framework articulates the relationship between consumption, income, wealth, and savings, over the life of individuals. Its central insight is that households have a finite life and a long-term view of their income and consumption needs. They, therefore, increase their wealth during their working life and use it to smooth consumption during retirement. The life-cycle hypothesis was one of the first models used to explain savings; it is supported by many empirical analyses in rich countries and is robust to varying assumptions.</p>
------------------------------
Question: What is the permanent income hypothesis?
Answer: The permanent income hypothesis is a theory of consumer spending stating that people will spend money at a level consistent with their expected long-term average income. A worker will save only if their current income is higher than the anticipated level of permanent income, in order to guard against future declines in income. An individual's liquidity is a factor in their management of income and spending.
------------------------------
Question: What is meant by buffer stock saving?
Answer: A buffer stock is a system or scheme which buys and stores stocks at times of good harvests to prevent prices falling below a target range (or price level), and releases stocks during bad harvests to prevent prices rising above a target range (or price level). The scheme aims at stabilizing the prices, ensuring the uninterrupted supply of goods, and preventing farmers and producers from going out of business as a result of an unexpected fall in prices. Genesis wheat stores, ever-normal granary, EU cap, International cocoa Organization (ICCO), and 1970 wool floor price scheme Australia are few examples of a buffer stock scheme.
------------------------------
Question: What is the Barro-Ricardo hypothesis?
Answer: Ricardian equivalence, sometimes called Barro-Ricardo equivalence, is a hypothesis used to suggest that deficit spending cannot stimulate the economy. The proposed equivalence is between taxes in the present and taxes in the future. According to Ricardian equivalence, deficit spending is equivalent to an immediate increase in taxes because participants in the economy will recognize that the deficit requires future taxes. The theory receives its name from David Ricardo, who suggested it in 1820. Ricardo himself, however, did not fully endorse the idea. The modern formulation was developed in 1974 by Robert Barro. Barro actively promoted the theory, and expressed it in a general form, stating that interest rates would not be affected by the distribution of deficit between debt and taxation.
------------------------------
Question: What does 'myopia' refer to in economics?
Answer: Policy myopia arises when rational voters allow politicians to bias public investments towards short-term investments. We argue that growth in government eventually leads to policy myopia. Marketing myopia is a situation when a company has a narrow-minded marketing approach and it focuses mainly on only one aspect out of many possible marketing attributes. For example, a brand focusing on development of high-quality products for a customer base that disregard quality and only focuses on the price is a classic example of marketing myopia.
------------------------------
Question: What is the q theory of investment?
Answer: <p>In the 60s and 70s, economists including Nicholas Kaldor and James Tobin came up with an investment theory: the q-investment theory, sometimes also referred to as Tobin’s q-investment theory. At its core, Tobin’s q theory of investment relates fluctuations in investment to changes in the stock market and although the theory gained popularity only in the 70s, first elements of the theory can already be found in works of John Maynard Keynes. That is, the q-investment theory explains investment using the difference between the stock market valuation of firms real assets and the replacement costs of these assets.</p>
------------------------------
Question: What is capital expenditure?
Answer: <p>A&nbsp;capital expenditure&nbsp;is something that adds to or upgrades a&nbsp;property’s&nbsp;physical assets and so, it is typically a one-time major&nbsp;expense. Examples of&nbsp;capital expenditures&nbsp;include a new roof, appliance or flooring. As a good practice, you should budget approximately 10% of your gross monthly rental income to save for capital expenditures and then when the water heater quits or other major expenses happen you will have a reserve of cash to handle the issue.</p>
------------------------------
Question: What is cost of capital?
Answer: <p>Cost of capital is a necessary economic and accounting tool that calculates investment opportunity costs and maximizes potential investments in the process. Cost of capital is the required return necessary to make a capital budgeting project, such as building a new factory, worthwhile. When analysts and investors discuss the cost of capital, they typically mean the weighted average of a firm's cost of debt and cost of equity blended together. The cost of capital metric is used by companies internally to judge whether a capital project is worth the expenditure of resources,&nbsp;and by investors who use it to determine whether an investment is worth the risk compared to the return. The cost of capital depends on the mode of financing used.</p>
------------------------------
Question: Explain the M1 component of money.
Answer: <p>M1 money is a country’s basic money supply that's used as a medium of exchange. M1 includes demand deposits and checking accounts, which are the most commonly used exchange mediums through the use of debit cards and ATMs. M1 covers types of money commonly used for payment, which includes the most basic payment form, currency, which is also referred to as M0. The M1 money supply is composed of Federal Reserve notes—otherwise known as bills or paper money—and coins that are in circulation outside of the Federal Reserve Banks and the vaults of depository institutions.</p>
------------------------------
Question: Explain the M2 component of money.
Answer: <p>M2 is a broader money classification than M1 because it includes assets that are highly liquid but are not cash. A consumer or business typically doesn't use savings deposits and other non-M1 components of M2 when making purchases or paying bills, but it could convert them to cash in relatively short order. M1 and M2 are closely related, and economists like to include the more broadly defined definition for M2 when discussing the money supply, because modern economies often involve transfers between different account types. For example, a business may transfer $10,000 from a money market account to its checking account and this transfer would increase M1, which doesn’t include money market funds, while keeping M2 stable, since M2 contains money market accounts. M2 as a measurement of the money supply is a critical factor in the forecasting of issues like inflation.</p>
------------------------------
Question: Explain the M3 component of money.
Answer: <p>M3 is a measure of the money supply that includes M2 as well as large time deposits, institutional money market funds, short-term repurchase agreements, and larger liquid assets. The M3 measurement includes assets that are less liquid than other components of the money supply and are referred to as "near money," which are more closely related to the finances of larger financial institutions and corporations than to those of small businesses and individuals.</p>
------------------------------
Question: Explain the MZM component of money.
Answer: <p>MZM has become one of the preferred&nbsp;measures of money supply because it better represents money readily available within the economy for spending and consumption. The Federal Reserve stopped tracking M3 in 2006, and this measurement derives its name from its mixture of all the liquid and zero maturity money found within the three M’s. MZM includes money in all of the following: Physical currency (coins and banknotes),&nbsp;Checking and savings accounts, and&nbsp;Money market funds. Economists and central bankers use MZM along with the velocity of MZM to better predict inflation and growth, because the more funds readily available, the more money there is to spend, which can be a sign of inflationary pressures. MZM includes the M2 measure less the time deposits, plus all money market funds.</p>
------------------------------
Question: What is Keynes' precautionary motive of holding money?
Answer: <p>The precautionary motive refers to the tendency of a firm to hold cash, to meet the contingencies or unforeseen circumstances arising in the course of business. Since the future is uncertain, a firm may have to face contingencies such as an increase in the price of raw materials, labor strike, lockouts, change in the demand, etc. Thus,&nbsp;in order to meet with these uncertainties, the cash is held by the firms to have an uninterrupted business operations. Hence, a firm holds cash to exploit the possible opportunities that are out of the normal course of business. It is significant as it is used to pay off the firm’s obligations and helps in the expansion of business operations.</p>
------------------------------
Question: What is the transactions demand for money?
Answer: <p>Transactions demand for money is the money demanded by the agents in the economy to meet the purpose of day-to-day purchases. This includes especially cash and checking accounts that people use to buy goods and services. The transactions demand for money is positively affected by the amount of real income and expenditure, and negatively affected by the interest rate on alternative assets.</p>
------------------------------
Question: What is meant by speculative demand for money?
Answer: <p>Speculative demand for money is that part of its demand which arises from the notion that money is best kept in the form of a portfolio with multiple assets. It is the demand for highly liquid financial assets like foreign currency. With speculation, the risk of loss is more than offset by the possibility of a substantial gain or other recompense. An investor who purchases a speculative investment is likely focused on price fluctuations. While the risk associated with the investment is high, the investor is typically more concerned about generating a profit based on market value changes for that investment than on long-term investing. When speculative investing involves the purchase of a foreign currency, it is known as currency speculation. In this scenario, an investor buys a currency in an effort to later sell that currency at an appreciated rate, as opposed to an investor who buys a currency in order to pay for an import or to finance a foreign investment.</p>
------------------------------
Question: What is quantity theory of money?
Answer: <p>The quantity theory of money describes the relationship between the supply of money and the price of goods in the economy. It states that percentage change in the money supply will result in an equivalent level of inflation or deflation. An increase in prices will be termed as inflation while a decrease in the price of goods is deflation. In addition, the theory assumes that changes in the money supply are the primary reason for changes in spending.</p>
------------------------------
Question: What is meant by velocity of money?
Answer: The velocity of money is a measurement of the rate at which money is exchanged in an economy. It is the number of times that money moves from one entity to another. It also refers to how much a unit of currency is used in a given period of time. Simply put, it's the rate at which consumers and businesses in an economy collectively spend money. It is used to help economists and investors gauge the health and vitality of an economy.
------------------------------
Question: Explain the currency-deposit ratio.
Answer: <p>Currency-deposit ratio is the amount of wealth a person holds in cash for every unit of wealth she holds in readily accessible bank accounts.</p><p>The formula for the currency-deposit ratio is cr = C/D, where C = wealth held in cash, D = wealth held in deposits</p><p>The more cash you keep on hand compared with your total deposits, the higher your currency-deposit ratio.</p>
------------------------------
Question: What is the reserve ratio?
Answer: <p>The reserve ratio is the portion of reservable liabilities that commercial banks must hold onto, rather than lend out or invest and this is a requirement determined by the country's central bank, which in the United States is the Federal Reserve. It is also known as the cash reserve ratio. The minimum amount of reserves that a bank must hold on to is referred to as the reserve requirement, and is sometimes used synonymously with the reserve ratio.</p>
------------------------------
Question: What is high powered money?
Answer: <p>In economics, the monetary base in a country is the total amount of bank notes and coins, the total currency circulating in the public, plus the currency that is physically held in the vaults of commercial banks, plus the commercial banks' reserves held in the central bank. The monetary base has traditionally been considered high-powered because its increase will typically result in a much larger increase in the supply of demand deposits through banks' loan-making, a ratio called the money multiplier.</p>
------------------------------
Question: What is the recognition lag in economics?
Answer: <p>Recognition lag is the delay between when an economic shock occurs and when it is recognized by economists, central bankers, and the government.<strong> </strong>Delays occur because economic processes always take place over time and data documenting the state of the economy is not immediately available and then takes time to accurately analyze. On average, a recognition lag takes between three and six months. Recognition lags occur for two main reasons: 1) because economic shocks, like any economic process, necessarily take time to play out, and 2) because it takes time to measure economic activity.</p>
------------------------------
Question: What is the decision lag in economics?
Answer: <p>After officials have recognized a macroeconomics issue they want to address, decided on the desired policy, and actually implemented the policy, it then takes time for the policy measures themselves, such as an injection of credit into the financial system or the issuance of stimulus payments, to work their way through the economy an ultimately have an effect on the economic variables of interest. This is the decision lag.</p>
------------------------------
Question: What are adaptive expectations?
Answer: <p>The term adaptive expectations refers to the way economic agents adjust their expectations about future events based on past information and on some adjustment term and this implies some sort of correction mechanism: if someone’s expectations are off the mark now, they can be corrected the next time, and so on. Economists view decision rules that govern an agent’s behavior as being continuously under revision. As new decision rules are tried and tested, rules that yield accurate outcomes supersede those that fail to do so. In this sense, Robert Lucas (1986) refers to the trial-and-error process through which the models of behavior are determined as “adaptive.” Adaptive expectation models are ways of predicting an agent’s behavior based on their past experiences and past expectations for that same event.</p>
------------------------------
Question: What is cyclical unemployment?
Answer: Cyclical unemployment is the component of overall unemployment that results directly from cycles of economic upturn and downturn. Unemployment typically rises during recessions and declines during economic expansions. Moderating cyclical unemployment during recessions is a major motivation behind the study of economics and the goal of the various policy tools that governments employ to stimulate the economy.
------------------------------
Question: What is structural unemployment?
Answer: <p>Any disparity between the abilities of available workers and the requirements for open positions can be considered structural unemployment. Structural unemployment is a longer-lasting form of unemployment caused by fundamental shifts in an economy and exacerbated by extraneous factors such as technology, competition, and government policy. Jobs are available, but there is a serious mismatch between what companies need and what workers can offer. For example, local job seekers may be generally skilled, but lack the specific skills required for available job openings and this type of unemployment can also result if sufficiently skilled workers are seeking employment, but available jobs are in another part of the country or the world.</p>
------------------------------
Question: What is hysteresis in economics?
Answer: <p>Hysteresis in economics arises when a single disturbance affects the course of the economy. The specific reasons for hysteresis vary depending on the precipitating event. That said, the persistence of a market malaise after the event has technically passed is most commonly attributed to changes in the attitudes of market participants due to the event. Hysteresis in the field of economics refers to an event in the economy that persists even after the factors that led to that event have been removed or otherwise run their course. Hysteresis often occurs following extreme or prolonged economic events such as an economic crash or recession. After a recession, for example, the unemployment rate may continue to increase despite growth in the economy and the technical end of the recession.</p>
------------------------------
Question: What is inflation-adjusted deficit?
Answer: <p>Inflation-adjusted deficit is given by -</p><p>Inflation-adjusted deficit = Total deficit - (inflation rate x national debt)</p><p>It removes that component of interest payments on the debt that is attributed directly to inflation and gives a more accurate picture of what the budget situation would be at a very low inflation rate.</p>
------------------------------
Question: What are outlays in economics?
Answer: <p>Outlay costs include the expenses paid by a business in order to manufacture a product or provide a service, and also include fees paid to outside parties to acquire assets or services. They can also include hiring costs for strategies or projects that require an addition to the workforce in order to be carried out. In accrual accounting, outlay costs are split across all the periods that the expense applies to and matched to related revenues.</p>
------------------------------
Question: What are capital receipts in economics?
Answer: <p>Capital Receipts are described as the money brought to the business from non-operating sources like proceeds from the sale of long-term assets, capital brought by the proprietor, sum received as a loan or from debenture holders etc. and in contrast revenue receipts are the result of firm’s routine activities during the financial year, which includes sales, commission, interest on investment. Capital Receipts, as mentioned above, are non-recurring in nature. And these sorts of receipts are also not received every now and then.</p>
------------------------------
Question: What are revenue receipts?
Answer: Revenue receipts are the rights of a business to compensation resulting from normal business operations, and are recorded when the business has earned the right to receive them. Generally, this means that once goods are delivered into the hands of the customer or services have been substantially provided; the business has earned the revenue. These receipts are recurring and will affect the business's profit or loss on the income statement. Regardless of whether cash is received or an accounts receivable balance is increased, these are still called revenue receipts.
------------------------------
Question: What is the debt-income ratio?
Answer: The debt-to-income (DTI) ratio is the percentage of your gross monthly income that goes to paying your monthly debt payments and is used by lenders to determine your borrowing risk. A DTI of 43% is typically the highest ratio a borrower can have and still get qualified for a mortgage, but lenders generally seek ratios of no more than 36%.?? A low DTI ratio indicates sufficient income relative to debt servicing, and makes a borrower more attractive. Your gross income is your pay before taxes and other deductions are taken out. The debt-to-income ratio is the percentage of your gross monthly income that goes to paying your monthly debt payments.
------------------------------
Question: What does the Laffer curve show?
Answer: <p>The Laffer Curve shows the relative rates of government revenues and taxation rates. A higher tax rate increases the burden on taxpayers. In the short term, it may increase revenues by a small amount but carries a larger effect in the long term. It reduces the disposable income of taxpayers, which in turn, reduces their consumption expenditure.</p><p><br></p><p><img src="https://images.squarespace-cdn.com/content/v1/56eddde762cd9413e151ac92/1487749780700-GPG8SL45OC3OB6DGMCUS/laffercurve.png?format=1500w" alt="Potential Laffer Curve incident spotted in the wild — Adam Smith Institute"></p>
------------------------------
Question: What is the J curve in economics?
Answer: <p>A J curve is any of a variety of J-shaped diagrams where a curve initially falls, then steeply rises above the starting point. A devalued currency means imports are more expensive, and on the assumption that the volumes of imports and exports change little at first, this causes a fall in the current account (a bigger deficit or smaller surplus). Likewise, if there is a currency revaluation or appreciation the same reasoning may be applied and will lead to an inverted J curve.</p><p><br></p><p><img src="https://financetrain.com/wp-content/uploads/j-curve.gif" alt="The J-Curve – Impact of Exchange Rate Changes on National Economies -  Finance Train"></p>
------------------------------
Question: What is the Gini coefficient?
Answer: The Gini index, or Gini coefficient, is a measure of the distribution of income across a population developed by the Italian statistician Corrado Gini in 1912. It is often used as a gauge of economic inequality, measuring income distribution or, less commonly, wealth distribution among a population. The coefficient ranges from 0 (or 0%) to 1 (or 100%), with 0 representing perfect equality and 1 representing perfect inequality. Values over 1 are theoretically possible due to negative income or wealth.
------------------------------
Question: What is purchasing power parity?
Answer: <p>Purchasing power parities (PPPs) are the rates of currency conversion that try to equalize the purchasing power of different currencies, by eliminating the differences in price levels between countries. The basket of goods and services priced is a sample of all those that are part of final expenditures: final consumption of households and government, fixed capital formation, and net exports and this indicator is measured in terms of national currency per US dollar. It is the theoretical exchange rate at which you can buy the same amount of goods and services with another currency.</p>
------------------------------
Question: What is sterilization in economics?
Answer: <p>The term sterilization is used in international economics and macroeconomics to describe the actions a central bank undertakes in order to neutralize the effects of central bank interventions in the foreign exchange market on the supply of domestic currency in the economy. Sterilization usually takes the form of an open market operation, in which a central bank sells or purchases 